{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cefc7-7c60-4838-9399-26a98ec9736e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43290374-89de-4616-8800-c86799248c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "using DataStructures\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851743ab-a511-40fb-850b-bf90efa9232d",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39765-4abe-409a-bea1-f44fa8ec2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              -10.0 +10.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf920d4-46af-4f22-8933-c3db011ff716",
   "metadata": {},
   "source": [
    "# Q-Learning Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f605b904-b397-4617-9dbe-a27c0b4fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Return the indices and scores of all the peak rewards in the data\n",
    "\"\"\"\n",
    "function find_state_history_R_peaks( X_hist, N_pks )\n",
    "    \n",
    "    epLen   = size( X_hist, 2 )\n",
    "    rising  = false\n",
    "    lastVal = 1e9\n",
    "    lastRis = false\n",
    "    pqPeaks = PriorityQueue();\n",
    "    rtnPeak = []\n",
    "    \n",
    "    for j = 1:epLen\n",
    "        X       = X_hist[:,j]\n",
    "        currVal = cartpole_reward( X )\n",
    "        rising  = (currVal > lastVal)\n",
    "        if (!rising) && lastRis\n",
    "            pqPeaks[j] = -currVal # Store the current index at its current (negative) value\n",
    "        end\n",
    "        lastVal = currVal\n",
    "        lastRis = rising\n",
    "    end\n",
    "    for i = 1:min( N_pks, length( pqPeaks ) )\n",
    "        append!( rtnPeak, dequeue!( pqPeaks ) )\n",
    "    end\n",
    "    \n",
    "    return rtnPeak;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d663e-1ccd-441f-807f-44f84a43e4d0",
   "metadata": {},
   "source": [
    "# Q-Function Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91f06c-df14-4fe7-b81d-12c3184b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721c7-88a9-4b57-bf9f-ad9f9acbf786",
   "metadata": {},
   "source": [
    "# CartPole Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc4097d-9b96-453c-ba4f-4b06fce7fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083b48-38dc-4616-979a-da8874303d32",
   "metadata": {},
   "source": [
    "# Agent Data Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f648d5-8d8e-4da4-bd1e-3f3d9ec7c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres     = Fmax/Fdiv\n",
    "spaceDiv = 4.0 # 1.0 # 2.0 # 5.0 # 7.5  \n",
    "\n",
    "### Construct grid of anchors ###\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ spaceDiv, spaceDiv, spaceDiv, spaceDiv, Fres ] );\n",
    "nPts = size( G )[2]; # ------- Number of anchors\n",
    "mDim = size( G )[1]; # ------- Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db1609-9df1-438b-9675-0286bf01a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states  = zeros( size( X_0, 1 ), T )\n",
    "actions = zeros( T );\n",
    "bestXs  = zeros( size( X_0, 1 ), T )\n",
    "bestAs  = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9f1ef-79bc-41fd-b6e9-ab0554460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d49b4c6-8353-4a01-8a16-9b544e1ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c954412-18b9-45a8-97a6-e61cf19f15d2",
   "metadata": {},
   "source": [
    "# Agent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358ff3d-44a5-491e-9597-0a0a73c6b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Q(TD)-Learning Params #####\n",
    "scale = 7.5; #1.650; # ----------- scale\n",
    "vNN   =  4 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\"\n",
    "\n",
    "alpha    = 0.02148 # 0.99 # 0.75 # 0.5 # 0.25 # 0.125 # 0.0625 # 0.03125 # 0.015625 # 0.00782 # 0.00391\n",
    "gamma    = 1.00\n",
    "swapDiv  = 64\n",
    "epsMin   = 0.00 # Last iter is policy eval\n",
    "epsMax   = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes = 64 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs   = 32 #128 #64 # 32 #16\n",
    "EXPrand  = 1.00 #0.25 #0.5 # 0.75\n",
    "Alpha    = 0.875\n",
    "aMargin  = (pi/180)*15.0;\n",
    "\n",
    "##### Q-Function Hacks #####\n",
    "beta   = 0.15\n",
    "blSode = false\n",
    "blPoch = false\n",
    "\n",
    "##### Eligibility Params #####\n",
    "useElig = true\n",
    "N_peaks =   2\n",
    "N_steps = 128\n",
    "lambda  =   0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e910ca2-281c-4d06-98e2-1c96fa7c1916",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d3689b-947a-400b-9031-9f1a13f4df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0\n",
      "Training Iteration 4 score: 0.36000000000000015, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.6400000000000003, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.3100000000000001, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.5400000000000003, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.2100000000000009, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.6700000000000004, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.22000000000000006, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.09999999999999999, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.2700000000000001, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.2375000000000001\n",
      "\n",
      "Epoch 2, Best Score: 1.5400000000000011\n",
      "Training Iteration 4 score: 0.09999999999999999, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.13999999999999999, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.18000000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.18000000000000002, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.49000000000000027, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.12999999999999998, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.5900000000000003, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.15, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.23000000000000007, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.18000000000000002, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.11999999999999998, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.13999999999999999, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.17, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.2631250000000001\n",
      "\n",
      "Epoch 3, Best Score: 1.5700000000000012\n",
      "Training Iteration 4 score: 1.1500000000000008, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.17, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.07515625000000005\n",
      "\n",
      "Epoch 4, Best Score: 1.5700000000000012\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.1400000000000008, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.6400000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.04859375000000002\n",
      "\n",
      "Epoch 5, Best Score: 1.5700000000000012\n",
      "Training Iteration 4 score: 0.9100000000000006, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.35000000000000014, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.21000000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.18000000000000002, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.23000000000000007, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.24000000000000007, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.15843750000000006\n",
      "\n",
      "Epoch 6, Best Score: 1.6100000000000012\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.08, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.3100000000000001, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.10843750000000006\n",
      "\n",
      "Epoch 7, Best Score: 1.6100000000000012\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7800000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 2.5899999999999888, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.21000000000000005, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.13999999999999999, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.18000000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.27499999999999974\n",
      "\n",
      "Epoch 8, Best Score: 2.5899999999999888\n",
      "Training Iteration 4 score: 0.060000000000000005, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.24000000000000007, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.3000000000000001, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.6600000000000004, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.24000000000000007, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.3200000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.4100000000000002, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.14500000000000007\n",
      "\n",
      "Epoch 9, Best Score: 2.5899999999999888\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 10, Best Score: 2.5899999999999888\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.07, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.09, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.060000000000000005, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.060000000000000005, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.05359375000000002\n",
      "\n",
      "Epoch 11, Best Score: 2.5899999999999888\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.35000000000000014, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.3900000000000002, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.34000000000000014, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.17375000000000013\n",
      "\n",
      "Epoch 12, Best Score: 2.5899999999999888\n",
      "Training Iteration 4 score: 0.3300000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.2700000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.45000000000000023, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.48000000000000026, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.2800000000000001, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.2700000000000001, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.2800000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.34000000000000014, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.3100000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.2700000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.34000000000000014, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.3100000000000001, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 3.839999999999962, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.8000000000000005, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.20000000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.21000000000000005, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.46921874999999963\n",
      "\n",
      "Epoch 13, Best Score: 3.839999999999962\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 14, Best Score: 3.839999999999962\n",
      "Training Iteration 4 score: 1.0700000000000007, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.7400000000000004, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 1.330000000000001, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.9300000000000006, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.7800000000000005, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.5800000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 1.0000000000000007, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.9400000000000006, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 1.7800000000000014, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 1.0900000000000007, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.0300000000000007, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.20000000000000004, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.22000000000000006, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.6900000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.9200000000000006, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.5237500000000003\n",
      "\n",
      "Epoch 15, Best Score: 3.839999999999962\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 16, Best Score: 3.839999999999962\n",
      "Training Iteration 4 score: 0.8700000000000006, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.23000000000000007, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7800000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.45000000000000023, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.9600000000000006, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.370000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 1.470000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 1.5200000000000011, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 1.430000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 1.0800000000000007, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.20000000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 1.1400000000000008, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.15, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 1.1600000000000008, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.9400000000000006, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.9393750000000003\n",
      "\n",
      "Epoch 17, Best Score: 3.839999999999962\n",
      "Training Iteration 4 score: 0.5200000000000002, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.5400000000000003, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.9700000000000006, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.7100000000000004, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.22000000000000006, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.6300000000000003, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.19000000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.5600000000000003, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.6000000000000003, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.48000000000000026, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.48000000000000026, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.48000000000000026, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.46000000000000024, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.4300000000000002, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.20000000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.4100000000000002, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.3801562500000003\n",
      "\n",
      "Epoch 18, Best Score: 3.839999999999962\n",
      "Training Iteration 4 score: 2.819999999999984, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.7600000000000013, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.4400000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 2.9299999999999815, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 2.259999999999996, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.8800000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 2.439999999999992, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 4.349999999999952, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 1.0500000000000007, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.8400000000000005, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 4.089999999999957, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.2500000000000009, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 2.669999999999987, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 4.589999999999947, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 5.209999999999933, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 3.47999999999997, epsilon: 8.881784197001252e-16\n",
      "Average Score: 2.4771874999999817\n",
      "\n",
      "Epoch 19, Best Score: 6.459999999999907\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.07, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.11999999999999998, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.46000000000000024, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.06703125000000003\n",
      "\n",
      "Epoch 20, Best Score: 6.459999999999907\n",
      "Training Iteration 4 score: 0.5700000000000003, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.22000000000000006, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 1.7900000000000014, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.22000000000000006, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.24000000000000007, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.24000000000000007, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.49000000000000027, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.2900000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.23000000000000007, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.36000000000000015, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.5300000000000002, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.20000000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.21000000000000005, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.22000000000000006, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.20000000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.49000000000000027, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.3237500000000002\n",
      "\n",
      "Epoch 21, Best Score: 6.459999999999907\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.6900000000000004, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.8600000000000005, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 1.2200000000000009, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.13046875000000008\n",
      "\n",
      "Epoch 22, Best Score: 6.459999999999907\n",
      "Training Iteration 4 score: 1.6100000000000012, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.290000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5100000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.4200000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.9900000000000007, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 1.9800000000000015, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.360000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.18000000000000002, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 1.300000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.17, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.18000000000000002, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.5200000000000002, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.3300000000000001, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.18000000000000002, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.6300000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.8900000000000006, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.6970312499999994\n",
      "\n",
      "Epoch 23, Best Score: 6.459999999999907\n",
      "Training Iteration 4 score: 0.5400000000000003, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.7900000000000005, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.7000000000000004, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.6900000000000004, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 24.080000000000965, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.8100000000000005, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 21.650000000000585, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 23.950000000000944, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 19.010000000000172, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 26.03000000000127, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 23.430000000000863, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.11999999999999998, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 23.120000000000815, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 12.179999999999785, epsilon: 8.881784197001252e-16\n",
      "Average Score: 9.59453125000036\n",
      "\n",
      "Epoch 24, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 1.280000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.8700000000000014, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.9900000000000007, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.22000000000000006, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.4200000000000002, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.25000000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.440000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 1.1600000000000008, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 1.8400000000000014, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.8000000000000005, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.7400000000000004, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.7600000000000013, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.6900000000000004, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.5400000000000003, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.8300000000000005, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.6700000000000004, epsilon: 8.881784197001252e-16\n",
      "Average Score: 1.05515625\n",
      "\n",
      "Epoch 25, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.012812500000000006\n",
      "\n",
      "Epoch 26, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.37000000000000016, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.8000000000000005, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7900000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.310000000000001, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.6000000000000003, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.8800000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.5500000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.38000000000000017, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.3200000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.7000000000000004, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.6900000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.25000000000000006, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.37000000000000016, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.7700000000000005, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.7200000000000004, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.6079687500000003\n",
      "\n",
      "Epoch 27, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.3000000000000001, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.13999999999999999, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.008906250000000001\n",
      "\n",
      "Epoch 28, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.46000000000000024, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.8500000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.2400000000000009, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 1.5400000000000011, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.02, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 2.579999999999989, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.13999999999999999, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 1.0000000000000007, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 1.0200000000000007, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.270000000000001, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.13999999999999999, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.34000000000000014, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 1.1100000000000008, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 1.0400000000000007, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.9464062499999999\n",
      "\n",
      "Epoch 29, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.3000000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.7600000000000005, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.3000000000000001, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.38000000000000017, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.35000000000000014, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.5000000000000002, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.0000000000000007, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.3900000000000002, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.36000000000000015, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.4000000000000002, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.9300000000000006, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.1200000000000008, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.34000000000000014, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.36000000000000015, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 1.1300000000000008, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.3300000000000001, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.6240625000000005\n",
      "\n",
      "Epoch 30, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.47000000000000025, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.5600000000000003, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7300000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.8200000000000005, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.24000000000000007, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.5000000000000002, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.18000000000000002, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.35000000000000014, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.45000000000000023, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.21000000000000005, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.2800000000000001, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.5800000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.19000000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.4300000000000002, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.37156250000000046\n",
      "\n",
      "Epoch 31, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 32, Best Score: 30.420000000001956\n",
      "Training Iteration 4 score: 0.23000000000000007, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.21000000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.17, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.4400000000000002, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.8100000000000005, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.6600000000000004, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.2800000000000001, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.35000000000000014, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.17250000000000004\n",
      "Saved a trained Q-table with size (76032,), After 29.186981296539308 minutes of training!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip970\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip970)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip971\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip970)\" d=\"\n",
       "M112.177 1486.45 L2352.76 1486.45 L2352.76 47.2441 L112.177 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip972\">\n",
       "    <rect x=\"112\" y=\"47\" width=\"2242\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  448.332,1486.45 448.332,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  789.26,1486.45 789.26,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1130.19,1486.45 1130.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1471.12,1486.45 1471.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1812.04,1486.45 1812.04,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2152.97,1486.45 2152.97,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  448.332,1486.45 448.332,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  789.26,1486.45 789.26,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1130.19,1486.45 1130.19,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1471.12,1486.45 1471.12,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1812.04,1486.45 1812.04,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2152.97,1486.45 2152.97,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip970)\" d=\"M438.61 1514.29 L456.966 1514.29 L456.966 1518.22 L442.892 1518.22 L442.892 1526.7 Q443.911 1526.35 444.929 1526.19 Q445.948 1526 446.966 1526 Q452.753 1526 456.133 1529.17 Q459.513 1532.34 459.513 1537.76 Q459.513 1543.34 456.04 1546.44 Q452.568 1549.52 446.249 1549.52 Q444.073 1549.52 441.804 1549.15 Q439.559 1548.78 437.152 1548.04 L437.152 1543.34 Q439.235 1544.47 441.457 1545.03 Q443.679 1545.58 446.156 1545.58 Q450.161 1545.58 452.499 1543.48 Q454.837 1541.37 454.837 1537.76 Q454.837 1534.15 452.499 1532.04 Q450.161 1529.94 446.156 1529.94 Q444.281 1529.94 442.406 1530.35 Q440.554 1530.77 438.61 1531.65 L438.61 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M763.948 1544.91 L771.587 1544.91 L771.587 1518.55 L763.277 1520.21 L763.277 1515.95 L771.54 1514.29 L776.216 1514.29 L776.216 1544.91 L783.855 1544.91 L783.855 1548.85 L763.948 1548.85 L763.948 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M803.299 1517.37 Q799.688 1517.37 797.86 1520.93 Q796.054 1524.47 796.054 1531.6 Q796.054 1538.71 797.86 1542.27 Q799.688 1545.82 803.299 1545.82 Q806.934 1545.82 808.739 1542.27 Q810.568 1538.71 810.568 1531.6 Q810.568 1524.47 808.739 1520.93 Q806.934 1517.37 803.299 1517.37 M803.299 1513.66 Q809.11 1513.66 812.165 1518.27 Q815.244 1522.85 815.244 1531.6 Q815.244 1540.33 812.165 1544.94 Q809.11 1549.52 803.299 1549.52 Q797.489 1549.52 794.411 1544.94 Q791.355 1540.33 791.355 1531.6 Q791.355 1522.85 794.411 1518.27 Q797.489 1513.66 803.299 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M1105.37 1544.91 L1113.01 1544.91 L1113.01 1518.55 L1104.7 1520.21 L1104.7 1515.95 L1112.97 1514.29 L1117.64 1514.29 L1117.64 1544.91 L1125.28 1544.91 L1125.28 1548.85 L1105.37 1548.85 L1105.37 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M1134.77 1514.29 L1153.13 1514.29 L1153.13 1518.22 L1139.05 1518.22 L1139.05 1526.7 Q1140.07 1526.35 1141.09 1526.19 Q1142.11 1526 1143.13 1526 Q1148.91 1526 1152.29 1529.17 Q1155.67 1532.34 1155.67 1537.76 Q1155.67 1543.34 1152.2 1546.44 Q1148.73 1549.52 1142.41 1549.52 Q1140.23 1549.52 1137.97 1549.15 Q1135.72 1548.78 1133.31 1548.04 L1133.31 1543.34 Q1135.4 1544.47 1137.62 1545.03 Q1139.84 1545.58 1142.32 1545.58 Q1146.32 1545.58 1148.66 1543.48 Q1151 1541.37 1151 1537.76 Q1151 1534.15 1148.66 1532.04 Q1146.32 1529.94 1142.32 1529.94 Q1140.44 1529.94 1138.57 1530.35 Q1136.72 1530.77 1134.77 1531.65 L1134.77 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M1449.89 1544.91 L1466.21 1544.91 L1466.21 1548.85 L1444.26 1548.85 L1444.26 1544.91 Q1446.93 1542.16 1451.51 1537.53 Q1456.12 1532.88 1457.3 1531.53 Q1459.54 1529.01 1460.42 1527.27 Q1461.32 1525.51 1461.32 1523.82 Q1461.32 1521.07 1459.38 1519.33 Q1457.46 1517.6 1454.36 1517.6 Q1452.16 1517.6 1449.7 1518.36 Q1447.27 1519.13 1444.5 1520.68 L1444.5 1515.95 Q1447.32 1514.82 1449.77 1514.24 Q1452.23 1513.66 1454.26 1513.66 Q1459.63 1513.66 1462.83 1516.35 Q1466.02 1519.03 1466.02 1523.52 Q1466.02 1525.65 1465.21 1527.57 Q1464.43 1529.47 1462.32 1532.07 Q1461.74 1532.74 1458.64 1535.95 Q1455.54 1539.15 1449.89 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M1486.02 1517.37 Q1482.41 1517.37 1480.58 1520.93 Q1478.78 1524.47 1478.78 1531.6 Q1478.78 1538.71 1480.58 1542.27 Q1482.41 1545.82 1486.02 1545.82 Q1489.66 1545.82 1491.46 1542.27 Q1493.29 1538.71 1493.29 1531.6 Q1493.29 1524.47 1491.46 1520.93 Q1489.66 1517.37 1486.02 1517.37 M1486.02 1513.66 Q1491.83 1513.66 1494.89 1518.27 Q1497.97 1522.85 1497.97 1531.6 Q1497.97 1540.33 1494.89 1544.94 Q1491.83 1549.52 1486.02 1549.52 Q1480.21 1549.52 1477.13 1544.94 Q1474.08 1540.33 1474.08 1531.6 Q1474.08 1522.85 1477.13 1518.27 Q1480.21 1513.66 1486.02 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M1791.32 1544.91 L1807.63 1544.91 L1807.63 1548.85 L1785.69 1548.85 L1785.69 1544.91 Q1788.35 1542.16 1792.94 1537.53 Q1797.54 1532.88 1798.72 1531.53 Q1800.97 1529.01 1801.85 1527.27 Q1802.75 1525.51 1802.75 1523.82 Q1802.75 1521.07 1800.81 1519.33 Q1798.88 1517.6 1795.78 1517.6 Q1793.58 1517.6 1791.13 1518.36 Q1788.7 1519.13 1785.92 1520.68 L1785.92 1515.95 Q1788.75 1514.82 1791.2 1514.24 Q1793.65 1513.66 1795.69 1513.66 Q1801.06 1513.66 1804.25 1516.35 Q1807.45 1519.03 1807.45 1523.52 Q1807.45 1525.65 1806.64 1527.57 Q1805.85 1529.47 1803.75 1532.07 Q1803.17 1532.74 1800.07 1535.95 Q1796.96 1539.15 1791.32 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M1817.5 1514.29 L1835.85 1514.29 L1835.85 1518.22 L1821.78 1518.22 L1821.78 1526.7 Q1822.8 1526.35 1823.81 1526.19 Q1824.83 1526 1825.85 1526 Q1831.64 1526 1835.02 1529.17 Q1838.4 1532.34 1838.4 1537.76 Q1838.4 1543.34 1834.93 1546.44 Q1831.45 1549.52 1825.13 1549.52 Q1822.96 1549.52 1820.69 1549.15 Q1818.44 1548.78 1816.04 1548.04 L1816.04 1543.34 Q1818.12 1544.47 1820.34 1545.03 Q1822.56 1545.58 1825.04 1545.58 Q1829.05 1545.58 1831.38 1543.48 Q1833.72 1541.37 1833.72 1537.76 Q1833.72 1534.15 1831.38 1532.04 Q1829.05 1529.94 1825.04 1529.94 Q1823.17 1529.94 1821.29 1530.35 Q1819.44 1530.77 1817.5 1531.65 L1817.5 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M2141.81 1530.21 Q2145.17 1530.93 2147.05 1533.2 Q2148.94 1535.47 2148.94 1538.8 Q2148.94 1543.92 2145.43 1546.72 Q2141.91 1549.52 2135.43 1549.52 Q2133.25 1549.52 2130.94 1549.08 Q2128.64 1548.66 2126.19 1547.81 L2126.19 1543.29 Q2128.13 1544.43 2130.45 1545.01 Q2132.76 1545.58 2135.29 1545.58 Q2139.69 1545.58 2141.98 1543.85 Q2144.29 1542.11 2144.29 1538.8 Q2144.29 1535.75 2142.14 1534.03 Q2140.01 1532.3 2136.19 1532.3 L2132.16 1532.3 L2132.16 1528.45 L2136.37 1528.45 Q2139.82 1528.45 2141.65 1527.09 Q2143.48 1525.7 2143.48 1523.11 Q2143.48 1520.45 2141.58 1519.03 Q2139.71 1517.6 2136.19 1517.6 Q2134.27 1517.6 2132.07 1518.01 Q2129.87 1518.43 2127.23 1519.31 L2127.23 1515.14 Q2129.89 1514.4 2132.21 1514.03 Q2134.55 1513.66 2136.61 1513.66 Q2141.93 1513.66 2145.03 1516.09 Q2148.13 1518.5 2148.13 1522.62 Q2148.13 1525.49 2146.49 1527.48 Q2144.85 1529.45 2141.81 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M2167.81 1517.37 Q2164.2 1517.37 2162.37 1520.93 Q2160.56 1524.47 2160.56 1531.6 Q2160.56 1538.71 2162.37 1542.27 Q2164.2 1545.82 2167.81 1545.82 Q2171.44 1545.82 2173.25 1542.27 Q2175.08 1538.71 2175.08 1531.6 Q2175.08 1524.47 2173.25 1520.93 Q2171.44 1517.37 2167.81 1517.37 M2167.81 1513.66 Q2173.62 1513.66 2176.68 1518.27 Q2179.75 1522.85 2179.75 1531.6 Q2179.75 1540.33 2176.68 1544.94 Q2173.62 1549.52 2167.81 1549.52 Q2162 1549.52 2158.92 1544.94 Q2155.87 1540.33 2155.87 1531.6 Q2155.87 1522.85 2158.92 1518.27 Q2162 1513.66 2167.81 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,1445.72 2352.76,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,1162.69 2352.76,1162.69 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,879.668 2352.76,879.668 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,596.645 2352.76,596.645 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip972)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,313.621 2352.76,313.621 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1486.45 112.177,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1445.72 131.075,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1162.69 131.075,1162.69 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,879.668 131.075,879.668 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,596.645 131.075,596.645 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,313.621 131.075,313.621 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip970)\" d=\"M64.2328 1431.51 Q60.6217 1431.51 58.793 1435.08 Q56.9875 1438.62 56.9875 1445.75 Q56.9875 1452.86 58.793 1456.42 Q60.6217 1459.96 64.2328 1459.96 Q67.867 1459.96 69.6726 1456.42 Q71.5013 1452.86 71.5013 1445.75 Q71.5013 1438.62 69.6726 1435.08 Q67.867 1431.51 64.2328 1431.51 M64.2328 1427.81 Q70.0429 1427.81 73.0985 1432.42 Q76.1772 1437 76.1772 1445.75 Q76.1772 1454.48 73.0985 1459.08 Q70.0429 1463.67 64.2328 1463.67 Q58.4226 1463.67 55.344 1459.08 Q52.2884 1454.48 52.2884 1445.75 Q52.2884 1437 55.344 1432.42 Q58.4226 1427.81 64.2328 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M59.8578 1176.04 L76.1772 1176.04 L76.1772 1179.97 L54.2328 1179.97 L54.2328 1176.04 Q56.8949 1173.28 61.4782 1168.65 Q66.0846 1164 67.2652 1162.66 Q69.5105 1160.13 70.3902 1158.4 Q71.2929 1156.64 71.2929 1154.95 Q71.2929 1152.19 69.3485 1150.46 Q67.4272 1148.72 64.3254 1148.72 Q62.1263 1148.72 59.6726 1149.49 Q57.2421 1150.25 54.4643 1151.8 L54.4643 1147.08 Q57.2884 1145.94 59.7421 1145.37 Q62.1958 1144.79 64.2328 1144.79 Q69.6031 1144.79 72.7976 1147.47 Q75.992 1150.16 75.992 1154.65 Q75.992 1156.78 75.1818 1158.7 Q74.3948 1160.6 72.2883 1163.19 Q71.7096 1163.86 68.6078 1167.08 Q65.5059 1170.27 59.8578 1176.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M66.5939 866.463 L54.7884 884.912 L66.5939 884.912 L66.5939 866.463 M65.367 862.388 L71.2466 862.388 L71.2466 884.912 L76.1772 884.912 L76.1772 888.8 L71.2466 888.8 L71.2466 896.948 L66.5939 896.948 L66.5939 888.8 L50.9921 888.8 L50.9921 884.287 L65.367 862.388 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M64.6495 594.781 Q61.5013 594.781 59.6495 596.934 Q57.8208 599.087 57.8208 602.837 Q57.8208 606.564 59.6495 608.74 Q61.5013 610.892 64.6495 610.892 Q67.7976 610.892 69.6263 608.74 Q71.4781 606.564 71.4781 602.837 Q71.4781 599.087 69.6263 596.934 Q67.7976 594.781 64.6495 594.781 M73.9318 580.129 L73.9318 584.388 Q72.1726 583.555 70.367 583.115 Q68.5846 582.675 66.8254 582.675 Q62.1958 582.675 59.7421 585.8 Q57.3115 588.925 56.9643 595.244 Q58.33 593.231 60.3902 592.166 Q62.4504 591.078 64.9272 591.078 Q70.1355 591.078 73.1448 594.249 Q76.1772 597.397 76.1772 602.837 Q76.1772 608.161 73.029 611.379 Q69.8809 614.596 64.6495 614.596 Q58.6541 614.596 55.4828 610.013 Q52.3116 605.406 52.3116 596.68 Q52.3116 588.485 56.2004 583.624 Q60.0893 578.74 66.6402 578.74 Q68.3994 578.74 70.1818 579.087 Q71.9874 579.434 73.9318 580.129 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M64.3254 314.489 Q60.9921 314.489 59.0708 316.272 Q57.1726 318.054 57.1726 321.179 Q57.1726 324.304 59.0708 326.086 Q60.9921 327.869 64.3254 327.869 Q67.6587 327.869 69.58 326.086 Q71.5013 324.281 71.5013 321.179 Q71.5013 318.054 69.58 316.272 Q67.6819 314.489 64.3254 314.489 M59.6495 312.499 Q56.6402 311.758 54.9504 309.698 Q53.2838 307.638 53.2838 304.675 Q53.2838 300.531 56.2236 298.124 Q59.1865 295.716 64.3254 295.716 Q69.4874 295.716 72.4272 298.124 Q75.367 300.531 75.367 304.675 Q75.367 307.638 73.6772 309.698 Q72.0105 311.758 69.0244 312.499 Q72.404 313.286 74.279 315.577 Q76.1772 317.869 76.1772 321.179 Q76.1772 326.202 73.0985 328.887 Q70.0429 331.573 64.3254 331.573 Q58.6078 331.573 55.5291 328.887 Q52.4736 326.202 52.4736 321.179 Q52.4736 317.869 54.3717 315.577 Q56.2699 313.286 59.6495 312.499 M57.9365 305.114 Q57.9365 307.8 59.6032 309.304 Q61.293 310.809 64.3254 310.809 Q67.3346 310.809 69.0244 309.304 Q70.7374 307.8 70.7374 305.114 Q70.7374 302.429 69.0244 300.925 Q67.3346 299.42 64.3254 299.42 Q61.293 299.42 59.6032 300.925 Q57.9365 302.429 57.9365 305.114 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip972)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.59,1412.11 243.775,1408.48 311.961,1435.08 380.147,1438.84 448.332,1423.29 516.518,1430.37 584.703,1406.8 652.889,1425.2 721.075,1445.72 789.26,1438.13 \n",
       "  857.446,1421.13 925.631,1379.32 993.817,1445.72 1062,1371.6 1130.19,1445.72 1198.37,1312.78 1266.56,1391.92 1334.74,1095.16 1402.93,1436.23 1471.12,1399.9 \n",
       "  1539.3,1427.25 1607.49,1347.08 1675.67,87.9763 1743.86,1296.4 1812.04,1443.9 1880.23,1359.68 1948.42,1444.46 2016.6,1311.79 2084.79,1357.4 2152.97,1393.14 \n",
       "  2221.16,1445.72 2289.34,1421.3 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip970)\" d=\"\n",
       "M2001.58 198.898 L2278.07 198.898 L2278.07 95.2176 L2001.58 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2001.58,198.898 2278.07,198.898 2278.07,95.2176 2001.58,95.2176 2001.58,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip970)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2026.48,147.058 2175.85,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip970)\" d=\"M2214.59 166.745 Q2212.78 171.375 2211.07 172.787 Q2209.36 174.199 2206.49 174.199 L2203.08 174.199 L2203.08 170.634 L2205.58 170.634 Q2207.34 170.634 2208.31 169.8 Q2209.29 168.967 2210.47 165.865 L2211.23 163.921 L2200.74 138.412 L2205.26 138.412 L2213.36 158.689 L2221.46 138.412 L2225.98 138.412 L2214.59 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip970)\" d=\"M2233.27 160.402 L2240.91 160.402 L2240.91 134.037 L2232.6 135.703 L2232.6 131.444 L2240.86 129.778 L2245.54 129.778 L2245.54 160.402 L2253.17 160.402 L2253.17 164.338 L2233.27 164.338 L2233.27 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgn       = time()\n",
    "averages  = []\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    bestEpSc    = -100.0;\n",
    "    statesBest  = zeros( size( X_0, 1 ), T )\n",
    "    actionsBest = zeros( T );\n",
    "    \n",
    "    if blSode\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    elseif blPoch\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    else\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    end\n",
    "    \n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/(episodes-1)\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        \n",
    "        s_l = 0.0\n",
    "        # while s_l == 0\n",
    "        \n",
    "            X  = X_0\n",
    "\n",
    "            ##### Double Q-Learning ###########################################\n",
    "\n",
    "            for k = 1:T\n",
    "\n",
    "                # 1. Choose action\n",
    "                if rand() < epsilon\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                else\n",
    "\n",
    "                    A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                    if A == 1000.0 # Indicates no values in this region\n",
    "                        if rand() < EXPrand \n",
    "                            A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                        else\n",
    "                            A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                # 2. Cache last state\n",
    "                qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "                # 3. Generate the next stae\n",
    "                Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "                # 4. Collect reward R( s, a, s' )\n",
    "                R_t = cartpole_reward( Xp )\n",
    "\n",
    "                # 5. Get the optimal action at the next state\n",
    "                a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "\n",
    "                # 6. Compute the value at the next state\n",
    "\n",
    "                V_tp1_opt = query_value_fuzzy( \n",
    "                    Q_kdTree, G, V, \n",
    "                    get_Q( \n",
    "                        select_X_vector( Xp ), \n",
    "                        a_tp1_opt \n",
    "                    ); \n",
    "                    k = vNN \n",
    "                )\n",
    "                if isnan( V_tp1_opt )\n",
    "                    V_tp1_opt = 0.0\n",
    "                end\n",
    "\n",
    "\n",
    "                # 7. Blend the value back into nearest points\n",
    "\n",
    "                idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "                nNear      = size( idxs, 1 )\n",
    "                for i = 1:nNear\n",
    "                    j    = idxs[i]\n",
    "                    if !isnan( wgts[i] ) \n",
    "\n",
    "                        # VS[j] = R_t + gamma * V_tp1_opt # Q-Learning\n",
    "                        VS[j] = VS[j] + alpha*( R_t + gamma*V_tp1_opt - V[j] ) # Q(TD)-Learning\n",
    "\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                states[:,k] = Xp\n",
    "                actions[k]  = A\n",
    "\n",
    "                X = Xp\n",
    "            end\n",
    "\n",
    "            s_l    = vertical_score_s( states, aMargin, ts )\n",
    "            \n",
    "        # end\n",
    "            \n",
    "        s_Totl += s_l\n",
    "    \n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if s_l > bestEpSc\n",
    "            bestEpSc    = s_l\n",
    "            statesBest  = copy( states  )\n",
    "            actionsBest = copy( actions )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        ##### Eligibility Traces ##########################################\n",
    "        # if useElig && (s_l > s_Totl/(1.0*l)) && (s_l > 0.0) \n",
    "        # if useElig && (s_l > 0.0) \n",
    "        if useElig \n",
    "            \n",
    "            # if s_l == 0.0\n",
    "            #     states  = copy( bestXs )\n",
    "            #     actions = copy( bestAs )\n",
    "            # end\n",
    "            \n",
    "            # println( \"Assign eligibility for a history with score \", s_l )\n",
    "        \n",
    "            # 1. Find `N_peaks`\n",
    "            peakDices = find_state_history_R_peaks( states, N_peaks )\n",
    "            # 2. For each peak, iterate back in time through states\n",
    "            for ii = 1:min(N_peaks, length(peakDices))\n",
    "                topDex = peakDices[ ii ]\n",
    "                X      = states[:,topDex]\n",
    "                R_jj    = cartpole_reward( X )\n",
    "                # 3. For each Q-state in the trace\n",
    "                for jj = (topDex-1):-1:max(1,topDex-N_steps)\n",
    "                    X = states[:,jj]\n",
    "                    R_jj *= lambda\n",
    "                    a_jj = actions[jj]\n",
    "                    q_jj = get_Q( select_X_vector( X ), a_jj )\n",
    "                    V_jj = query_value_fuzzy( Q_kdTree, G, V, q_jj; k = vNN )\n",
    "\n",
    "                    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, q_jj; k = bNN )\n",
    "                    nNear      = size( idxs, 1 )\n",
    "\n",
    "                    for kk = 1:nNear\n",
    "                        ll = idxs[kk]\n",
    "                        if !isnan( wgts[kk] ) \n",
    "                            VS[ll] = VS[ll] + alpha*( R_jj + V_jj - V[ll] ) # Q(TD)-Learning\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Decay the exploration probability\n",
    "        epsilon -= deltaEp\n",
    "        \n",
    "        \n",
    "        ##### Double Q-Learning ##########################################\n",
    "        # Every `swapDiv` episodes, swap Q-functions for Double Q-Learning\n",
    "        \n",
    "        if (l % swapDiv == 0)\n",
    "            \n",
    "            vSwp = copy( VS   )\n",
    "            VS   = copy( V    )\n",
    "            V    = copy( vSwp )\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "     \n",
    "    \n",
    "    ##### Q-Function Hacks ################################################\n",
    "    \n",
    "    # Blend Method 1: Best Episode\n",
    "    if blSode\n",
    "        V  = blend_alpha_of_A_into_B( beta, vBst, V  )\n",
    "        VS = blend_alpha_of_A_into_B( beta, vBst, VS )\n",
    "    end\n",
    "    \n",
    "    # if (s_Avg > bestAvg) && true\n",
    "    #     println( \"BLEND\" )\n",
    "    #     bestAvg = s_Avg\n",
    "    #     vBAv    = copy( V ) # Try a blend of both next # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    #     vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V ) # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    # end\n",
    "        \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c1d8a-58c5-4719-89c8-b69bf6623266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
