{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cefc7-7c60-4838-9399-26a98ec9736e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43290374-89de-4616-8800-c86799248c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "using DataStructures\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851743ab-a511-40fb-850b-bf90efa9232d",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39765-4abe-409a-bea1-f44fa8ec2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              -10.0 +10.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf920d4-46af-4f22-8933-c3db011ff716",
   "metadata": {},
   "source": [
    "# Q-Learning Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f605b904-b397-4617-9dbe-a27c0b4fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Return the indices and scores of all the peak rewards in the data\n",
    "\"\"\"\n",
    "function find_state_history_R_peaks( X_hist, N_pks )\n",
    "    \n",
    "    epLen   = size( X_hist, 2 )\n",
    "    rising  = false\n",
    "    lastVal = 1e9\n",
    "    lastRis = false\n",
    "    pqPeaks = PriorityQueue();\n",
    "    rtnPeak = []\n",
    "    \n",
    "    for j = 1:epLen\n",
    "        X       = X_hist[:,j]\n",
    "        currVal = cartpole_reward( X )\n",
    "        rising  = (currVal > lastVal)\n",
    "        if (!rising) && lastRis\n",
    "            pqPeaks[j] = -currVal # Store the current index at its current (negative) value\n",
    "        end\n",
    "        lastVal = currVal\n",
    "        lastRis = rising\n",
    "    end\n",
    "    for i = 1:min( N_pks, length( pqPeaks ) )\n",
    "        append!( rtnPeak, dequeue!( pqPeaks ) )\n",
    "    end\n",
    "    \n",
    "    return rtnPeak;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d663e-1ccd-441f-807f-44f84a43e4d0",
   "metadata": {},
   "source": [
    "# Q-Function Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91f06c-df14-4fe7-b81d-12c3184b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721c7-88a9-4b57-bf9f-ad9f9acbf786",
   "metadata": {},
   "source": [
    "# CartPole Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc4097d-9b96-453c-ba4f-4b06fce7fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083b48-38dc-4616-979a-da8874303d32",
   "metadata": {},
   "source": [
    "# Agent Data Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f648d5-8d8e-4da4-bd1e-3f3d9ec7c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres     = Fmax/Fdiv\n",
    "spaceDiv = 4.0 # 1.0 # 2.0 # 5.0 # 7.5  \n",
    "\n",
    "### Construct grid of anchors ###\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ spaceDiv, spaceDiv, spaceDiv, spaceDiv, Fres ] );\n",
    "nPts = size( G )[2]; # ------- Number of anchors\n",
    "mDim = size( G )[1]; # ------- Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db1609-9df1-438b-9675-0286bf01a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states  = zeros( size( X_0, 1 ), T )\n",
    "actions = zeros( T );\n",
    "bestXs  = zeros( size( X_0, 1 ), T )\n",
    "bestAs  = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9f1ef-79bc-41fd-b6e9-ab0554460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d49b4c6-8353-4a01-8a16-9b544e1ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c954412-18b9-45a8-97a6-e61cf19f15d2",
   "metadata": {},
   "source": [
    "# Agent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358ff3d-44a5-491e-9597-0a0a73c6b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Q(TD)-Learning Params #####\n",
    "scale = 7.5; #1.650; # ----------- scale\n",
    "vNN   =  4 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\"\n",
    "\n",
    "alpha    = 0.02148 # 0.99 # 0.75 # 0.5 # 0.25 # 0.125 # 0.0625 # 0.03125 # 0.015625 # 0.00782 # 0.00391\n",
    "gamma    = 1.00 \n",
    "swapDiv  = 128\n",
    "epsMin   = 0.00 # Last iter is policy eval\n",
    "epsMax   = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes = 128 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs   =  16 #128 #64 # 32 #16\n",
    "EXPrand  = 1.00 #0.25 #0.5 # 0.75\n",
    "Alpha    = 0.875\n",
    "aMargin  = (pi/180)*15.0;\n",
    "\n",
    "##### Q-Function Hacks #####\n",
    "beta   = 0.15\n",
    "blSode = false\n",
    "blPoch = false\n",
    "\n",
    "##### Eligibility Params #####\n",
    "useElig = false\n",
    "N_peaks =  40\n",
    "N_steps = 200\n",
    "lambda  =   0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e910ca2-281c-4d06-98e2-1c96fa7c1916",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d3689b-947a-400b-9031-9f1a13f4df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0\n",
      "Training Iteration 4 score: 0.47000000000000025, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.7300000000000004, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.17, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.3000000000000001, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.16, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.4400000000000002, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.17, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.3200000000000001, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 1.0200000000000007, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.0, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.22000000000000006, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.13999999999999999, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.0, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.15, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.19000000000000003, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.26000000000000006, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.09999999999999999, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.8500000000000005, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.20000000000000004, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.2900000000000001, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.15, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.0, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.0, epsilon: 0.00390625\n",
      "Average Score: 0.22507812500000016\n",
      "\n",
      "Epoch 2, Best Score: 1.7700000000000014\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.16, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.5100000000000002, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.5900000000000003, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.15, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 2.389999999999993, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.23000000000000007, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.26000000000000006, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 1.5100000000000011, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.5400000000000003, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.26000000000000006, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.5400000000000003, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.09999999999999999, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.0, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.5700000000000003, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.0, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.2900000000000001, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.3200000000000001, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.38000000000000017, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.0, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.10999999999999999, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.13999999999999999, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.6000000000000003, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.23000000000000007, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 1.7100000000000013, epsilon: 0.00390625\n",
      "Average Score: 0.24601562499999996\n",
      "\n",
      "Epoch 3, Best Score: 2.389999999999993\n",
      "Training Iteration 4 score: 0.5200000000000002, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.48000000000000026, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.22000000000000006, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.08, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.10999999999999999, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.23000000000000007, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.21000000000000005, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.8200000000000005, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.34000000000000014, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 2.349999999999994, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 2.859999999999983, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.21000000000000005, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.9800000000000006, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.6600000000000004, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.18000000000000002, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.0, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.0, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.35000000000000014, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.6700000000000004, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.5000000000000002, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.21000000000000005, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.0, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.15, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 1.2300000000000009, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.3200000000000001, epsilon: 0.00390625\n",
      "Average Score: 0.319453125\n",
      "\n",
      "Epoch 4, Best Score: 2.859999999999983\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.0, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.0, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.0, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.0, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.0, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.0, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.0, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.0, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.0, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.0, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.0, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.0, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.0, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.0, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.0, epsilon: 0.00390625\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 5, Best Score: 2.859999999999983\n",
      "Training Iteration 4 score: 1.1500000000000008, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.3000000000000001, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.5900000000000003, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.12999999999999998, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 1.5100000000000011, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.21000000000000005, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.7600000000000005, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.3300000000000001, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.15, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.5400000000000003, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.23000000000000007, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 11.769999999999794, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.0, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.17, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 25.53000000000119, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.08, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.2800000000000001, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 11.139999999999807, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.0, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 11.519999999999799, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.4200000000000002, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.0, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.2700000000000001, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.37000000000000016, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.0, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.4000000000000002, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.0, epsilon: 0.00390625\n",
      "Average Score: 1.6247656250000417\n",
      "\n",
      "Epoch 6, Best Score: 31.28000000000209\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.0, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.0, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.0, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.0, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.0, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.0, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.0, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.0, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.0, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.0, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.0, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.0, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.0, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.0, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.0, epsilon: 0.00390625\n",
      "Average Score: 0.006796875000000001\n",
      "\n",
      "Epoch 7, Best Score: 31.28000000000209\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.5000000000000002, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.0, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.0, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.0, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.0, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.0, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.0, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.0, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.0, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.0, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.0, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.0, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.0, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.0, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.0, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.0, epsilon: 0.00390625\n",
      "Average Score: 0.031875000000000014\n",
      "\n",
      "Epoch 8, Best Score: 31.28000000000209\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.7500000000000004, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.05, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.7600000000000005, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.46000000000000024, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.2800000000000001, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.16, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.36000000000000015, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.6500000000000004, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.07, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.09999999999999999, epsilon: 0.23828125\n",
      "Training Iteration 72 score: 0.09999999999999999, epsilon: 0.22265625\n",
      "Training Iteration 76 score: 0.0, epsilon: 0.20703125\n",
      "Training Iteration 80 score: 0.0, epsilon: 0.19140625\n",
      "Training Iteration 84 score: 0.0, epsilon: 0.17578125\n",
      "Training Iteration 88 score: 0.0, epsilon: 0.16015625\n",
      "Training Iteration 92 score: 0.0, epsilon: 0.14453125\n",
      "Training Iteration 96 score: 0.0, epsilon: 0.12890625\n",
      "Training Iteration 100 score: 0.26000000000000006, epsilon: 0.11328125\n",
      "Training Iteration 104 score: 0.45000000000000023, epsilon: 0.09765625\n",
      "Training Iteration 108 score: 0.0, epsilon: 0.08203125\n",
      "Training Iteration 112 score: 0.45000000000000023, epsilon: 0.06640625\n",
      "Training Iteration 116 score: 0.20000000000000004, epsilon: 0.05078125\n",
      "Training Iteration 120 score: 0.5600000000000003, epsilon: 0.03515625\n",
      "Training Iteration 124 score: 0.0, epsilon: 0.01953125\n",
      "Training Iteration 128 score: 0.0, epsilon: 0.00390625\n",
      "Average Score: 0.12843750000000004\n",
      "\n",
      "Epoch 9, Best Score: 31.28000000000209\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.48828125\n",
      "Training Iteration 8 score: 0.15, epsilon: 0.47265625\n",
      "Training Iteration 12 score: 0.2900000000000001, epsilon: 0.45703125\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.44140625\n",
      "Training Iteration 20 score: 0.17, epsilon: 0.42578125\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.41015625\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.39453125\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.37890625\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.36328125\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.34765625\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.33203125\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.31640625\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.30078125\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.28515625\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.26953125\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.25390625\n",
      "Training Iteration 68 score: 0.0, epsilon: 0.23828125\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] cartpole_dyn(X::Vector{Float64}, A::Float64, ts::Float64)",
      "   @ Main ~/CogArch/CORVID/Contin_Q-Learning/env_cartpole.jl:81",
      " [2] learned_action_for_state(X::Vector{Float64}, domain::Matrix{Float64}, res::Vector{Float64}, ts::Float64)",
      "   @ Main ./In[3]:129",
      " [3] top-level scope",
      "   @ ./In[11]:39",
      " [4] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [5] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "bgn       = time()\n",
    "averages  = []\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    if blSode\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    elseif blPoch\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    else\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    end\n",
    "    \n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/episodes\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        X  = X_0\n",
    "        \n",
    "        ##### Double Q-Learning ###########################################\n",
    "\n",
    "        for k = 1:T\n",
    "\n",
    "            # 1. Choose action\n",
    "            if rand() < epsilon\n",
    "                if rand() < EXPrand \n",
    "                    A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                else\n",
    "                    A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                end\n",
    "            else\n",
    "\n",
    "                A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                if A == 1000.0 # Indicates no values in this region\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # 2. Cache last state\n",
    "            qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "            # 3. Generate the next stae\n",
    "            Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "            # 4. Collect reward R( s, a, s' )\n",
    "            R_t = cartpole_reward( Xp )\n",
    "\n",
    "            # 5. Get the optimal action at the next state\n",
    "            a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "\n",
    "            # 6. Compute the value at the next state\n",
    "\n",
    "            V_tp1_opt = query_value_fuzzy( \n",
    "                Q_kdTree, G, V, \n",
    "                get_Q( \n",
    "                    select_X_vector( Xp ), \n",
    "                    a_tp1_opt \n",
    "                ); \n",
    "                k = vNN \n",
    "            )\n",
    "            if isnan( V_tp1_opt )\n",
    "                V_tp1_opt = 0.0\n",
    "            end\n",
    "\n",
    "\n",
    "            # 7. Blend the value back into nearest points\n",
    "\n",
    "            idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "            nNear      = size( idxs, 1 )\n",
    "            for i = 1:nNear\n",
    "                j    = idxs[i]\n",
    "                if !isnan( wgts[i] ) \n",
    "\n",
    "                    # VS[j] = R_t + gamma * V_tp1_opt # Q-Learning\n",
    "                    VS[j] = VS[j] + alpha*( R_t + gamma*V_tp1_opt - V[j] ) # Q(TD)-Learning\n",
    "                    \n",
    "                end\n",
    "            end\n",
    "\n",
    "            states[:,k] = Xp\n",
    "            actions[k]  = A\n",
    "\n",
    "            X = Xp\n",
    "        end\n",
    "\n",
    "        s_l    = vertical_score_s( states, aMargin, ts )\n",
    "        s_Totl += s_l\n",
    "    \n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        ##### Eligibility Traces ##########################################\n",
    "        if useElig\n",
    "        \n",
    "            # 1. Find `N_peaks`\n",
    "            peakDices = find_state_history_R_peaks( states, N_peaks )\n",
    "            # 2. For each peak, iterate back in time through states\n",
    "            for ii = 1:min(N_peaks, length(peakDices))\n",
    "                topDex = peakDices[ ii ]\n",
    "                X      = states[:,topDex]\n",
    "                R_jj    = cartpole_reward( X )\n",
    "                # 3. For each Q-state in the trace\n",
    "                for jj = (topDex-1):-1:max(1,topDex-N_steps)\n",
    "                    X = states[:,jj]\n",
    "                    R_jj *= lambda\n",
    "                    a_jj = actions[jj]\n",
    "                    q_jj = get_Q( select_X_vector( X ), a_jj )\n",
    "                    V_jj = query_value_fuzzy( Q_kdTree, G, V, q_jj; k = vNN )\n",
    "\n",
    "                    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, q_jj; k = bNN )\n",
    "                    nNear      = size( idxs, 1 )\n",
    "\n",
    "                    for kk = 1:nNear\n",
    "                        ll = idxs[kk]\n",
    "                        if !isnan( wgts[kk] ) \n",
    "                            VS[ll] = VS[ll] + alpha*( R_jj + V_jj - V[ll] ) # Q(TD)-Learning\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        # Decay the exploration probability\n",
    "        epsilon -= deltaEp\n",
    "        \n",
    "        \n",
    "        ##### Double Q-Learning ##########################################\n",
    "        # Every `swapDiv` episodes, swap Q-functions for Double Q-Learning\n",
    "        \n",
    "        if (l % swapDiv == 0)\n",
    "            \n",
    "            vSwp = copy( VS   )\n",
    "            VS   = copy( V    )\n",
    "            V    = copy( vSwp )\n",
    "            # println(\"SWAP\")\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "     \n",
    "    \n",
    "    ##### Q-Function Hacks ################################################\n",
    "    \n",
    "    # Blend Method 1: Best Episode\n",
    "    if blSode\n",
    "        V  = blend_alpha_of_A_into_B( beta, vBst, V  )\n",
    "        VS = blend_alpha_of_A_into_B( beta, vBst, VS )\n",
    "    end\n",
    "    \n",
    "    # if (s_Avg > bestAvg) && true\n",
    "    #     println( \"BLEND\" )\n",
    "    #     bestAvg = s_Avg\n",
    "    #     vBAv    = copy( V ) # Try a blend of both next # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    #     vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V ) # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    # end\n",
    "        \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709555b9-2598-4281-a634-c7b0681277d0",
   "metadata": {},
   "source": [
    "# Method 2 Performance, Average Vertical Duration [s]\n",
    "Each score is the best average score of the last two epochs: 32 epochs of 64 episodes each, Q-function swap after every episode \n",
    "\n",
    "### TD Tuning\n",
    "\n",
    "$\\gamma = 1.00$  \n",
    "\n",
    "| Param                |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|----------------------|------| ------- | ------- | ------- | ------- | ------- |------| ----- |\n",
    "| $$\\alpha = 0.99$$    |&nbsp;| 0.251   | 0.198   | 0.146   | 0.147   | 0.210   |&nbsp;| 0.190 |\n",
    "| $$\\alpha = 0.75$$    |&nbsp;| 0.179   | 0.185   | 0.239   | 0.179   | 0.175   |&nbsp;| 0.191 |\n",
    "| $$\\alpha = 0.50$$    |&nbsp;| 0.204   | 0.100   | 0.238   | 0.158   | 0.139   |&nbsp;| 0.168 |\n",
    "| $$\\alpha = 0.25$$    |&nbsp;| 0.294   | 0.170   | 0.107   | 0.223   | 0.147   |&nbsp;| 0.188 |\n",
    "| $$\\alpha = 0.125$$   |&nbsp;| 0.187   | 0.254   | 0.177   | 0.163   | 0.204   |&nbsp;| 0.197 |  \n",
    "| $$\\alpha = 0.0625$$  |&nbsp;| 0.113   | 0.241   | 0.353   | 0.134   | 0.749   |&nbsp;| 0.318 |\n",
    "| $$\\alpha = 0.03125$$ |&nbsp;| 0.231   | 0.322   | 0.018   | 0.098   | 0.000   |&nbsp;| 0.134 |\n",
    "| $$\\alpha = 0.02344$$ |&nbsp;| 1.289   | 0.119   | 0.380   | 0.168   | 0.086   |&nbsp;| 0.408 |\n",
    "| $$\\alpha = \\mathbf{0.02148}$$ |&nbsp;| 0.498   | 0.813   | 0.286   | 7.130   | 0.281   |&nbsp;| **1.802** |\n",
    "| $$\\alpha = 0.01953$$ |&nbsp;| 0.234   | 0.113   | 0.445   | 0.119   | 1.637   |&nbsp;| 0.510 |\n",
    "| $$\\alpha = 0.01758$$ |&nbsp;| 0.175   | 0.249   | 0.217   | 0.047   | 1.006   |&nbsp;| 0.339 |\n",
    "| $$\\alpha = 0.01563$$ |&nbsp;| 0.281   | 1.371   | 0.066   | 0.037   | 0.751   |&nbsp;| 0.501 |\n",
    "| $$\\alpha = 0.00782$$ |&nbsp;| 0.133   | 0.241   | 0.149   | 0.493   | 0.146   |&nbsp;| 0.232 |\n",
    "| $$\\alpha = 0.00391$$ |&nbsp;| 0.037   | 0.626   | 1.000   | 0.525   | 0.139   |&nbsp;| 0.465 |\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### $\\gamma$ Tuning\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "\n",
    "| Param                 |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "| :-------------------- |------| ------- | ------- | ------- | ------- | ------- |------| ----- |\n",
    "| $$\\gamma = 0.999$$    |&nbsp;| 0.925   | 0.247   | 0.145   | 0.364   | 3.038   |&nbsp;| 0.944 |\n",
    "| $$\\gamma = 0.99$$     |&nbsp;| 0.011   | 0.448   | 0.453   | 0.915   | 0.013   |&nbsp;| 0.368 |\n",
    "| $$\\gamma = 0.85$$     |&nbsp;| 0.314   | 2.778   | 0.275   | 1.183   | 0.079   |&nbsp;| 0.926 |\n",
    "| $$\\gamma = 0.80$$     |&nbsp;| 0.082   | 0.033   | 0.173   | 0.251   | 1.741   |&nbsp;| 0.456 |\n",
    "| $$\\gamma = 0.75$$     |&nbsp;| 0.283   | 0.239   | 2.223   | 0.264   | 0.753   |&nbsp;| 0.752 |\n",
    "| $$\\gamma = 0.50$$     |&nbsp;| 0.167   | 0.289   | 0.474   | 0.266   | 0.230   |&nbsp;| 0.285 |\n",
    "\n",
    " \n",
    "### Double-Q Tuning, Swap Evey N Episodes\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "Epochs = 32\n",
    "\n",
    "| Param                 |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|-----------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| $$\\%\\ \\ 2$$           |&nbsp;|  0.570  | 0.132   | 1.053   | 0.731   | 0.900   |&nbsp;| 0.677 |\n",
    "| $$\\%\\ \\ 4$$           |&nbsp;|  1.313  | 0.087   | 2.282   | 0.417   | 0.409   |&nbsp;| 0.901 |\n",
    "| $$\\%\\ \\ 8$$           |&nbsp;|  0.097  | 0.040   | 0.621   | 0.030   | 0.608   |&nbsp;| 0.279 |\n",
    "| $$\\%16$$              |&nbsp;|  0.260  | 0.219   | 0.054   | 0.407   | 0.845   |&nbsp;| 0.357 |\n",
    "| $$\\%32$$              |&nbsp;|  0.674  | 0.130   | 0.301   | 0.286   | 0.313   |&nbsp;| 0.341 |\n",
    "| $$\\%\\mathbf{64}$$     |&nbsp;| 15.261  | 2.072   | 0.380   | 0.056   | 0.727   |&nbsp;| **3.699** |\n",
    "  \n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "Episodes = 128\n",
    "Epochs = 16\n",
    "\n",
    "| Param            |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| $$\\%\\ \\ \\ \\ 4$$  |&nbsp;|  1.333  | 0.117   | 0.138   | 0.430   |         |&nbsp;|       |\n",
    "| $$\\%\\ \\ 64$$     |&nbsp;|  0.166  | 0.110   | 0.240   | 1.615   |         |&nbsp;|       |\n",
    "| $$\\%128$$        |&nbsp;|  2.700  | 0.228   | 0.222   | 0.183   |         |&nbsp;|       |\n",
    "\n",
    "  \n",
    "### Trace Tuning\n",
    "\n",
    "\n",
    "### Blend: Best Episode\n",
    "\n",
    "$\\beta = 0.07$:  \n",
    "$\\beta = 0.15$: 0.244\n",
    "\n",
    "| Method      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 | Mean |\n",
    "| ----------- | ------- | ------- | ------- | ------- | ------- | ---- |\n",
    "| Blend (Epi) |         |         |         |         |         |      |\n",
    "| Blend (Epo) |         |         |         |         |         |      |\n",
    "| TD          |         |         |         |         |         |      |\n",
    "| TD  + ????? |         |         |         |         |         |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c1d8a-58c5-4719-89c8-b69bf6623266",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`play -nq -t alsa synth 3 sine 300`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5ade7-5f94-43b1-837f-85ccdd6b5c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
