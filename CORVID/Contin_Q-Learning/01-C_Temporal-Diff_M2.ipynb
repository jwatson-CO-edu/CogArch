{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cefc7-7c60-4838-9399-26a98ec9736e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43290374-89de-4616-8800-c86799248c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "using DataStructures\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851743ab-a511-40fb-850b-bf90efa9232d",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39765-4abe-409a-bea1-f44fa8ec2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              -10.0 +10.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf920d4-46af-4f22-8933-c3db011ff716",
   "metadata": {},
   "source": [
    "# Q-Learning Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f605b904-b397-4617-9dbe-a27c0b4fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Return the indices and scores of all the peak rewards in the data\n",
    "\"\"\"\n",
    "function find_state_history_R_peaks( X_hist, N_pks )\n",
    "    \n",
    "    epLen   = size( X_hist, 2 )\n",
    "    rising  = false\n",
    "    lastVal = 1e9\n",
    "    lastRis = false\n",
    "    pqPeaks = PriorityQueue();\n",
    "    rtnPeak = []\n",
    "    \n",
    "    for j = 1:epLen\n",
    "        X       = X_hist[:,j]\n",
    "        currVal = cartpole_reward( X )\n",
    "        rising  = (currVal > lastVal)\n",
    "        if (!rising) && lastRis\n",
    "            pqPeaks[j] = -currVal # Store the current index at its current (negative) value\n",
    "        end\n",
    "        lastVal = currVal\n",
    "        lastRis = rising\n",
    "    end\n",
    "    for i = 1:min( N_pks, length( pqPeaks ) )\n",
    "        append!( rtnPeak, dequeue!( pqPeaks ) )\n",
    "    end\n",
    "    \n",
    "    return rtnPeak;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d663e-1ccd-441f-807f-44f84a43e4d0",
   "metadata": {},
   "source": [
    "# Q-Function Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91f06c-df14-4fe7-b81d-12c3184b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721c7-88a9-4b57-bf9f-ad9f9acbf786",
   "metadata": {},
   "source": [
    "# CartPole Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc4097d-9b96-453c-ba4f-4b06fce7fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083b48-38dc-4616-979a-da8874303d32",
   "metadata": {},
   "source": [
    "# Agent Data Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f648d5-8d8e-4da4-bd1e-3f3d9ec7c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres     = Fmax/Fdiv\n",
    "spaceDiv = 4.0 # 1.0 # 2.0 # 5.0 # 7.5  \n",
    "\n",
    "### Construct grid of anchors ###\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ spaceDiv, spaceDiv, spaceDiv, spaceDiv, Fres ] );\n",
    "nPts = size( G )[2]; # ------- Number of anchors\n",
    "mDim = size( G )[1]; # ------- Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db1609-9df1-438b-9675-0286bf01a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states  = zeros( size( X_0, 1 ), T )\n",
    "actions = zeros( T );\n",
    "bestXs  = zeros( size( X_0, 1 ), T )\n",
    "bestAs  = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9f1ef-79bc-41fd-b6e9-ab0554460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d49b4c6-8353-4a01-8a16-9b544e1ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c954412-18b9-45a8-97a6-e61cf19f15d2",
   "metadata": {},
   "source": [
    "# Agent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358ff3d-44a5-491e-9597-0a0a73c6b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Q(TD)-Learning Params #####\n",
    "scale = 7.5; #1.650; # ----------- scale\n",
    "vNN   =  4 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\"\n",
    "\n",
    "alpha    = 0.02148 # 0.99 # 0.75 # 0.5 # 0.25 # 0.125 # 0.0625 # 0.03125 # 0.015625 # 0.00782 # 0.00391\n",
    "gamma    = 1.00 \n",
    "swapDiv  = 64\n",
    "epsMin   = 0.00 # Last iter is policy eval\n",
    "epsMax   = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes = 64 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs   = 32 #128 #64 # 32 #16\n",
    "EXPrand  = 1.00 #0.25 #0.5 # 0.75\n",
    "Alpha    = 0.875\n",
    "aMargin  = (pi/180)*15.0;\n",
    "\n",
    "##### Q-Function Hacks #####\n",
    "beta   = 0.15\n",
    "blSode = false\n",
    "blPoch = false\n",
    "\n",
    "##### Eligibility Params #####\n",
    "useElig = true\n",
    "N_peaks =  32\n",
    "N_steps = 192\n",
    "lambda  =   0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e910ca2-281c-4d06-98e2-1c96fa7c1916",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d3689b-947a-400b-9031-9f1a13f4df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0\n",
      "Training Iteration 4 score: 0.3000000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.16, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.18000000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.6900000000000004, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.2800000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.6500000000000004, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.19000000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 1.1800000000000008, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 1.1600000000000008, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.2451562500000002\n",
      "\n",
      "Epoch 2, Best Score: 1.6900000000000013\n",
      "Training Iteration 4 score: 0.5300000000000002, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.5100000000000002, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.12999999999999998, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.10999999999999999, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.15, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.8900000000000006, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.12999999999999998, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.5300000000000002, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.11999999999999998, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.9500000000000006, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.22000000000000006, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.48000000000000026, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.37000000000000016, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.42015624999999834\n",
      "\n",
      "Epoch 3, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 4, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.5700000000000003, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.6200000000000003, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.5800000000000003, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.060000000000000005, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.24000000000000007, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.11999999999999998, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.17, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.18000000000000002, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.6300000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.38000000000000017, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.6500000000000004, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.30062500000000014\n",
      "\n",
      "Epoch 5, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.20000000000000004, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.12999999999999998, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.09999999999999999, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.05421874999999999\n",
      "\n",
      "Epoch 6, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.4000000000000002, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.34000000000000014, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.16, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.09, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.08, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.08, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.20000000000000004, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.26000000000000006, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.20000000000000004, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.11999999999999998, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.12999999999999998, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.15, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.16734375000000004\n",
      "\n",
      "Epoch 7, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.00515625\n",
      "\n",
      "Epoch 8, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 9, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.9700000000000006, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.3900000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.22000000000000006, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.21000000000000005, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.260000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.060000000000000005, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.4200000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.46000000000000024, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.9900000000000007, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.6400000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 1.0900000000000007, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.7500000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.41046875000000027\n",
      "\n",
      "Epoch 10, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.35000000000000014, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.35000000000000014, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.7800000000000005, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.7700000000000005, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.6000000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.21000000000000005, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.45000000000000023, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.15, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.04, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.4200000000000002, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.16, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.39468750000000014\n",
      "\n",
      "Epoch 11, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.37000000000000016, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.06500000000000003\n",
      "\n",
      "Epoch 12, Best Score: 6.949999999999896\n",
      "Training Iteration 4 score: 0.3200000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 28.160000000001602, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 30.31000000000194, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.34000000000000014, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 27.520000000001502, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 31.14000000000207, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 29.450000000001804, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 25.26000000000115, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 30.410000000001954, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.7300000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 21.670000000000588, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 21.010000000000485, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.8200000000000005, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 14.525625000000703\n",
      "\n",
      "Epoch 13, Best Score: 32.92000000000202\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.5300000000000011, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.03703125000000002\n",
      "\n",
      "Epoch 14, Best Score: 32.92000000000202\n",
      "Training Iteration 4 score: 0.3000000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.36000000000000015, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 1.260000000000001, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.3300000000000001, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.2800000000000001, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.8900000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.2900000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.5500000000000003, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 1.320000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.2700000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.8700000000000006, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.7200000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.9000000000000006, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 1.2500000000000009, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 1.2100000000000009, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.8800000000000006, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.8639062500000004\n",
      "\n",
      "Epoch 15, Best Score: 32.92000000000202\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.8400000000000014, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 1.5200000000000011, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 1.7200000000000013, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.46546875\n",
      "\n",
      "Epoch 16, Best Score: 32.92000000000202\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.1800000000000008, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5600000000000003, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.13999999999999999, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.9300000000000006, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.3300000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.14343750000000008\n",
      "\n",
      "Epoch 17, Best Score: 32.92000000000202\n",
      "Training Iteration 4 score: 1.0900000000000007, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.0700000000000007, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5500000000000003, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 5.799999999999921, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.6400000000000003, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 9.339999999999845, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 2.5199999999999902, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 22.64000000000074, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.25000000000000006, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 12.689999999999774, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.5700000000000012, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 3.659999999999966, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 3.609999999999967, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.2800000000000001, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.2700000000000001, epsilon: 8.881784197001252e-16\n",
      "Average Score: 3.723906250000116\n",
      "\n",
      "Epoch 18, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.4300000000000002, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.2700000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5200000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.9600000000000006, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.7100000000000004, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.5000000000000002, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.2800000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.36000000000000015, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.6200000000000003, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.2900000000000001, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.35000000000000014, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.5800000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.5300000000000002, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.3932812500000003\n",
      "\n",
      "Epoch 19, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.24000000000000007, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.3000000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5200000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.1700000000000008, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.7700000000000005, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 25.27000000000115, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.2800000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.2700000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.3200000000000001, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.3200000000000001, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.45000000000000023, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.26000000000000006, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.26000000000000006, epsilon: 8.881784197001252e-16\n",
      "Average Score: 2.204531250000036\n",
      "\n",
      "Epoch 20, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.4100000000000002, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.20000000000000004, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.25000000000000006, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.38000000000000017, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.3200000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.25000000000000006, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.4300000000000002, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 25.110000000001126, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.5925000000000177\n",
      "\n",
      "Epoch 21, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.3100000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.46000000000000024, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5800000000000003, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 2.489999999999991, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 5.479999999999928, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.4200000000000002, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.3900000000000002, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.4200000000000002, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.3900000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.4400000000000002, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.3300000000000001, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.34000000000000014, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.5600000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.36000000000000015, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.2800000000000001, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.2900000000000001, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.6715624999999981\n",
      "\n",
      "Epoch 22, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 1.330000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.34000000000000014, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.37000000000000016, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.36000000000000015, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.2700000000000001, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.8200000000000005, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.3200000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.5100000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.2900000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.49000000000000027, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.5200000000000002, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.35000000000000014, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.23000000000000007, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.21000000000000005, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.21000000000000005, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.3981250000000002\n",
      "\n",
      "Epoch 23, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.18000000000000002, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.18000000000000002, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.23000000000000007, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.22000000000000006, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.26000000000000006, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.24000000000000007, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.23000000000000007, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.24000000000000007, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.21000000000000005, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.1523437500000001\n",
      "\n",
      "Epoch 24, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.6200000000000003, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.6800000000000004, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.5900000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.6300000000000003, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.37000000000000016, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.2800000000000001, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.13999999999999999, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 2.3299999999999943, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.2998437500000001\n",
      "\n",
      "Epoch 25, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.2800000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.37000000000000016, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.20000000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.6300000000000003, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.34000000000000014, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.36000000000000015, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.3100000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.45000000000000023, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.5100000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.5100000000000002, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.22000000000000006, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.6900000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.5500000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.25000000000000006, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.24000000000000007, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.23000000000000007, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.4150000000000002\n",
      "\n",
      "Epoch 26, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.2700000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.8400000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.5100000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.4400000000000002, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.47000000000000025, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.2900000000000001, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.2900000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.4100000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.8800000000000006, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.5700000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.8000000000000005, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.5700000000000003, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.7400000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.5800000000000003, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.49000000000000044\n",
      "\n",
      "Epoch 27, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.34000000000000014, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.19000000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.09, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.026562500000000006\n",
      "\n",
      "Epoch 28, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 1.1100000000000008, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.6700000000000004, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 4.249999999999954, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.47000000000000025, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 2.439999999999992, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.3100000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 4.549999999999947, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 2.2799999999999954, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 2.7799999999999847, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 1.9600000000000015, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 1.8700000000000014, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 1.1028124999999935\n",
      "\n",
      "Epoch 29, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.5400000000000003, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.6500000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.5900000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.09125000000000007\n",
      "\n",
      "Epoch 30, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 31, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.030937500000000017\n",
      "\n",
      "Epoch 32, Best Score: 33.79000000000185\n",
      "Training Iteration 4 score: 0.21000000000000005, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.3000000000000001, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.25000000000000006, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.3200000000000001, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.22000000000000006, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.15, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.12796875000000008\n",
      "Saved a trained Q-table with size (76032,), After 13.769614581267039 minutes of training!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip060\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip060)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip061\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip060)\" d=\"\n",
       "M137.362 1486.45 L2352.76 1486.45 L2352.76 47.2441 L137.362 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip062\">\n",
       "    <rect x=\"137\" y=\"47\" width=\"2216\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  469.739,1486.45 469.739,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  806.835,1486.45 806.835,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1143.93,1486.45 1143.93,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1481.03,1486.45 1481.03,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1818.12,1486.45 1818.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2155.22,1486.45 2155.22,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  469.739,1486.45 469.739,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  806.835,1486.45 806.835,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1143.93,1486.45 1143.93,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1481.03,1486.45 1481.03,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1818.12,1486.45 1818.12,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2155.22,1486.45 2155.22,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M460.017 1514.29 L478.373 1514.29 L478.373 1518.22 L464.299 1518.22 L464.299 1526.7 Q465.317 1526.35 466.336 1526.19 Q467.354 1526 468.373 1526 Q474.16 1526 477.54 1529.17 Q480.919 1532.34 480.919 1537.76 Q480.919 1543.34 477.447 1546.44 Q473.975 1549.52 467.655 1549.52 Q465.479 1549.52 463.211 1549.15 Q460.966 1548.78 458.558 1548.04 L458.558 1543.34 Q460.642 1544.47 462.864 1545.03 Q465.086 1545.58 467.563 1545.58 Q471.567 1545.58 473.905 1543.48 Q476.243 1541.37 476.243 1537.76 Q476.243 1534.15 473.905 1532.04 Q471.567 1529.94 467.563 1529.94 Q465.688 1529.94 463.813 1530.35 Q461.961 1530.77 460.017 1531.65 L460.017 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M781.522 1544.91 L789.161 1544.91 L789.161 1518.55 L780.851 1520.21 L780.851 1515.95 L789.115 1514.29 L793.791 1514.29 L793.791 1544.91 L801.429 1544.91 L801.429 1548.85 L781.522 1548.85 L781.522 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M820.874 1517.37 Q817.263 1517.37 815.434 1520.93 Q813.628 1524.47 813.628 1531.6 Q813.628 1538.71 815.434 1542.27 Q817.263 1545.82 820.874 1545.82 Q824.508 1545.82 826.314 1542.27 Q828.142 1538.71 828.142 1531.6 Q828.142 1524.47 826.314 1520.93 Q824.508 1517.37 820.874 1517.37 M820.874 1513.66 Q826.684 1513.66 829.739 1518.27 Q832.818 1522.85 832.818 1531.6 Q832.818 1540.33 829.739 1544.94 Q826.684 1549.52 820.874 1549.52 Q815.064 1549.52 811.985 1544.94 Q808.929 1540.33 808.929 1531.6 Q808.929 1522.85 811.985 1518.27 Q815.064 1513.66 820.874 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1119.12 1544.91 L1126.75 1544.91 L1126.75 1518.55 L1118.44 1520.21 L1118.44 1515.95 L1126.71 1514.29 L1131.38 1514.29 L1131.38 1544.91 L1139.02 1544.91 L1139.02 1548.85 L1119.12 1548.85 L1119.12 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1148.51 1514.29 L1166.87 1514.29 L1166.87 1518.22 L1152.8 1518.22 L1152.8 1526.7 Q1153.81 1526.35 1154.83 1526.19 Q1155.85 1526 1156.87 1526 Q1162.66 1526 1166.04 1529.17 Q1169.42 1532.34 1169.42 1537.76 Q1169.42 1543.34 1165.94 1546.44 Q1162.47 1549.52 1156.15 1549.52 Q1153.98 1549.52 1151.71 1549.15 Q1149.46 1548.78 1147.06 1548.04 L1147.06 1543.34 Q1149.14 1544.47 1151.36 1545.03 Q1153.58 1545.58 1156.06 1545.58 Q1160.06 1545.58 1162.4 1543.48 Q1164.74 1541.37 1164.74 1537.76 Q1164.74 1534.15 1162.4 1532.04 Q1160.06 1529.94 1156.06 1529.94 Q1154.18 1529.94 1152.31 1530.35 Q1150.46 1530.77 1148.51 1531.65 L1148.51 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1459.8 1544.91 L1476.12 1544.91 L1476.12 1548.85 L1454.17 1548.85 L1454.17 1544.91 Q1456.84 1542.16 1461.42 1537.53 Q1466.03 1532.88 1467.21 1531.53 Q1469.45 1529.01 1470.33 1527.27 Q1471.23 1525.51 1471.23 1523.82 Q1471.23 1521.07 1469.29 1519.33 Q1467.37 1517.6 1464.27 1517.6 Q1462.07 1517.6 1459.61 1518.36 Q1457.18 1519.13 1454.41 1520.68 L1454.41 1515.95 Q1457.23 1514.82 1459.68 1514.24 Q1462.14 1513.66 1464.17 1513.66 Q1469.54 1513.66 1472.74 1516.35 Q1475.93 1519.03 1475.93 1523.52 Q1475.93 1525.65 1475.12 1527.57 Q1474.34 1529.47 1472.23 1532.07 Q1471.65 1532.74 1468.55 1535.95 Q1465.45 1539.15 1459.8 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1495.93 1517.37 Q1492.32 1517.37 1490.49 1520.93 Q1488.69 1524.47 1488.69 1531.6 Q1488.69 1538.71 1490.49 1542.27 Q1492.32 1545.82 1495.93 1545.82 Q1499.57 1545.82 1501.37 1542.27 Q1503.2 1538.71 1503.2 1531.6 Q1503.2 1524.47 1501.37 1520.93 Q1499.57 1517.37 1495.93 1517.37 M1495.93 1513.66 Q1501.74 1513.66 1504.8 1518.27 Q1507.88 1522.85 1507.88 1531.6 Q1507.88 1540.33 1504.8 1544.94 Q1501.74 1549.52 1495.93 1549.52 Q1490.12 1549.52 1487.04 1544.94 Q1483.99 1540.33 1483.99 1531.6 Q1483.99 1522.85 1487.04 1518.27 Q1490.12 1513.66 1495.93 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1797.39 1544.91 L1813.71 1544.91 L1813.71 1548.85 L1791.77 1548.85 L1791.77 1544.91 Q1794.43 1542.16 1799.01 1537.53 Q1803.62 1532.88 1804.8 1531.53 Q1807.05 1529.01 1807.93 1527.27 Q1808.83 1525.51 1808.83 1523.82 Q1808.83 1521.07 1806.88 1519.33 Q1804.96 1517.6 1801.86 1517.6 Q1799.66 1517.6 1797.21 1518.36 Q1794.78 1519.13 1792 1520.68 L1792 1515.95 Q1794.82 1514.82 1797.28 1514.24 Q1799.73 1513.66 1801.77 1513.66 Q1807.14 1513.66 1810.33 1516.35 Q1813.53 1519.03 1813.53 1523.52 Q1813.53 1525.65 1812.72 1527.57 Q1811.93 1529.47 1809.82 1532.07 Q1809.24 1532.74 1806.14 1535.95 Q1803.04 1539.15 1797.39 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M1823.57 1514.29 L1841.93 1514.29 L1841.93 1518.22 L1827.86 1518.22 L1827.86 1526.7 Q1828.87 1526.35 1829.89 1526.19 Q1830.91 1526 1831.93 1526 Q1837.72 1526 1841.1 1529.17 Q1844.48 1532.34 1844.48 1537.76 Q1844.48 1543.34 1841 1546.44 Q1837.53 1549.52 1831.21 1549.52 Q1829.04 1549.52 1826.77 1549.15 Q1824.52 1548.78 1822.11 1548.04 L1822.11 1543.34 Q1824.2 1544.47 1826.42 1545.03 Q1828.64 1545.58 1831.12 1545.58 Q1835.12 1545.58 1837.46 1543.48 Q1839.8 1541.37 1839.8 1537.76 Q1839.8 1534.15 1837.46 1532.04 Q1835.12 1529.94 1831.12 1529.94 Q1829.24 1529.94 1827.37 1530.35 Q1825.52 1530.77 1823.57 1531.65 L1823.57 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2144.06 1530.21 Q2147.42 1530.93 2149.29 1533.2 Q2151.19 1535.47 2151.19 1538.8 Q2151.19 1543.92 2147.67 1546.72 Q2144.15 1549.52 2137.67 1549.52 Q2135.5 1549.52 2133.18 1549.08 Q2130.89 1548.66 2128.44 1547.81 L2128.44 1543.29 Q2130.38 1544.43 2132.69 1545.01 Q2135.01 1545.58 2137.53 1545.58 Q2141.93 1545.58 2144.22 1543.85 Q2146.54 1542.11 2146.54 1538.8 Q2146.54 1535.75 2144.38 1534.03 Q2142.25 1532.3 2138.44 1532.3 L2134.41 1532.3 L2134.41 1528.45 L2138.62 1528.45 Q2142.07 1528.45 2143.9 1527.09 Q2145.73 1525.7 2145.73 1523.11 Q2145.73 1520.45 2143.83 1519.03 Q2141.95 1517.6 2138.44 1517.6 Q2136.51 1517.6 2134.32 1518.01 Q2132.12 1518.43 2129.48 1519.31 L2129.48 1515.14 Q2132.14 1514.4 2134.45 1514.03 Q2136.79 1513.66 2138.85 1513.66 Q2144.18 1513.66 2147.28 1516.09 Q2150.38 1518.5 2150.38 1522.62 Q2150.38 1525.49 2148.74 1527.48 Q2147.09 1529.45 2144.06 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2170.06 1517.37 Q2166.44 1517.37 2164.62 1520.93 Q2162.81 1524.47 2162.81 1531.6 Q2162.81 1538.71 2164.62 1542.27 Q2166.44 1545.82 2170.06 1545.82 Q2173.69 1545.82 2175.5 1542.27 Q2177.32 1538.71 2177.32 1531.6 Q2177.32 1524.47 2175.5 1520.93 Q2173.69 1517.37 2170.06 1517.37 M2170.06 1513.66 Q2175.87 1513.66 2178.92 1518.27 Q2182 1522.85 2182 1531.6 Q2182 1540.33 2178.92 1544.94 Q2175.87 1549.52 2170.06 1549.52 Q2164.25 1549.52 2161.17 1544.94 Q2158.11 1540.33 2158.11 1531.6 Q2158.11 1522.85 2161.17 1518.27 Q2164.25 1513.66 2170.06 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  137.362,1445.72 2352.76,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  137.362,1165.3 2352.76,1165.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  137.362,884.884 2352.76,884.884 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  137.362,604.468 2352.76,604.468 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip062)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  137.362,324.052 2352.76,324.052 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,1486.45 137.362,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,1445.72 156.26,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,1165.3 156.26,1165.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,884.884 156.26,884.884 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,604.468 156.26,604.468 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  137.362,324.052 156.26,324.052 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M89.4178 1431.51 Q85.8068 1431.51 83.9781 1435.08 Q82.1725 1438.62 82.1725 1445.75 Q82.1725 1452.86 83.9781 1456.42 Q85.8068 1459.96 89.4178 1459.96 Q93.0521 1459.96 94.8576 1456.42 Q96.6863 1452.86 96.6863 1445.75 Q96.6863 1438.62 94.8576 1435.08 Q93.0521 1431.51 89.4178 1431.51 M89.4178 1427.81 Q95.228 1427.81 98.2835 1432.42 Q101.362 1437 101.362 1445.75 Q101.362 1454.48 98.2835 1459.08 Q95.228 1463.67 89.4178 1463.67 Q83.6077 1463.67 80.529 1459.08 Q77.4735 1454.48 77.4735 1445.75 Q77.4735 1437 80.529 1432.42 Q83.6077 1427.81 89.4178 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M94.2326 1163.95 Q97.5891 1164.66 99.4641 1166.93 Q101.362 1169.2 101.362 1172.53 Q101.362 1177.65 97.8437 1180.45 Q94.3252 1183.25 87.8438 1183.25 Q85.6679 1183.25 83.3531 1182.81 Q81.0614 1182.39 78.6077 1181.54 L78.6077 1177.02 Q80.5522 1178.16 82.867 1178.74 Q85.1818 1179.32 87.7049 1179.32 Q92.103 1179.32 94.3947 1177.58 Q96.7095 1175.84 96.7095 1172.53 Q96.7095 1169.48 94.5567 1167.76 Q92.4271 1166.03 88.6077 1166.03 L84.5799 1166.03 L84.5799 1162.19 L88.7928 1162.19 Q92.2419 1162.19 94.0706 1160.82 Q95.8993 1159.43 95.8993 1156.84 Q95.8993 1154.18 94.0012 1152.76 Q92.1262 1151.33 88.6077 1151.33 Q86.6864 1151.33 84.4873 1151.75 Q82.2883 1152.16 79.6494 1153.04 L79.6494 1148.88 Q82.3114 1148.14 84.6262 1147.77 Q86.9642 1147.39 89.0243 1147.39 Q94.3484 1147.39 97.4502 1149.83 Q100.552 1152.23 100.552 1156.35 Q100.552 1159.22 98.9085 1161.21 Q97.265 1163.18 94.2326 1163.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M89.8345 883.02 Q86.6864 883.02 84.8345 885.173 Q83.0058 887.326 83.0058 891.076 Q83.0058 894.803 84.8345 896.978 Q86.6864 899.131 89.8345 899.131 Q92.9826 899.131 94.8113 896.978 Q96.6632 894.803 96.6632 891.076 Q96.6632 887.326 94.8113 885.173 Q92.9826 883.02 89.8345 883.02 M99.1169 868.367 L99.1169 872.627 Q97.3576 871.793 95.5521 871.354 Q93.7697 870.914 92.0104 870.914 Q87.3808 870.914 84.9271 874.039 Q82.4966 877.164 82.1494 883.483 Q83.5151 881.469 85.5753 880.404 Q87.6354 879.317 90.1123 879.317 Q95.3206 879.317 98.3298 882.488 Q101.362 885.636 101.362 891.076 Q101.362 896.4 98.2141 899.617 Q95.066 902.835 89.8345 902.835 Q83.8392 902.835 80.6679 898.252 Q77.4966 893.645 77.4966 884.918 Q77.4966 876.724 81.3855 871.863 Q85.2743 866.979 91.8252 866.979 Q93.5845 866.979 95.3669 867.326 Q97.1724 867.673 99.1169 868.367 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M79.742 621.03 L79.742 616.771 Q81.5012 617.604 83.3068 618.044 Q85.1123 618.484 86.8484 618.484 Q91.478 618.484 93.9086 615.382 Q96.3623 612.257 96.7095 605.914 Q95.3669 607.905 93.3067 608.97 Q91.2465 610.035 88.7466 610.035 Q83.5614 610.035 80.529 606.91 Q77.5198 603.762 77.5198 598.322 Q77.5198 592.998 80.6679 589.78 Q83.816 586.563 89.0475 586.563 Q95.0428 586.563 98.1909 591.169 Q101.362 595.752 101.362 604.502 Q101.362 612.674 97.4734 617.558 Q93.6076 622.419 87.0567 622.419 Q85.2975 622.419 83.492 622.072 Q81.6864 621.724 79.742 621.03 M89.0475 606.377 Q92.1956 606.377 94.0243 604.225 Q95.8761 602.072 95.8761 598.322 Q95.8761 594.595 94.0243 592.442 Q92.1956 590.266 89.0475 590.266 Q85.8993 590.266 84.0475 592.442 Q82.2188 594.595 82.2188 598.322 Q82.2188 602.072 84.0475 604.225 Q85.8993 606.377 89.0475 606.377 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M51.6634 337.396 L59.3023 337.396 L59.3023 311.031 L50.9921 312.697 L50.9921 308.438 L59.256 306.772 L63.9319 306.772 L63.9319 337.396 L71.5707 337.396 L71.5707 341.332 L51.6634 341.332 L51.6634 337.396 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M85.0429 337.396 L101.362 337.396 L101.362 341.332 L79.4179 341.332 L79.4179 337.396 Q82.0799 334.642 86.6632 330.012 Q91.2697 325.359 92.4502 324.017 Q94.6956 321.494 95.5752 319.758 Q96.478 317.998 96.478 316.309 Q96.478 313.554 94.5336 311.818 Q92.6123 310.082 89.5104 310.082 Q87.3114 310.082 84.8577 310.846 Q82.4271 311.609 79.6494 313.16 L79.6494 308.438 Q82.4734 307.304 84.9271 306.725 Q87.3808 306.147 89.4178 306.147 Q94.7882 306.147 97.9826 308.832 Q101.177 311.517 101.177 316.008 Q101.177 318.137 100.367 320.058 Q99.5798 321.957 97.4734 324.549 Q96.8947 325.221 93.7928 328.438 Q90.691 331.633 85.0429 337.396 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip062)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  200.062,1422.8 267.481,1406.44 334.9,1445.72 402.32,1417.62 469.739,1440.65 537.158,1430.07 604.577,1445.23 671.996,1445.72 739.415,1407.35 806.835,1408.82 \n",
       "  874.254,1439.64 941.673,87.9763 1009.09,1442.25 1076.51,1364.96 1143.93,1402.21 1211.35,1432.31 1278.77,1097.63 1346.19,1408.95 1413.61,1239.65 1481.03,1390.33 \n",
       "  1548.45,1382.94 1615.86,1408.5 1683.28,1431.48 1750.7,1417.69 1818.12,1406.92 1885.54,1399.91 1952.96,1443.23 2020.38,1342.63 2087.8,1437.19 2155.22,1445.72 \n",
       "  2222.64,1442.82 2290.06,1433.75 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"\n",
       "M1980.32 198.898 L2278.91 198.898 L2278.91 95.2176 L1980.32 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1980.32,198.898 2278.91,198.898 2278.91,95.2176 1980.32,95.2176 1980.32,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip060)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2004.94,147.058 2152.63,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip060)\" d=\"M2191.09 166.745 Q2189.29 171.375 2187.57 172.787 Q2185.86 174.199 2182.99 174.199 L2179.59 174.199 L2179.59 170.634 L2182.09 170.634 Q2183.85 170.634 2184.82 169.8 Q2185.79 168.967 2186.97 165.865 L2187.73 163.921 L2177.25 138.412 L2181.76 138.412 L2189.86 158.689 L2197.97 138.412 L2202.48 138.412 L2191.09 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip060)\" d=\"M2209.77 160.402 L2217.41 160.402 L2217.41 134.037 L2209.1 135.703 L2209.1 131.444 L2217.36 129.778 L2222.04 129.778 L2222.04 160.402 L2229.68 160.402 L2229.68 164.338 L2209.77 164.338 L2209.77 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgn       = time()\n",
    "averages  = []\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    bestEpSc    = -100.0;\n",
    "    statesBest  = zeros( size( X_0, 1 ), T )\n",
    "    actionsBest = zeros( T );\n",
    "    \n",
    "    if blSode\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    elseif blPoch\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    else\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    end\n",
    "    \n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/(episodes-1)\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        s_l = 0.0\n",
    "        # while s_l == 0\n",
    "        \n",
    "            X  = X_0\n",
    "\n",
    "            ##### Double Q-Learning ###########################################\n",
    "\n",
    "            for k = 1:T\n",
    "\n",
    "                # 1. Choose action\n",
    "                if rand() < epsilon\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                else\n",
    "\n",
    "                    A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                    if A == 1000.0 # Indicates no values in this region\n",
    "                        if rand() < EXPrand \n",
    "                            A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                        else\n",
    "                            A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                # 2. Cache last state\n",
    "                qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "                # 3. Generate the next stae\n",
    "                Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "                # 4. Collect reward R( s, a, s' )\n",
    "                R_t = cartpole_reward( Xp )\n",
    "\n",
    "                # 5. Get the optimal action at the next state\n",
    "                a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "\n",
    "                # 6. Compute the value at the next state\n",
    "\n",
    "                V_tp1_opt = query_value_fuzzy( \n",
    "                    Q_kdTree, G, V, \n",
    "                    get_Q( \n",
    "                        select_X_vector( Xp ), \n",
    "                        a_tp1_opt \n",
    "                    ); \n",
    "                    k = vNN \n",
    "                )\n",
    "                if isnan( V_tp1_opt )\n",
    "                    V_tp1_opt = 0.0\n",
    "                end\n",
    "\n",
    "\n",
    "                # 7. Blend the value back into nearest points\n",
    "\n",
    "                idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "                nNear      = size( idxs, 1 )\n",
    "                for i = 1:nNear\n",
    "                    j    = idxs[i]\n",
    "                    if !isnan( wgts[i] ) \n",
    "\n",
    "                        # VS[j] = R_t + gamma * V_tp1_opt # Q-Learning\n",
    "                        VS[j] = VS[j] + alpha*( R_t + gamma*V_tp1_opt - V[j] ) # Q(TD)-Learning\n",
    "\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                states[:,k] = Xp\n",
    "                actions[k]  = A\n",
    "\n",
    "                X = Xp\n",
    "            end\n",
    "\n",
    "            s_l    = vertical_score_s( states, aMargin, ts )\n",
    "            \n",
    "        # end\n",
    "        s_Totl += s_l\n",
    "    \n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if s_l > bestEpSc\n",
    "            bestEpSc    = s_l\n",
    "            statesBest  = copy( states  )\n",
    "            actionsBest = copy( actions )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        ##### Eligibility Traces ##########################################\n",
    "        # if useElig && (s_l > s_Totl/(1.0*l)) && (s_l > 0.0) \n",
    "        # if useElig && (s_l > 0.0) \n",
    "        if useElig \n",
    "            \n",
    "            # if s_l == 0.0\n",
    "            #     states  = copy( bestXs )\n",
    "            #     actions = copy( bestAs )\n",
    "            # end\n",
    "        \n",
    "            # 1. Find `N_peaks`\n",
    "            peakDices = find_state_history_R_peaks( states, N_peaks )\n",
    "            # 2. For each peak, iterate back in time through states\n",
    "            for ii = 1:min(N_peaks, length(peakDices))\n",
    "                topDex = peakDices[ ii ]\n",
    "                X      = states[:,topDex]\n",
    "                R_jj    = cartpole_reward( X )\n",
    "                # 3. For each Q-state in the trace\n",
    "                for jj = (topDex-1):-1:max(1,topDex-N_steps)\n",
    "                    X = states[:,jj]\n",
    "                    R_jj *= lambda\n",
    "                    a_jj = actions[jj]\n",
    "                    q_jj = get_Q( select_X_vector( X ), a_jj )\n",
    "                    V_jj = query_value_fuzzy( Q_kdTree, G, V, q_jj; k = vNN )\n",
    "\n",
    "                    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, q_jj; k = bNN )\n",
    "                    nNear      = size( idxs, 1 )\n",
    "\n",
    "                    for kk = 1:nNear\n",
    "                        ll = idxs[kk]\n",
    "                        if !isnan( wgts[kk] ) \n",
    "                            VS[ll] = VS[ll] + alpha*( R_jj + V_jj - V[ll] ) # Q(TD)-Learning\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Decay the exploration probability\n",
    "        epsilon -= deltaEp\n",
    "        \n",
    "        \n",
    "        ##### Double Q-Learning ##########################################\n",
    "        # Every `swapDiv` episodes, swap Q-functions for Double Q-Learning\n",
    "        \n",
    "        if (l % swapDiv == 0)\n",
    "            \n",
    "            vSwp = copy( VS   )\n",
    "            VS   = copy( V    )\n",
    "            V    = copy( vSwp )\n",
    "            # println(\"SWAP\")\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "     \n",
    "    \n",
    "    ##### Q-Function Hacks ################################################\n",
    "    \n",
    "    # Blend Method 1: Best Episode\n",
    "    if blSode\n",
    "        V  = blend_alpha_of_A_into_B( beta, vBst, V  )\n",
    "        VS = blend_alpha_of_A_into_B( beta, vBst, VS )\n",
    "    end\n",
    "    \n",
    "    # if (s_Avg > bestAvg) && true\n",
    "    #     println( \"BLEND\" )\n",
    "    #     bestAvg = s_Avg\n",
    "    #     vBAv    = copy( V ) # Try a blend of both next # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    #     vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V ) # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    # end\n",
    "        \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709555b9-2598-4281-a634-c7b0681277d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Method 2 Performance, Average Vertical Duration [s]\n",
    "Each score is the best average score of the last two epochs: 32 epochs of 64 episodes each, Q-function swap after every episode \n",
    "\n",
    "### TD Tuning\n",
    "\n",
    "$\\gamma = 1.00$  \n",
    "\n",
    "| Param                |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|----------------------|------| ------- | ------- | ------- | ------- | ------- |------| ----- |\n",
    "| $$\\alpha = 0.99$$    |&nbsp;| 0.251   | 0.198   | 0.146   | 0.147   | 0.210   |&nbsp;| 0.190 |\n",
    "| $$\\alpha = 0.75$$    |&nbsp;| 0.179   | 0.185   | 0.239   | 0.179   | 0.175   |&nbsp;| 0.191 |\n",
    "| $$\\alpha = 0.50$$    |&nbsp;| 0.204   | 0.100   | 0.238   | 0.158   | 0.139   |&nbsp;| 0.168 |\n",
    "| $$\\alpha = 0.25$$    |&nbsp;| 0.294   | 0.170   | 0.107   | 0.223   | 0.147   |&nbsp;| 0.188 |\n",
    "| $$\\alpha = 0.125$$   |&nbsp;| 0.187   | 0.254   | 0.177   | 0.163   | 0.204   |&nbsp;| 0.197 |  \n",
    "| $$\\alpha = 0.0625$$  |&nbsp;| 0.113   | 0.241   | 0.353   | 0.134   | 0.749   |&nbsp;| 0.318 |\n",
    "| $$\\alpha = 0.03125$$ |&nbsp;| 0.231   | 0.322   | 0.018   | 0.098   | 0.000   |&nbsp;| 0.134 |\n",
    "| $$\\alpha = 0.02344$$ |&nbsp;| 1.289   | 0.119   | 0.380   | 0.168   | 0.086   |&nbsp;| 0.408 |\n",
    "| $$\\alpha = \\mathbf{0.02148}$$ |&nbsp;| 0.498   | 0.813   | 0.286   | 7.130   | 0.281   |&nbsp;| **1.802** |\n",
    "| $$\\alpha = 0.01953$$ |&nbsp;| 0.234   | 0.113   | 0.445   | 0.119   | 1.637   |&nbsp;| 0.510 |\n",
    "| $$\\alpha = 0.01758$$ |&nbsp;| 0.175   | 0.249   | 0.217   | 0.047   | 1.006   |&nbsp;| 0.339 |\n",
    "| $$\\alpha = 0.01563$$ |&nbsp;| 0.281   | 1.371   | 0.066   | 0.037   | 0.751   |&nbsp;| 0.501 |\n",
    "| $$\\alpha = 0.00782$$ |&nbsp;| 0.133   | 0.241   | 0.149   | 0.493   | 0.146   |&nbsp;| 0.232 |\n",
    "| $$\\alpha = 0.00391$$ |&nbsp;| 0.037   | 0.626   | 1.000   | 0.525   | 0.139   |&nbsp;| 0.465 |\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### $\\gamma$ Tuning\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "\n",
    "| Param                 |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "| :-------------------- |------| ------- | ------- | ------- | ------- | ------- |------| ----- |\n",
    "| $$\\gamma = 0.999$$    |&nbsp;| 0.925   | 0.247   | 0.145   | 0.364   | 3.038   |&nbsp;| 0.944 |\n",
    "| $$\\gamma = 0.99$$     |&nbsp;| 0.011   | 0.448   | 0.453   | 0.915   | 0.013   |&nbsp;| 0.368 |\n",
    "| $$\\gamma = 0.85$$     |&nbsp;| 0.314   | 2.778   | 0.275   | 1.183   | 0.079   |&nbsp;| 0.926 |\n",
    "| $$\\gamma = 0.80$$     |&nbsp;| 0.082   | 0.033   | 0.173   | 0.251   | 1.741   |&nbsp;| 0.456 |\n",
    "| $$\\gamma = 0.75$$     |&nbsp;| 0.283   | 0.239   | 2.223   | 0.264   | 0.753   |&nbsp;| 0.752 |\n",
    "| $$\\gamma = 0.50$$     |&nbsp;| 0.167   | 0.289   | 0.474   | 0.266   | 0.230   |&nbsp;| 0.285 |\n",
    "\n",
    " \n",
    "### Double-Q Tuning, Swap Evey N Episodes  \n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "Epochs = 32\n",
    "\n",
    "| Param                 |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|-----------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| $$\\%\\ \\ 2$$           |&nbsp;|  0.570  | 0.132   | 1.053   | 0.731   | 0.900   |&nbsp;| 0.677 |\n",
    "| $$\\%\\ \\ 4$$           |&nbsp;|  1.313  | 0.087   | 2.282   | 0.417   | 0.409   |&nbsp;| 0.901 |\n",
    "| $$\\%\\ \\ 8$$           |&nbsp;|  0.097  | 0.040   | 0.621   | 0.030   | 0.608   |&nbsp;| 0.279 |\n",
    "| $$\\%16$$              |&nbsp;|  0.260  | 0.219   | 0.054   | 0.407   | 0.845   |&nbsp;| 0.357 |\n",
    "| $$\\%32$$              |&nbsp;|  0.674  | 0.130   | 0.301   | 0.286   | 0.313   |&nbsp;| 0.341 |\n",
    "| $$\\%\\mathbf{64}$$     |&nbsp;| 15.261  | 2.072   | 0.380   | 0.056   | 0.727   |&nbsp;| **3.699** |\n",
    "  \n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "Episodes = 128  \n",
    "Epochs = 16  \n",
    "\n",
    "| Param            |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| $$\\%\\ \\ \\ \\ 4$$  |&nbsp;|  1.333  | 0.117   | 0.138   | 0.430   | 0.149   |&nbsp;| 0.433 |\n",
    "| $$\\%\\ \\ 64$$     |&nbsp;|  0.166  | 0.110   | 0.240   | 1.615   | 0.049   |&nbsp;| 0.436 |\n",
    "| $$\\%128$$        |&nbsp;|  2.700  | 0.228   | 0.222   | 0.183   | 0.002   |&nbsp;| 0.667 |\n",
    "\n",
    "  \n",
    "### Trace Tuning\n",
    "\n",
    "This is not a Sutton and Barto eligibility trace.  Instead, I look for $N_\\text{peak}$ peaks in the value history of the episode, and apply the learning rule in reverse order from the peak through $N_\\text{step}$ previous timesteps, with a $\\lambda$ decay each step. This is neither classical nor rigorous, and if it does not work, then I will switch to one of the classic $Q(\\lambda)$ methods found in Sutton and Barto.\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "$\\lambda = 0.95$  \n",
    "Peaks = 16  \n",
    "Episodes = 64  \n",
    "Swap = \\%64  \n",
    "Epochs = 32  \n",
    "\n",
    "\n",
    "| Param                  |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| Steps: &nbsp; &nbsp; 4 |&nbsp;| 0.418   | 0.098   | 0.231   | 0.080   | 0.571   |&nbsp;| 0.280 |\n",
    "| Steps: &nbsp; 16       |&nbsp;| 0.540   | 0.365   | 1.629   | 0.215   | 0.470   |&nbsp;| 0.644 |\n",
    "| Steps: &nbsp; 64       |&nbsp;| 1.197   | 0.410   | 1.187   | 1.017   | 0.975   |&nbsp;| 0.957 |\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "$\\lambda = 0.99$  \n",
    "Peaks = 32  \n",
    "Episodes = 64  \n",
    "Swap = \\%64  \n",
    "Epochs = 32  \n",
    "\n",
    "Variations\n",
    "* Only trace non-zero uptime episodes, Result: Many more zero score episodes and no improvement\n",
    "* Only trace episodes with above-average uptime, Result: Many more zero score episodes and no improvement\n",
    "* Re-run the episode until there is non-zero uptime, Result: Extremely slow and no improvement in average uptime  \n",
    "\n",
    "| Param                  |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| Steps: &nbsp; 64       |&nbsp;| 0.265   |  1.204  |         |         |         |&nbsp;|       |\n",
    "| Steps: &nbsp; 96       |&nbsp;| 0.212   | 12.012  |         |         |         |&nbsp;|       |\n",
    "| Steps: 128             |&nbsp;| 0.065   | 17.540  | 0.493   | 0.325   | 15.345  |&nbsp;| 6.754 |\n",
    "| Steps: 192             |&nbsp;| 0.271   |  0.127  |         |         |         |&nbsp;|       |\n",
    "| Steps: 256             |&nbsp;| 0.958   |  2.014  | 0.947   | 0.433   |  0.045  |&nbsp;| 0.879 |\n",
    "| Steps: 512             |&nbsp;| 1.487   |  0.343  | 0.159   | 1.057   |  0.640  |&nbsp;| 0.737 |\n",
    "\n",
    "\n",
    "### Blend: Best Episode\n",
    "\n",
    "$\\beta = 0.07$:  \n",
    "$\\beta = 0.15$: 0.244\n",
    "\n",
    "| Method      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 | Mean |\n",
    "| ----------- | ------- | ------- | ------- | ------- | ------- | ---- |\n",
    "| Blend (Epi) |         |         |         |         |         |      |\n",
    "| Blend (Epo) |         |         |         |         |         |      |\n",
    "| TD          |         |         |         |         |         |      |\n",
    "| TD  + ????? |         |         |         |         |         |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60c1d8a-58c5-4719-89c8-b69bf6623266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Process(`\u001b[4mplay\u001b[24m \u001b[4m-nq\u001b[24m \u001b[4m-t\u001b[24m \u001b[4malsa\u001b[24m \u001b[4msynth\u001b[24m \u001b[4m3\u001b[24m \u001b[4msine\u001b[24m \u001b[4m300\u001b[24m`, ProcessExited(0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(`play -nq -t alsa synth 3 sine 300`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5ade7-5f94-43b1-837f-85ccdd6b5c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
