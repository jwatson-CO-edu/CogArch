{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cefc7-7c60-4838-9399-26a98ec9736e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43290374-89de-4616-8800-c86799248c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "using DataStructures\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851743ab-a511-40fb-850b-bf90efa9232d",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39765-4abe-409a-bea1-f44fa8ec2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              -10.0 +10.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf920d4-46af-4f22-8933-c3db011ff716",
   "metadata": {},
   "source": [
    "# Q-Learning Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f605b904-b397-4617-9dbe-a27c0b4fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Return the indices and scores of all the peak rewards in the data\n",
    "\"\"\"\n",
    "function find_state_history_R_peaks( X_hist, N_pks )\n",
    "    \n",
    "    epLen   = size( X_hist, 2 )\n",
    "    rising  = false\n",
    "    lastVal = 1e9\n",
    "    lastRis = false\n",
    "    pqPeaks = PriorityQueue();\n",
    "    rtnPeak = []\n",
    "    \n",
    "    for j = 1:epLen\n",
    "        X       = X_hist[:,j]\n",
    "        currVal = cartpole_reward( X )\n",
    "        rising  = (currVal > lastVal)\n",
    "        if (!rising) && lastRis\n",
    "            pqPeaks[j] = -currVal # Store the current index at its current (negative) value\n",
    "        end\n",
    "        lastVal = currVal\n",
    "        lastRis = rising\n",
    "    end\n",
    "    for i = 1:min( N_pks, length( pqPeaks ) )\n",
    "        append!( rtnPeak, dequeue!( pqPeaks ) )\n",
    "    end\n",
    "    \n",
    "    return rtnPeak;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d663e-1ccd-441f-807f-44f84a43e4d0",
   "metadata": {},
   "source": [
    "# Q-Function Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91f06c-df14-4fe7-b81d-12c3184b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721c7-88a9-4b57-bf9f-ad9f9acbf786",
   "metadata": {},
   "source": [
    "# CartPole Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc4097d-9b96-453c-ba4f-4b06fce7fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083b48-38dc-4616-979a-da8874303d32",
   "metadata": {},
   "source": [
    "# Agent Data Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f648d5-8d8e-4da4-bd1e-3f3d9ec7c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres     = Fmax/Fdiv\n",
    "spaceDiv = 4.0 # 1.0 # 2.0 # 5.0 # 7.5  \n",
    "\n",
    "### Construct grid of anchors ###\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ spaceDiv, spaceDiv, spaceDiv, spaceDiv, Fres ] );\n",
    "nPts = size( G )[2]; # ------- Number of anchors\n",
    "mDim = size( G )[1]; # ------- Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db1609-9df1-438b-9675-0286bf01a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states  = zeros( size( X_0, 1 ), T )\n",
    "actions = zeros( T );\n",
    "bestXs  = zeros( size( X_0, 1 ), T )\n",
    "bestAs  = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9f1ef-79bc-41fd-b6e9-ab0554460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d49b4c6-8353-4a01-8a16-9b544e1ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c954412-18b9-45a8-97a6-e61cf19f15d2",
   "metadata": {},
   "source": [
    "# Agent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358ff3d-44a5-491e-9597-0a0a73c6b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Q(TD)-Learning Params #####\n",
    "scale = 7.5; #1.650; # ----------- scale\n",
    "vNN   =  3 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\"\n",
    "\n",
    "alpha    = 0.02148 # 0.99 # 0.75 # 0.5 # 0.25 # 0.125 # 0.0625 # 0.03125 # 0.015625 # 0.00782 # 0.00391\n",
    "gamma    = 1.00 \n",
    "swapDiv  = 64\n",
    "epsMin   = 0.00 # Last iter is policy eval\n",
    "epsMax   = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes = 64 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs   = 32 #128 #64 # 32 #16\n",
    "EXPrand  = 1.00 #0.25 #0.5 # 0.75\n",
    "Alpha    = 0.875\n",
    "aMargin  = (pi/180)*15.0;\n",
    "\n",
    "##### Q-Function Hacks #####\n",
    "beta   = 0.15\n",
    "blSode = false\n",
    "blPoch = false\n",
    "\n",
    "##### Eligibility Params #####\n",
    "useElig = true\n",
    "N_peaks = 32\n",
    "N_steps = 128\n",
    "lambda  =   0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e910ca2-281c-4d06-98e2-1c96fa7c1916",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d3689b-947a-400b-9031-9f1a13f4df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0\n",
      "Training Iteration 4 score: 0.38000000000000017, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.2800000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.22000000000000006, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.1200000000000008, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.47000000000000025, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.09999999999999999, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.49000000000000027, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.19000000000000003, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.4000000000000002, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.23234375000000013\n",
      "\n",
      "Epoch 2, Best Score: 1.370000000000001\n",
      "Training Iteration 4 score: 0.2900000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.24000000000000007, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.21000000000000005, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.9100000000000006, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.26000000000000006, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.5200000000000002, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.5200000000000002, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.09, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.5000000000000002, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.4000000000000002, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.35000000000000014, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.2700000000000001, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.37000000000000016, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.19000000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 1.0300000000000007, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.40093750000000017\n",
      "\n",
      "Epoch 3, Best Score: 1.480000000000001\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0\n",
      "\n",
      "Epoch 4, Best Score: 1.480000000000001\n",
      "Training Iteration 4 score: 0.09, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.08, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.12999999999999998, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.37000000000000016, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.45000000000000023, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.7900000000000005, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.12171875000000004\n",
      "\n",
      "Epoch 5, Best Score: 1.480000000000001\n",
      "Training Iteration 4 score: 0.12999999999999998, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.18000000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.12999999999999998, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.13999999999999999, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.11999999999999998, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.10999999999999999, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.20000000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.08140625000000001\n",
      "\n",
      "Epoch 6, Best Score: 1.480000000000001\n",
      "Training Iteration 4 score: 0.12999999999999998, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.3100000000000001, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.11999999999999998, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.15, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.22000000000000006, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.4400000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.09, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 2.159999999999998, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.2800000000000001, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.23000000000000007, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.24000000000000007, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.27140624999999996\n",
      "\n",
      "Epoch 7, Best Score: 2.159999999999998\n",
      "Training Iteration 4 score: 0.24000000000000007, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.10999999999999999, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.10999999999999999, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.10999999999999999, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.11999999999999998, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.10999999999999999, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.11999999999999998, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.09, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.08, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.10499999999999995\n",
      "\n",
      "Epoch 8, Best Score: 2.159999999999998\n",
      "Training Iteration 4 score: 0.7200000000000004, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.47000000000000025, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.24000000000000007, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.47000000000000025, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.7300000000000004, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.20000000000000004, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.37000000000000016, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.6000000000000003, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.5500000000000003, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.5000000000000002, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.5500000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.48000000000000026, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.6100000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.47000000000000025, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.47218750000000037\n",
      "\n",
      "Epoch 9, Best Score: 2.159999999999998\n",
      "Training Iteration 4 score: 1.7800000000000014, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 1.2100000000000009, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.36000000000000015, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.5200000000000002, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.5800000000000012, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 8.229999999999869, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 2.0400000000000005, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 1.0300000000000007, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 1.9900000000000015, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.7100000000000013, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 17.559999999999945, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 28.030000000001582, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 23.690000000000904, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 5.107187500000085\n",
      "\n",
      "Epoch 10, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.3200000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 1.0100000000000007, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.5800000000000012, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.6500000000000004, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.6800000000000004, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.9800000000000006, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.2700000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.3000000000000001, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.7300000000000004, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.7300000000000004, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.6100000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.6500000000000004, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.5500000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.6700000000000004, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.4925000000000004\n",
      "\n",
      "Epoch 11, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.16, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5900000000000003, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.34000000000000014, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.20000000000000004, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.15, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.13999999999999999, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.3000000000000001, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.7600000000000005, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.21000000000000005, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.10750000000000004\n",
      "\n",
      "Epoch 12, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.6700000000000004, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.3300000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.3300000000000001, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.15, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.05, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.16, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.37000000000000016, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.15, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.20000000000000004, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.47000000000000025, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.18000000000000002, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.15, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.5800000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.6400000000000003, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.27000000000000013\n",
      "\n",
      "Epoch 13, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.6000000000000003, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.3100000000000001, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.35000000000000014, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.26000000000000006, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.2800000000000001, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.22000000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.1400000000000008, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.6400000000000003, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.9200000000000006, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.26000000000000006, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 1.0100000000000007, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.44953125000000044\n",
      "\n",
      "Epoch 14, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.35000000000000014, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.35000000000000014, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7400000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.11999999999999998, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.7600000000000005, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.49000000000000027, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.24000000000000007, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.9400000000000006, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.11999999999999998, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.2900000000000001, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.21828125000000007\n",
      "\n",
      "Epoch 15, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.6500000000000004, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.6200000000000003, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.4000000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.7500000000000004, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.5200000000000002, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.3900000000000002, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.17, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.5100000000000002, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.8800000000000006, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.45000000000000023, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.3551562500000001\n",
      "\n",
      "Epoch 16, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.3100000000000001, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 1.5500000000000012, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.4200000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 1.8500000000000014, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.9100000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 1.5300000000000011, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.34000000000000014, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.5500000000000003, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.25000000000000006, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 1.6900000000000013, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.6300000000000003, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.5400000000000003, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 2.8399999999999834, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 1.1100000000000008, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.9053124999999989\n",
      "\n",
      "Epoch 17, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.35000000000000014, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.5200000000000002, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.5400000000000003, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.7900000000000005, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.1653125000000001\n",
      "\n",
      "Epoch 18, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.5400000000000003, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.9100000000000006, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7100000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.4000000000000002, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.6600000000000004, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.25000000000000006, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.09999999999999999, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.19000000000000003, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.3900000000000002, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.16, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.17, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.5700000000000003, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.5100000000000002, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.28546875000000016\n",
      "\n",
      "Epoch 19, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.25000000000000006, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.08, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.6700000000000004, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.12312500000000007\n",
      "\n",
      "Epoch 20, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.18000000000000002, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.6900000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.23000000000000007, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.15, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.5600000000000003, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.6900000000000004, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.11999999999999998, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.14750000000000008\n",
      "\n",
      "Epoch 21, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.38000000000000017, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.07, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.6900000000000004, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.2800000000000001, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.45000000000000023, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.12703125000000007\n",
      "\n",
      "Epoch 22, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.48000000000000026, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.37000000000000016, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.6200000000000003, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.11999999999999998, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 1.0700000000000007, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.9300000000000006, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.13999999999999999, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.17062500000000017\n",
      "\n",
      "Epoch 23, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.5900000000000003, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.47000000000000025, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 1.0000000000000007, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.36000000000000015, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.11562500000000007\n",
      "\n",
      "Epoch 24, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.45000000000000023, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.3900000000000002, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.5600000000000003, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.4000000000000002, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.48000000000000026, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.16187500000000007\n",
      "\n",
      "Epoch 25, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.7200000000000004, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.7200000000000004, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.04359375000000003\n",
      "\n",
      "Epoch 26, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.12999999999999998, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.018906250000000003\n",
      "\n",
      "Epoch 27, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.025781250000000012\n",
      "\n",
      "Epoch 28, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.5300000000000002, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.4400000000000002, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.2700000000000001, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.05953125000000003\n",
      "\n",
      "Epoch 29, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.45000000000000023, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.5700000000000003, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.02921875000000001\n",
      "\n",
      "Epoch 30, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.03328125000000002\n",
      "\n",
      "Epoch 31, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.16, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.0025\n",
      "\n",
      "Epoch 32, Best Score: 29.99000000000189\n",
      "Training Iteration 4 score: 0.26000000000000006, epsilon: 0.4761904761904763\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.44444444444444464\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.412698412698413\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3809523809523814\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.34920634920634974\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3174603174603181\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2857142857142865\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.25396825396825484\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2222222222222231\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.19047619047619135\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1587301587301596\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.12698412698412787\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.09523809523809612\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.06349206349206438\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.03174603174603263\n",
      "Training Iteration 64 score: 0.0, epsilon: 8.881784197001252e-16\n",
      "Average Score: 0.02578125000000001\n",
      "Saved a trained Q-table with size (76032,), After 13.26019625266393 minutes of training!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip610\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip610)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip611\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip610)\" d=\"\n",
       "M112.177 1486.45 L2352.76 1486.45 L2352.76 47.2441 L112.177 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip612\">\n",
       "    <rect x=\"112\" y=\"47\" width=\"2242\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  448.332,1486.45 448.332,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  789.26,1486.45 789.26,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1130.19,1486.45 1130.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1471.12,1486.45 1471.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1812.04,1486.45 1812.04,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2152.97,1486.45 2152.97,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  448.332,1486.45 448.332,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  789.26,1486.45 789.26,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1130.19,1486.45 1130.19,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1471.12,1486.45 1471.12,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1812.04,1486.45 1812.04,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2152.97,1486.45 2152.97,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip610)\" d=\"M438.61 1514.29 L456.966 1514.29 L456.966 1518.22 L442.892 1518.22 L442.892 1526.7 Q443.911 1526.35 444.929 1526.19 Q445.948 1526 446.966 1526 Q452.753 1526 456.133 1529.17 Q459.513 1532.34 459.513 1537.76 Q459.513 1543.34 456.04 1546.44 Q452.568 1549.52 446.249 1549.52 Q444.073 1549.52 441.804 1549.15 Q439.559 1548.78 437.152 1548.04 L437.152 1543.34 Q439.235 1544.47 441.457 1545.03 Q443.679 1545.58 446.156 1545.58 Q450.161 1545.58 452.499 1543.48 Q454.837 1541.37 454.837 1537.76 Q454.837 1534.15 452.499 1532.04 Q450.161 1529.94 446.156 1529.94 Q444.281 1529.94 442.406 1530.35 Q440.554 1530.77 438.61 1531.65 L438.61 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M763.948 1544.91 L771.587 1544.91 L771.587 1518.55 L763.277 1520.21 L763.277 1515.95 L771.54 1514.29 L776.216 1514.29 L776.216 1544.91 L783.855 1544.91 L783.855 1548.85 L763.948 1548.85 L763.948 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M803.299 1517.37 Q799.688 1517.37 797.86 1520.93 Q796.054 1524.47 796.054 1531.6 Q796.054 1538.71 797.86 1542.27 Q799.688 1545.82 803.299 1545.82 Q806.934 1545.82 808.739 1542.27 Q810.568 1538.71 810.568 1531.6 Q810.568 1524.47 808.739 1520.93 Q806.934 1517.37 803.299 1517.37 M803.299 1513.66 Q809.11 1513.66 812.165 1518.27 Q815.244 1522.85 815.244 1531.6 Q815.244 1540.33 812.165 1544.94 Q809.11 1549.52 803.299 1549.52 Q797.489 1549.52 794.411 1544.94 Q791.355 1540.33 791.355 1531.6 Q791.355 1522.85 794.411 1518.27 Q797.489 1513.66 803.299 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M1105.37 1544.91 L1113.01 1544.91 L1113.01 1518.55 L1104.7 1520.21 L1104.7 1515.95 L1112.97 1514.29 L1117.64 1514.29 L1117.64 1544.91 L1125.28 1544.91 L1125.28 1548.85 L1105.37 1548.85 L1105.37 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M1134.77 1514.29 L1153.13 1514.29 L1153.13 1518.22 L1139.05 1518.22 L1139.05 1526.7 Q1140.07 1526.35 1141.09 1526.19 Q1142.11 1526 1143.13 1526 Q1148.91 1526 1152.29 1529.17 Q1155.67 1532.34 1155.67 1537.76 Q1155.67 1543.34 1152.2 1546.44 Q1148.73 1549.52 1142.41 1549.52 Q1140.23 1549.52 1137.97 1549.15 Q1135.72 1548.78 1133.31 1548.04 L1133.31 1543.34 Q1135.4 1544.47 1137.62 1545.03 Q1139.84 1545.58 1142.32 1545.58 Q1146.32 1545.58 1148.66 1543.48 Q1151 1541.37 1151 1537.76 Q1151 1534.15 1148.66 1532.04 Q1146.32 1529.94 1142.32 1529.94 Q1140.44 1529.94 1138.57 1530.35 Q1136.72 1530.77 1134.77 1531.65 L1134.77 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M1449.89 1544.91 L1466.21 1544.91 L1466.21 1548.85 L1444.26 1548.85 L1444.26 1544.91 Q1446.93 1542.16 1451.51 1537.53 Q1456.12 1532.88 1457.3 1531.53 Q1459.54 1529.01 1460.42 1527.27 Q1461.32 1525.51 1461.32 1523.82 Q1461.32 1521.07 1459.38 1519.33 Q1457.46 1517.6 1454.36 1517.6 Q1452.16 1517.6 1449.7 1518.36 Q1447.27 1519.13 1444.5 1520.68 L1444.5 1515.95 Q1447.32 1514.82 1449.77 1514.24 Q1452.23 1513.66 1454.26 1513.66 Q1459.63 1513.66 1462.83 1516.35 Q1466.02 1519.03 1466.02 1523.52 Q1466.02 1525.65 1465.21 1527.57 Q1464.43 1529.47 1462.32 1532.07 Q1461.74 1532.74 1458.64 1535.95 Q1455.54 1539.15 1449.89 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M1486.02 1517.37 Q1482.41 1517.37 1480.58 1520.93 Q1478.78 1524.47 1478.78 1531.6 Q1478.78 1538.71 1480.58 1542.27 Q1482.41 1545.82 1486.02 1545.82 Q1489.66 1545.82 1491.46 1542.27 Q1493.29 1538.71 1493.29 1531.6 Q1493.29 1524.47 1491.46 1520.93 Q1489.66 1517.37 1486.02 1517.37 M1486.02 1513.66 Q1491.83 1513.66 1494.89 1518.27 Q1497.97 1522.85 1497.97 1531.6 Q1497.97 1540.33 1494.89 1544.94 Q1491.83 1549.52 1486.02 1549.52 Q1480.21 1549.52 1477.13 1544.94 Q1474.08 1540.33 1474.08 1531.6 Q1474.08 1522.85 1477.13 1518.27 Q1480.21 1513.66 1486.02 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M1791.32 1544.91 L1807.63 1544.91 L1807.63 1548.85 L1785.69 1548.85 L1785.69 1544.91 Q1788.35 1542.16 1792.94 1537.53 Q1797.54 1532.88 1798.72 1531.53 Q1800.97 1529.01 1801.85 1527.27 Q1802.75 1525.51 1802.75 1523.82 Q1802.75 1521.07 1800.81 1519.33 Q1798.88 1517.6 1795.78 1517.6 Q1793.58 1517.6 1791.13 1518.36 Q1788.7 1519.13 1785.92 1520.68 L1785.92 1515.95 Q1788.75 1514.82 1791.2 1514.24 Q1793.65 1513.66 1795.69 1513.66 Q1801.06 1513.66 1804.25 1516.35 Q1807.45 1519.03 1807.45 1523.52 Q1807.45 1525.65 1806.64 1527.57 Q1805.85 1529.47 1803.75 1532.07 Q1803.17 1532.74 1800.07 1535.95 Q1796.96 1539.15 1791.32 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M1817.5 1514.29 L1835.85 1514.29 L1835.85 1518.22 L1821.78 1518.22 L1821.78 1526.7 Q1822.8 1526.35 1823.81 1526.19 Q1824.83 1526 1825.85 1526 Q1831.64 1526 1835.02 1529.17 Q1838.4 1532.34 1838.4 1537.76 Q1838.4 1543.34 1834.93 1546.44 Q1831.45 1549.52 1825.13 1549.52 Q1822.96 1549.52 1820.69 1549.15 Q1818.44 1548.78 1816.04 1548.04 L1816.04 1543.34 Q1818.12 1544.47 1820.34 1545.03 Q1822.56 1545.58 1825.04 1545.58 Q1829.05 1545.58 1831.38 1543.48 Q1833.72 1541.37 1833.72 1537.76 Q1833.72 1534.15 1831.38 1532.04 Q1829.05 1529.94 1825.04 1529.94 Q1823.17 1529.94 1821.29 1530.35 Q1819.44 1530.77 1817.5 1531.65 L1817.5 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M2141.81 1530.21 Q2145.17 1530.93 2147.05 1533.2 Q2148.94 1535.47 2148.94 1538.8 Q2148.94 1543.92 2145.43 1546.72 Q2141.91 1549.52 2135.43 1549.52 Q2133.25 1549.52 2130.94 1549.08 Q2128.64 1548.66 2126.19 1547.81 L2126.19 1543.29 Q2128.13 1544.43 2130.45 1545.01 Q2132.76 1545.58 2135.29 1545.58 Q2139.69 1545.58 2141.98 1543.85 Q2144.29 1542.11 2144.29 1538.8 Q2144.29 1535.75 2142.14 1534.03 Q2140.01 1532.3 2136.19 1532.3 L2132.16 1532.3 L2132.16 1528.45 L2136.37 1528.45 Q2139.82 1528.45 2141.65 1527.09 Q2143.48 1525.7 2143.48 1523.11 Q2143.48 1520.45 2141.58 1519.03 Q2139.71 1517.6 2136.19 1517.6 Q2134.27 1517.6 2132.07 1518.01 Q2129.87 1518.43 2127.23 1519.31 L2127.23 1515.14 Q2129.89 1514.4 2132.21 1514.03 Q2134.55 1513.66 2136.61 1513.66 Q2141.93 1513.66 2145.03 1516.09 Q2148.13 1518.5 2148.13 1522.62 Q2148.13 1525.49 2146.49 1527.48 Q2144.85 1529.45 2141.81 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M2167.81 1517.37 Q2164.2 1517.37 2162.37 1520.93 Q2160.56 1524.47 2160.56 1531.6 Q2160.56 1538.71 2162.37 1542.27 Q2164.2 1545.82 2167.81 1545.82 Q2171.44 1545.82 2173.25 1542.27 Q2175.08 1538.71 2175.08 1531.6 Q2175.08 1524.47 2173.25 1520.93 Q2171.44 1517.37 2167.81 1517.37 M2167.81 1513.66 Q2173.62 1513.66 2176.68 1518.27 Q2179.75 1522.85 2179.75 1531.6 Q2179.75 1540.33 2176.68 1544.94 Q2173.62 1549.52 2167.81 1549.52 Q2162 1549.52 2158.92 1544.94 Q2155.87 1540.33 2155.87 1531.6 Q2155.87 1522.85 2158.92 1518.27 Q2162 1513.66 2167.81 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,1445.72 2352.76,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,1179.87 2352.76,1179.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,914.018 2352.76,914.018 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,648.169 2352.76,648.169 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,382.321 2352.76,382.321 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip612)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  112.177,116.472 2352.76,116.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1486.45 112.177,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1445.72 131.075,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,1179.87 131.075,1179.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,914.018 131.075,914.018 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,648.169 131.075,648.169 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,382.321 131.075,382.321 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  112.177,116.472 131.075,116.472 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip610)\" d=\"M64.2328 1431.51 Q60.6217 1431.51 58.793 1435.08 Q56.9875 1438.62 56.9875 1445.75 Q56.9875 1452.86 58.793 1456.42 Q60.6217 1459.96 64.2328 1459.96 Q67.867 1459.96 69.6726 1456.42 Q71.5013 1452.86 71.5013 1445.75 Q71.5013 1438.62 69.6726 1435.08 Q67.867 1431.51 64.2328 1431.51 M64.2328 1427.81 Q70.0429 1427.81 73.0985 1432.42 Q76.1772 1437 76.1772 1445.75 Q76.1772 1454.48 73.0985 1459.08 Q70.0429 1463.67 64.2328 1463.67 Q58.4226 1463.67 55.344 1459.08 Q52.2884 1454.48 52.2884 1445.75 Q52.2884 1437 55.344 1432.42 Q58.4226 1427.81 64.2328 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M56.2699 1193.21 L63.9087 1193.21 L63.9087 1166.85 L55.5986 1168.51 L55.5986 1164.25 L63.8624 1162.59 L68.5383 1162.59 L68.5383 1193.21 L76.1772 1193.21 L76.1772 1197.15 L56.2699 1197.15 L56.2699 1193.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M59.8578 927.363 L76.1772 927.363 L76.1772 931.298 L54.2328 931.298 L54.2328 927.363 Q56.8949 924.608 61.4782 919.979 Q66.0846 915.326 67.2652 913.983 Q69.5105 911.46 70.3902 909.724 Q71.2929 907.965 71.2929 906.275 Q71.2929 903.521 69.3485 901.784 Q67.4272 900.048 64.3254 900.048 Q62.1263 900.048 59.6726 900.812 Q57.2421 901.576 54.4643 903.127 L54.4643 898.405 Q57.2884 897.271 59.7421 896.692 Q62.1958 896.113 64.2328 896.113 Q69.6031 896.113 72.7976 898.798 Q75.992 901.484 75.992 905.974 Q75.992 908.104 75.1818 910.025 Q74.3948 911.923 72.2883 914.516 Q71.7096 915.187 68.6078 918.405 Q65.5059 921.599 59.8578 927.363 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M69.0476 646.815 Q72.404 647.533 74.279 649.801 Q76.1772 652.07 76.1772 655.403 Q76.1772 660.519 72.6587 663.32 Q69.1402 666.121 62.6587 666.121 Q60.4828 666.121 58.168 665.681 Q55.8764 665.264 53.4227 664.408 L53.4227 659.894 Q55.3671 661.028 57.6819 661.607 Q59.9967 662.186 62.5198 662.186 Q66.918 662.186 69.2096 660.449 Q71.5244 658.713 71.5244 655.403 Q71.5244 652.348 69.3717 650.635 Q67.242 648.899 63.4226 648.899 L59.3949 648.899 L59.3949 645.056 L63.6078 645.056 Q67.0569 645.056 68.8855 643.69 Q70.7142 642.301 70.7142 639.709 Q70.7142 637.047 68.8161 635.635 Q66.9411 634.2 63.4226 634.2 Q61.5013 634.2 59.3023 634.616 Q57.1032 635.033 54.4643 635.913 L54.4643 631.746 Q57.1264 631.005 59.4412 630.635 Q61.7791 630.264 63.8393 630.264 Q69.1633 630.264 72.2652 632.695 Q75.367 635.102 75.367 639.223 Q75.367 642.093 73.7235 644.084 Q72.08 646.051 69.0476 646.815 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M66.5939 369.115 L54.7884 387.564 L66.5939 387.564 L66.5939 369.115 M65.367 365.041 L71.2466 365.041 L71.2466 387.564 L76.1772 387.564 L76.1772 391.453 L71.2466 391.453 L71.2466 399.601 L66.5939 399.601 L66.5939 391.453 L50.9921 391.453 L50.9921 386.939 L65.367 365.041 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M55.2745 99.1919 L73.6309 99.1919 L73.6309 103.127 L59.5569 103.127 L59.5569 111.599 Q60.5754 111.252 61.5939 111.09 Q62.6124 110.905 63.6309 110.905 Q69.418 110.905 72.7976 114.076 Q76.1772 117.247 76.1772 122.664 Q76.1772 128.243 72.705 131.345 Q69.2328 134.423 62.9134 134.423 Q60.7374 134.423 58.4689 134.053 Q56.2236 133.682 53.8162 132.942 L53.8162 128.243 Q55.8995 129.377 58.1217 129.933 Q60.3439 130.488 62.8208 130.488 Q66.8254 130.488 69.1633 128.382 Q71.5013 126.275 71.5013 122.664 Q71.5013 119.053 69.1633 116.946 Q66.8254 114.84 62.8208 114.84 Q60.9458 114.84 59.0708 115.257 Q57.2189 115.673 55.2745 116.553 L55.2745 99.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip612)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.59,1383.95 243.775,1339.13 311.961,1445.72 380.147,1413.36 448.332,1424.07 516.518,1373.56 584.703,1417.8 652.889,1320.19 721.075,87.9763 789.26,1314.79 \n",
       "  857.446,1417.14 925.631,1373.94 993.817,1326.21 1062,1387.69 1130.19,1351.3 1198.37,1205.04 1266.56,1401.77 1334.74,1369.82 1402.93,1412.98 1471.12,1406.5 \n",
       "  1539.3,1411.94 1607.49,1400.36 1675.67,1414.98 1743.86,1402.68 1812.04,1434.13 1880.23,1440.69 1948.42,1438.86 2016.6,1429.89 2084.79,1437.95 2152.97,1436.87 \n",
       "  2221.16,1445.05 2289.34,1438.86 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip610)\" d=\"\n",
       "M1976.69 198.898 L2278.07 198.898 L2278.07 95.2176 L1976.69 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1976.69,198.898 2278.07,198.898 2278.07,95.2176 1976.69,95.2176 1976.69,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip610)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2001.58,147.058 2150.95,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip610)\" d=\"M2189.69 166.745 Q2187.89 171.375 2186.17 172.787 Q2184.46 174.199 2181.59 174.199 L2178.19 174.199 L2178.19 170.634 L2180.69 170.634 Q2182.45 170.634 2183.42 169.8 Q2184.39 168.967 2185.57 165.865 L2186.34 163.921 L2175.85 138.412 L2180.36 138.412 L2188.46 158.689 L2196.57 138.412 L2201.08 138.412 L2189.69 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip610)\" d=\"M2208.37 160.402 L2216.01 160.402 L2216.01 134.037 L2207.7 135.703 L2207.7 131.444 L2215.96 129.778 L2220.64 129.778 L2220.64 160.402 L2228.28 160.402 L2228.28 164.338 L2208.37 164.338 L2208.37 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgn       = time()\n",
    "averages  = []\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    bestEpSc    = -100.0;\n",
    "    statesBest  = zeros( size( X_0, 1 ), T )\n",
    "    actionsBest = zeros( T );\n",
    "    \n",
    "    if blSode\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    elseif blPoch\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    else\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    end\n",
    "    \n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/(episodes-1)\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        s_l = 0.0\n",
    "        # while s_l == 0\n",
    "        \n",
    "            X  = X_0\n",
    "\n",
    "            ##### Double Q-Learning ###########################################\n",
    "\n",
    "            for k = 1:T\n",
    "\n",
    "                # 1. Choose action\n",
    "                if rand() < epsilon\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                else\n",
    "\n",
    "                    A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                    if A == 1000.0 # Indicates no values in this region\n",
    "                        if rand() < EXPrand \n",
    "                            A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                        else\n",
    "                            A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                # 2. Cache last state\n",
    "                qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "                # 3. Generate the next stae\n",
    "                Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "                # 4. Collect reward R( s, a, s' )\n",
    "                R_t = cartpole_reward( Xp )\n",
    "\n",
    "                # 5. Get the optimal action at the next state\n",
    "                a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "\n",
    "                # 6. Compute the value at the next state\n",
    "\n",
    "                V_tp1_opt = query_value_fuzzy( \n",
    "                    Q_kdTree, G, V, \n",
    "                    get_Q( \n",
    "                        select_X_vector( Xp ), \n",
    "                        a_tp1_opt \n",
    "                    ); \n",
    "                    k = vNN \n",
    "                )\n",
    "                if isnan( V_tp1_opt )\n",
    "                    V_tp1_opt = 0.0\n",
    "                end\n",
    "\n",
    "\n",
    "                # 7. Blend the value back into nearest points\n",
    "\n",
    "                idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "                nNear      = size( idxs, 1 )\n",
    "                for i = 1:nNear\n",
    "                    j    = idxs[i]\n",
    "                    if !isnan( wgts[i] ) \n",
    "\n",
    "                        # VS[j] = R_t + gamma * V_tp1_opt # Q-Learning\n",
    "                        VS[j] = VS[j] + alpha*( R_t + gamma*V_tp1_opt - V[j] ) # Q(TD)-Learning\n",
    "\n",
    "                    end\n",
    "                end\n",
    "\n",
    "                states[:,k] = Xp\n",
    "                actions[k]  = A\n",
    "\n",
    "                X = Xp\n",
    "            end\n",
    "\n",
    "            s_l    = vertical_score_s( states, aMargin, ts )\n",
    "            \n",
    "        # end\n",
    "        s_Totl += s_l\n",
    "    \n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if s_l > bestEpSc\n",
    "            bestEpSc    = s_l\n",
    "            statesBest  = copy( states  )\n",
    "            actionsBest = copy( actions )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        ##### Eligibility Traces ##########################################\n",
    "        # if useElig && (s_l > s_Totl/(1.0*l)) && (s_l > 0.0) \n",
    "        # if useElig && (s_l > 0.0) \n",
    "        if useElig \n",
    "            \n",
    "            # if s_l == 0.0\n",
    "            #     states  = copy( bestXs )\n",
    "            #     actions = copy( bestAs )\n",
    "            # end\n",
    "        \n",
    "            # 1. Find `N_peaks`\n",
    "            peakDices = find_state_history_R_peaks( states, N_peaks )\n",
    "            # 2. For each peak, iterate back in time through states\n",
    "            for ii = 1:min(N_peaks, length(peakDices))\n",
    "                topDex = peakDices[ ii ]\n",
    "                X      = states[:,topDex]\n",
    "                R_jj    = cartpole_reward( X )\n",
    "                # 3. For each Q-state in the trace\n",
    "                for jj = (topDex-1):-1:max(1,topDex-N_steps)\n",
    "                    X = states[:,jj]\n",
    "                    R_jj *= lambda\n",
    "                    a_jj = actions[jj]\n",
    "                    q_jj = get_Q( select_X_vector( X ), a_jj )\n",
    "                    V_jj = query_value_fuzzy( Q_kdTree, G, V, q_jj; k = vNN )\n",
    "\n",
    "                    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, q_jj; k = bNN )\n",
    "                    nNear      = size( idxs, 1 )\n",
    "\n",
    "                    for kk = 1:nNear\n",
    "                        ll = idxs[kk]\n",
    "                        if !isnan( wgts[kk] ) \n",
    "                            VS[ll] = VS[ll] + alpha*( R_jj + V_jj - V[ll] ) # Q(TD)-Learning\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Decay the exploration probability\n",
    "        epsilon -= deltaEp\n",
    "        \n",
    "        \n",
    "        ##### Double Q-Learning ##########################################\n",
    "        # Every `swapDiv` episodes, swap Q-functions for Double Q-Learning\n",
    "        \n",
    "        if (l % swapDiv == 0)\n",
    "            \n",
    "            vSwp = copy( VS   )\n",
    "            VS   = copy( V    )\n",
    "            V    = copy( vSwp )\n",
    "            # println(\"SWAP\")\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "     \n",
    "    \n",
    "    ##### Q-Function Hacks ################################################\n",
    "    \n",
    "    # Blend Method 1: Best Episode\n",
    "    if blSode\n",
    "        V  = blend_alpha_of_A_into_B( beta, vBst, V  )\n",
    "        VS = blend_alpha_of_A_into_B( beta, vBst, VS )\n",
    "    end\n",
    "    \n",
    "    # if (s_Avg > bestAvg) && true\n",
    "    #     println( \"BLEND\" )\n",
    "    #     bestAvg = s_Avg\n",
    "    #     vBAv    = copy( V ) # Try a blend of both next # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    #     vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V ) # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    # end\n",
    "        \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709555b9-2598-4281-a634-c7b0681277d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Method 2 Performance, Average Vertical Duration [s]\n",
    "Each score is the best average score of the last two epochs: 32 epochs of 64 episodes each, Q-function swap after every episode \n",
    "\n",
    "### TD Tuning\n",
    "\n",
    "$\\gamma = 1.00$  \n",
    "\n",
    "| Param                |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|----------------------|------| ------- | ------- | ------- | ------- | ------- |------| ----- |\n",
    "| $$\\alpha = 0.99$$    |&nbsp;| 0.251   | 0.198   | 0.146   | 0.147   | 0.210   |&nbsp;| 0.190 |\n",
    "| $$\\alpha = 0.75$$    |&nbsp;| 0.179   | 0.185   | 0.239   | 0.179   | 0.175   |&nbsp;| 0.191 |\n",
    "| $$\\alpha = 0.50$$    |&nbsp;| 0.204   | 0.100   | 0.238   | 0.158   | 0.139   |&nbsp;| 0.168 |\n",
    "| $$\\alpha = 0.25$$    |&nbsp;| 0.294   | 0.170   | 0.107   | 0.223   | 0.147   |&nbsp;| 0.188 |\n",
    "| $$\\alpha = 0.125$$   |&nbsp;| 0.187   | 0.254   | 0.177   | 0.163   | 0.204   |&nbsp;| 0.197 |  \n",
    "| $$\\alpha = 0.0625$$  |&nbsp;| 0.113   | 0.241   | 0.353   | 0.134   | 0.749   |&nbsp;| 0.318 |\n",
    "| $$\\alpha = 0.03125$$ |&nbsp;| 0.231   | 0.322   | 0.018   | 0.098   | 0.000   |&nbsp;| 0.134 |\n",
    "| $$\\alpha = 0.02344$$ |&nbsp;| 1.289   | 0.119   | 0.380   | 0.168   | 0.086   |&nbsp;| 0.408 |\n",
    "| $$\\alpha = \\mathbf{0.02148}$$ |&nbsp;| 0.498   | 0.813   | 0.286   | 7.130   | 0.281   |&nbsp;| **1.802** |\n",
    "| $$\\alpha = 0.01953$$ |&nbsp;| 0.234   | 0.113   | 0.445   | 0.119   | 1.637   |&nbsp;| 0.510 |\n",
    "| $$\\alpha = 0.01758$$ |&nbsp;| 0.175   | 0.249   | 0.217   | 0.047   | 1.006   |&nbsp;| 0.339 |\n",
    "| $$\\alpha = 0.01563$$ |&nbsp;| 0.281   | 1.371   | 0.066   | 0.037   | 0.751   |&nbsp;| 0.501 |\n",
    "| $$\\alpha = 0.00782$$ |&nbsp;| 0.133   | 0.241   | 0.149   | 0.493   | 0.146   |&nbsp;| 0.232 |\n",
    "| $$\\alpha = 0.00391$$ |&nbsp;| 0.037   | 0.626   | 1.000   | 0.525   | 0.139   |&nbsp;| 0.465 |\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "### $\\gamma$ Tuning\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "\n",
    "| Param                 |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "| :-------------------- |------| ------- | ------- | ------- | ------- | ------- |------| ----- |\n",
    "| $$\\gamma = 0.999$$    |&nbsp;| 0.925   | 0.247   | 0.145   | 0.364   | 3.038   |&nbsp;| 0.944 |\n",
    "| $$\\gamma = 0.99$$     |&nbsp;| 0.011   | 0.448   | 0.453   | 0.915   | 0.013   |&nbsp;| 0.368 |\n",
    "| $$\\gamma = 0.85$$     |&nbsp;| 0.314   | 2.778   | 0.275   | 1.183   | 0.079   |&nbsp;| 0.926 |\n",
    "| $$\\gamma = 0.80$$     |&nbsp;| 0.082   | 0.033   | 0.173   | 0.251   | 1.741   |&nbsp;| 0.456 |\n",
    "| $$\\gamma = 0.75$$     |&nbsp;| 0.283   | 0.239   | 2.223   | 0.264   | 0.753   |&nbsp;| 0.752 |\n",
    "| $$\\gamma = 0.50$$     |&nbsp;| 0.167   | 0.289   | 0.474   | 0.266   | 0.230   |&nbsp;| 0.285 |\n",
    "\n",
    " \n",
    "### Double-Q Tuning, Swap Evey N Episodes  \n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "Epochs = 32\n",
    "\n",
    "| Param                 |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|-----------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| $$\\%\\ \\ 2$$           |&nbsp;|  0.570  | 0.132   | 1.053   | 0.731   | 0.900   |&nbsp;| 0.677 |\n",
    "| $$\\%\\ \\ 4$$           |&nbsp;|  1.313  | 0.087   | 2.282   | 0.417   | 0.409   |&nbsp;| 0.901 |\n",
    "| $$\\%\\ \\ 8$$           |&nbsp;|  0.097  | 0.040   | 0.621   | 0.030   | 0.608   |&nbsp;| 0.279 |\n",
    "| $$\\%16$$              |&nbsp;|  0.260  | 0.219   | 0.054   | 0.407   | 0.845   |&nbsp;| 0.357 |\n",
    "| $$\\%32$$              |&nbsp;|  0.674  | 0.130   | 0.301   | 0.286   | 0.313   |&nbsp;| 0.341 |\n",
    "| $$\\%\\mathbf{64}$$     |&nbsp;| 15.261  | 2.072   | 0.380   | 0.056   | 0.727   |&nbsp;| **3.699** |\n",
    "  \n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "Episodes = 128  \n",
    "Epochs = 16  \n",
    "\n",
    "| Param            |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| $$\\%\\ \\ \\ \\ 4$$  |&nbsp;|  1.333  | 0.117   | 0.138   | 0.430   | 0.149   |&nbsp;| 0.433 |\n",
    "| $$\\%\\ \\ 64$$     |&nbsp;|  0.166  | 0.110   | 0.240   | 1.615   | 0.049   |&nbsp;| 0.436 |\n",
    "| $$\\%128$$        |&nbsp;|  2.700  | 0.228   | 0.222   | 0.183   | 0.002   |&nbsp;| 0.667 |\n",
    "\n",
    "  \n",
    "### Trace Tuning\n",
    "\n",
    "This is not a Sutton and Barto eligibility trace.  Instead, I look for $N_\\text{peak}$ peaks in the value history of the episode, and apply the learning rule in reverse order from the peak through $N_\\text{step}$ previous timesteps, with a $\\lambda$ decay each step. This is neither classical nor rigorous, and if it does not work, then I will switch to one of the classic $Q(\\lambda)$ methods found in Sutton and Barto.\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "$\\lambda = 0.95$  \n",
    "Peaks = 16  \n",
    "Episodes = 64  \n",
    "Swap = \\%64  \n",
    "Epochs = 32  \n",
    "\n",
    "\n",
    "| Param                  |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| Steps: &nbsp; &nbsp; 4 |&nbsp;| 0.418   | 0.098   | 0.231   | 0.080   | 0.571   |&nbsp;| 0.280 |\n",
    "| Steps: &nbsp; 16       |&nbsp;| 0.540   | 0.365   | 1.629   | 0.215   | 0.470   |&nbsp;| 0.644 |\n",
    "| Steps: &nbsp; 64       |&nbsp;| 1.197   | 0.410   | 1.187   | 1.017   | 0.975   |&nbsp;| 0.957 |\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "$\\lambda = 0.99$  \n",
    "Peaks = 32  \n",
    "Episodes = 64  \n",
    "Swap = \\%64  \n",
    "Epochs = 32  \n",
    "\n",
    "Variations\n",
    "* Only trace non-zero uptime episodes, Result: Many more zero score episodes and no improvement\n",
    "* Only trace episodes with above-average uptime, Result: Many more zero score episodes and no improvement\n",
    "* Re-run the episode until there is non-zero uptime, Result: Extremely slow and no improvement in average uptime  \n",
    "\n",
    "| Param                  |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| Steps: &nbsp; 64       |&nbsp;| 0.265   |  1.204  | 6.919   | 1.575   |  3.940  |&nbsp;| 2.781 |\n",
    "| Steps: &nbsp; 96       |&nbsp;| 0.212   | 12.012  | 0.213   | 0.195   | 13.896  |&nbsp;| 5.306 |\n",
    "| **Steps: 128**         |&nbsp;| 0.065   | 17.540  | 0.493   | 0.325   | 15.345  |&nbsp;| **6.754** |\n",
    "| Steps: 192             |&nbsp;| 0.271   |  0.127  | 0.220   | 0.299   |  0.600  |&nbsp;| 0.303 |\n",
    "| Steps: 256             |&nbsp;| 0.958   |  2.014  | 0.947   | 0.433   |  0.045  |&nbsp;| 0.879 |\n",
    "| Steps: 512             |&nbsp;| 1.487   |  0.343  | 0.159   | 1.057   |  0.640  |&nbsp;| 0.737 |  \n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "$\\lambda = 0.99$  \n",
    "Steps = 128  \n",
    "Episodes = 64  \n",
    "Swap = \\%64  \n",
    "Epochs = 32  \n",
    "\n",
    "| Param                  |      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |      | Mean  |\n",
    "|------------------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| Peaks: &nbsp; &nbsp; 2 |&nbsp;|  0.635  | 5.336   | 0.198   |  0.069  | 0.624   |&nbsp;|       |\n",
    "| Peaks: &nbsp; &nbsp; 4 |&nbsp;|  0.086  | 0.270   | 0.286   |  0.704  | 0.122   |&nbsp;|       |\n",
    "| Peaks: &nbsp; &nbsp; 8 |&nbsp;| 17.591  | 0.446   | 0.334   |  0.317  | 1.206   |&nbsp;| 3.979 |\n",
    "| Peaks: &nbsp; 16       |&nbsp;|  0.449  | 6.393   | 0.763   | 11.826  | 0.089   |&nbsp;|       |\n",
    "| Peaks: &nbsp; 64       |&nbsp;|  0.455  | 0.910   | 1.000   |  0.555  | 0.891   |&nbsp;|       |\n",
    "| Peaks: 128             |&nbsp;|  0.354  | 0.242   | 1.780   |  0.393  | 0.166   |&nbsp;|       |\n",
    "\n",
    "\n",
    "### Value Estimation Tuning\n",
    "\n",
    "$\\alpha = \\mathbf{0.02148}$  \n",
    "$\\gamma = \\mathbf{1.00}$  \n",
    "$\\lambda = 0.99$  \n",
    "Peaks =  32  \n",
    "Steps = 128 \n",
    "Episodes = 64  \n",
    "Swap = \\%64  \n",
    "Epochs = 32 \n",
    "\n",
    "| Param 1   | Param 2     |&nbsp;| Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |&nbsp;| Mean  |\n",
    "|-----------|-------------|------|---------|---------|---------|---------|---------|------|-------|\n",
    "| Inputs: 2 | Outputs: 1  |      | 1.172   | 0.413   |  0.057  |         |         |      |       | \n",
    "| Inputs: 2 | Outputs: 2  |      | 1.327   | 0.258   | 12.772  |         |         |      |       |\n",
    "| Inputs: 3 | Outputs: 1  |      | 0.365   | 0.575   |  0.033  |         |         |      |       | \n",
    "| Inputs: 3 | Outputs: 2  |      |         |         |         |         |         |      |       | \n",
    "| Inputs: 3 | Outputs: 3  |      |         |         |         |         |         |      |       | \n",
    "| Inputs: 4 | Outputs: 2  |      |         |         |         |         |         |      |       | \n",
    "| Inputs: 4 | Outputs: 3  |      |         |         |         |         |         |      |       | \n",
    "| Inputs: 4 | Outputs: 4  |      |         |         |         |         |         |      |       | \n",
    "| Inputs: 5 | Outputs: 3  |      |         |         |         |         |         |      |       | \n",
    "\n",
    "### Learning Rate Decay\n",
    "\n",
    "### `!!!` IMPLEMENTATION CHECK `!!!`\n",
    "\n",
    "### Blend: Best Episode\n",
    "\n",
    "$\\beta = 0.07$:  \n",
    "$\\beta = 0.15$: 0.244\n",
    "\n",
    "| Method      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 | Mean |\n",
    "| ----------- | ------- | ------- | ------- | ------- | ------- | ---- |\n",
    "| Blend (Epi) |         |         |         |         |         |      |\n",
    "| Blend (Epo) |         |         |         |         |         |      |\n",
    "| TD          |         |         |         |         |         |      |\n",
    "| TD  + ????? |         |         |         |         |         |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a60c1d8a-58c5-4719-89c8-b69bf6623266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Process(`\u001b[4mplay\u001b[24m \u001b[4m-nq\u001b[24m \u001b[4m-t\u001b[24m \u001b[4malsa\u001b[24m \u001b[4msynth\u001b[24m \u001b[4m3\u001b[24m \u001b[4msine\u001b[24m \u001b[4m300\u001b[24m`, ProcessExited(0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(`play -nq -t alsa synth 3 sine 300`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5ade7-5f94-43b1-837f-85ccdd6b5c93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
