{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56af411-53e7-4593-ac4d-b06743557135",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f310b05c-a806-4927-a997-3e46aa8e4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357714b-adcc-4b58-97f4-9e3816bfe05c",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308c62c8-c0a4-48a4-ac32-c02213ae46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              # -10.0 +10.0 ] # xDot\n",
    "              -10.0 +10.0 ] # xDot\n",
    "              # -50.0 +50.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d96a4762-c92c-40fe-b5cf-cf73261bc7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "select_X_vector"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4adb1-6cb4-421b-9c80-e3f8b01d028e",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1c6de-1ef8-4b6f-b33e-4dbd821cdee0",
   "metadata": {},
   "source": [
    "## Agent Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a6901f-3c7e-44d9-8590-a9dd8a665495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres = Fmax/Fdiv\n",
    "# Fres = 1.0 #2.0 #1.0\n",
    "\n",
    "# Construct grid of anchors\n",
    "# G    = regular_grid_pts_nD( _Q_DOMAIN, [ 1.0, 1.0, 1.0, 1.0, Fres ] );\n",
    "# G    = regular_grid_pts_nD( _Q_DOMAIN, [ 2.0, 2.0, 2.0, 2.0, Fres ] );\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ 4.0, 4.0, 4.0, 4.0, Fres ] );\n",
    "# G    = regular_grid_pts_nD( _Q_DOMAIN, [ 5.0, 5.0, 5.0, 5.0, Fres ] );\n",
    "# G    = regular_grid_pts_nD( _Q_DOMAIN, [ 7.5, 7.5, 7.5, 7.5, Fres ] );\n",
    "# G    = regular_grid_pts_nD( _Q_DOMAIN, [ 12.0, 8.0, 12.0, 8.0, Fres ] );\n",
    "nPts = size( G )[2]; # - Number of anchors\n",
    "mDim = size( G )[1]; # - Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb301a-dc3a-4416-b6d9-eb38f7947812",
   "metadata": {},
   "source": [
    "## Agent Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95773886-9bda-4922-a87c-b7486e1f4064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Params #####\n",
    "scale = 7.0; #1.650; # ----------- scale\n",
    "vNN   =  5 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309b199-8eae-4ebf-b59f-fd594e7a63d2",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7413bb-a70b-493f-95db-238d52ce6978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertical_score_s"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    # if thetaDotN > maxThetaDot\n",
    "    #     R -= thetaDotN\n",
    "    # end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296d0fd-79da-43f8-94a4-c3d71f94e43b",
   "metadata": {},
   "source": [
    "# Optimal Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62c0f6d-7983-49ca-8ef8-1a84be5e0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01\n",
    "T         = Int64((1/ts)*dur_s)\n",
    "N_0       = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0       = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states    = zeros( size( X_0, 1 ), T )\n",
    "actions   = zeros( T );\n",
    "bestXs    = zeros( size( X_0, 1 ), T )\n",
    "bestAs    = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fd3c77e-0032-4f2d-b83e-c1a2fc7c991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ebb31b-ee8e-42a7-b70a-fe9f07bb69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bedfabf5-4eb2-4289-929f-15e90bc57402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exchange_nonzeros"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19a5d94e-4933-42ab-a0dd-07b68fbb9b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.6000000000000003, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.38000000000000017, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] Array",
      "    @ ./boot.jl:457 [inlined]",
      "  [2] Array",
      "    @ ./boot.jl:466 [inlined]",
      "  [3] Array",
      "    @ ./boot.jl:474 [inlined]",
      "  [4] cat_similar",
      "    @ ./abstractarray.jl:1646 [inlined]",
      "  [5] _cat_t",
      "    @ ./abstractarray.jl:1709 [inlined]",
      "  [6] #cat_t#136",
      "    @ ./abstractarray.jl:1705 [inlined]",
      "  [7] _cat",
      "    @ ./abstractarray.jl:1703 [inlined]",
      "  [8] #cat#141",
      "    @ ./abstractarray.jl:1861 [inlined]",
      "  [9] vcat(::Vector{Any}, ::Float64)",
      "    @ Base ./abstractarray.jl:1772",
      " [10] regular_grid_helper(tickLists::Vector{Any}, i::Int64, curVec::Nothing, allPts::Nothing)",
      "    @ Main ~/za_Other/CogArch/CORVID/Contin_Q-Learning/utils.jl:34",
      " [11] regular_grid_helper",
      "    @ ~/za_Other/CogArch/CORVID/Contin_Q-Learning/utils.jl:15 [inlined]",
      " [12] regular_grid_pts_nD",
      "    @ ~/za_Other/CogArch/CORVID/Contin_Q-Learning/utils.jl:84 [inlined]",
      " [13] learned_action_for_state(X::Vector{Float64}, domain::Matrix{Float64}, res::Vector{Float64}, ts::Float64)",
      "    @ Main ./In[6]:68",
      " [14] top-level scope",
      "    @ ./In[11]:47",
      " [15] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [16] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "gamma     = 0.99 #0.75 #0.950; # ---------- FIXME\n",
    "epsMin    = 0.05 # 0.05\n",
    "epsMax    = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes  =  64 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs    =  64 #128 #64 # 32 #16\n",
    "EXPrand   = 1.00 #0.25 #0.5 # 0.75\n",
    "bgn       = time()\n",
    "Alpha     = 0.875\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "averages  = []\n",
    "aMargin   = (pi/180)*15.0;\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/episodes\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    # V = blend_alpha_of_A_into_B( 0.75, vBst, V ) # 0.25 # 0.50 # 0.75\n",
    "    \n",
    "    # if m % 2 == 0\n",
    "    #     V = copy( vBst )\n",
    "    # end\n",
    "    \n",
    "    # V        = copy( vBst )\n",
    "    # VS, vBst = exchange_nonzeros( VS, vBst )\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        X  = X_0\n",
    "\n",
    "        for k = 1:T\n",
    "\n",
    "            # 1. Choose action\n",
    "            if rand() < epsilon\n",
    "                # A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                if rand() < EXPrand \n",
    "                    A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                else\n",
    "                    A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                end\n",
    "            else\n",
    "\n",
    "                A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                if A == 1000.0 # Indicates no values in this region\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                end\n",
    "                \n",
    "                # A = learned_action_for_state_exp( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "\n",
    "            end\n",
    "\n",
    "            # 2. Cache last state\n",
    "            # println( typeof( select_X_vector( X ) ) )\n",
    "            # println( typeof( A ) )\n",
    "            qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "            # 3. Generate the next stae\n",
    "            Xp = cartpole_dyn( X, A, ts )\n",
    "            # println( \"Xp: \", Xp )\n",
    "\n",
    "            # 4. Collect reward R( s, a, s' )\n",
    "            R_t = cartpole_reward( Xp )\n",
    "            # println( \"R_t: \", R_t )\n",
    "\n",
    "            # 5. Get the optimal action at the next state\n",
    "            a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "            # a_tp1_opt = learned_action_for_state( Xp, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "            # if a_tp1_opt == 1000.0 # Indicates no values in this region\n",
    "            #     a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "            # end\n",
    "\n",
    "            # 6. Compute the value at the next state\n",
    "\n",
    "            V_tp1_opt = query_value_fuzzy( \n",
    "            # V_tp1_opt = query_value_fuzzy_exp( \n",
    "\n",
    "                Q_kdTree, G, V, \n",
    "                get_Q( \n",
    "                    select_X_vector( Xp ), \n",
    "                    a_tp1_opt \n",
    "                ); \n",
    "                k = vNN \n",
    "            )\n",
    "            if isnan( V_tp1_opt )\n",
    "                V_tp1_opt = 0.0\n",
    "            end\n",
    "\n",
    "            # println( \"R_t: \", R_t, \"V_tp1: \", V_tp1_opt )\n",
    "\n",
    "\n",
    "            # 7. Blend the value back into nearest points\n",
    "\n",
    "            idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "            # idxs, wgts = query_contrib_to_neighbors_exp( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "            nNear      = size( idxs, 1 )\n",
    "            for i = 1:nNear\n",
    "                j    = idxs[i]\n",
    "                if !isnan( wgts[i] ) \n",
    "\n",
    "                    VS[j]   =  R_t + gamma * V_tp1_opt # --------------------------------- 0.66 | 20.9\n",
    "                    vAll[j] =  R_t + gamma * V_tp1_opt # --------------------------------- 0.66 | 20.9\n",
    "                    \n",
    "                    # VS[j] = wgts[i] * ( R_t + gamma * V_tp1_opt ) # -------------------- 0.20 |  0.3\n",
    "                    # VS[j] = R_t + wgts[i] * ( gamma * V_tp1_opt ) # -------------------- 0.04 |  8.9\n",
    "                    # VS[j] = ( 1.0 - wgts[i] )*R_t + wgts[i]*( gamma * V_tp1_opt ) # ---- 0.52 |  3.9\n",
    "                    # VS[j] = wgts[i]*R_t + gamma*V_tp1_opt # ---------------------------- 0.00 |  4.3\n",
    "                    # VS[j] = (1.0 - wgts[i])*V[j] + wgts[i]*( R_t + gamma * V_tp1_opt ) # 0.00 |  1.6\n",
    "                    \n",
    "                end\n",
    "                # println( [i j], \" \", V[j] )\n",
    "            end\n",
    "\n",
    "            states[:,k] = Xp\n",
    "            actions[k]  = A\n",
    "\n",
    "            X = Xp\n",
    "        end\n",
    "\n",
    "        s_l    = vertical_score_s( states, aMargin, ts )\n",
    "        s_Totl += s_l\n",
    "        # s_l = (1.0 - Alpha)*s_Prev  + Alpha*vertical_score_s( states, aMargin, ts )\n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        # if s_l > s_Prev\n",
    "        #     epsilon -= deltaEp\n",
    "        # end\n",
    "        epsilon -= deltaEp\n",
    "\n",
    "        # if l%2 == 0\n",
    "        #     vSwp = copy( VS   )\n",
    "        #     VS   = copy( V    )\n",
    "        #     V    = copy( vSwp )\n",
    "        #     println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        # end\n",
    "        \n",
    "        # if l%2 == 0\n",
    "        #     V = copy( VS )\n",
    "        #     println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        # else\n",
    "        #     V = copy( vBst )\n",
    "        # end\n",
    "        \n",
    "        vSwp = copy( VS   )\n",
    "        VS   = copy( V    )\n",
    "        V    = copy( vSwp )\n",
    "        \n",
    "        # s_Prev = s_l\n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "    \n",
    "    if s_Avg > bestAvg\n",
    "        bestAvg = s_Avg\n",
    "        vBAv    = copy( V ) # Try a blend of both next\n",
    "        vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V )\n",
    "    end\n",
    "    \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d584ed5-181d-41b3-8a94-2268b9f5b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6300000000000003 [s]\n"
     ]
    }
   ],
   "source": [
    "println( \"Best Score: \", bestScore, \" [s]\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8282328f-994d-48fa-b1c5-ae58576d81ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vector_fullness"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Return fraction of elements that are nonzero\n",
    "\"\"\"\n",
    "function vector_fullness( vec )\n",
    "    N = size( vec, 1 )\n",
    "    c = 0\n",
    "    for j = 1:N\n",
    "        if vec[j] != 0.0\n",
    "            c += 1\n",
    "        end\n",
    "    end\n",
    "    return c/N\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c06a12-c018-47c8-bb8e-9ac038e3eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternate Weights are 0.08391203703703703 full\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: vTrn not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: vTrn not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[14]:2",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "println( \"Alternate Weights are \", vector_fullness( VS   ), \" full\" )\n",
    "println( \"Trained   Weights are \", vector_fullness( vTrn ), \" full\" )\n",
    "println( \"Best      Weights are \", vector_fullness( vBst ), \" full\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16994cde-95a7-4dc3-8ca6-75d4238a2c77",
   "metadata": {},
   "source": [
    "# Optimal Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8230a37-b186-4a22-91ae-e05cc4a88455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimal_cartpole_agent_P2_episode"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use the desginated value table to navigate cartpole env\n",
    "WARNING: This function OVERWRITES `V`!, so make sure to save the Q-tables that you need ELSEWHERE!!\n",
    "\"\"\"\n",
    "function optimal_cartpole_agent_P2_episode( Vfunc, T, X_0, ts, epsilon )\n",
    "    global V\n",
    "    \n",
    "    # Init #\n",
    "    V      = copy( Vfunc )\n",
    "    record = zeros( size( X_0, 1 ), T ) # Recording of all system states\n",
    "    s_Totl = 0.0\n",
    "    X      = X_0\n",
    "\n",
    "    for k = 1:T\n",
    "\n",
    "        # 1. Choose action\n",
    "        if rand() < epsilon\n",
    "            A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "        else\n",
    "\n",
    "            A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "            if A == 1000.0 # Indicates no values in this region\n",
    "                # A = optimal_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # 3. Generate the next stae\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "        record[:,k] = Xp\n",
    "\n",
    "        X = Xp\n",
    "    end\n",
    "\n",
    "    return record\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e62a0f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.6300000000000003 [s]\n"
     ]
    }
   ],
   "source": [
    "println( \"Best Score: \", bestScore, \" [s]\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32d90a86",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: vTrn not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: vTrn not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[17]:11",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "episodes = 20\n",
    "scoreSum = 0.0\n",
    "trials   = [ 0.05, 0.075, 0.0875, 0.10, 0.1125, 0.125, 0.15, 0.1625, 0.175, 0.1875, 0.20 ]  \n",
    "\n",
    "for trlEps in trials\n",
    "    \n",
    "    scoreSum = 0.0\n",
    "    \n",
    "    for epNo = 1:episodes\n",
    "\n",
    "        res = optimal_cartpole_agent_P2_episode( vTrn, T, X_0, ts, trlEps )\n",
    "        s_e = vertical_score_s( res, aMargin, ts )\n",
    "        scoreSum += s_e\n",
    "        if epNo % 5 == 0\n",
    "            println( \"\\tEp \", epNo, \", Score: \", s_e )\n",
    "        end\n",
    "\n",
    "    end\n",
    "    println( \"Average Trained Score (out of \", episodes, \"): \", scoreSum/episodes, \", epsilon = \", trlEps, \"\\n\" )\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1b05ede-3ac0-4f38-bc8c-3f7925ea2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose vBst as primary!\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: vTrn not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: vTrn not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[18]:17",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "# Best 75 : Train 25\n",
    "# vB75 = blend_alpha_of_A_into_B( 0.75, vBst, vTrn )\n",
    "# vB71 = blend_alpha_of_A_into_B( 0.7188, vBst, vBAv )\n",
    "# vB75 = blend_alpha_of_A_into_B( 0.75, vBst, vAll )\n",
    "\n",
    "eps = 0.000\n",
    "\n",
    "if vertical_score_s( optimal_cartpole_agent_P2_episode( vBAv, T, X_0, ts, eps ), aMargin, ts ) > vertical_score_s( optimal_cartpole_agent_P2_episode( vBst, T, X_0, ts, eps ), aMargin, ts )\n",
    "    println( \"Chose vBAv as primary!\" )\n",
    "    vAug, _ = exchange_nonzeros( vBAv, vBst ) # LARGE improvement\n",
    "else\n",
    "    println( \"Chose vBst as primary!\" )\n",
    "    vAug, _ = exchange_nonzeros( vBst, vBAv )\n",
    "end\n",
    "\n",
    "\n",
    "if vertical_score_s( optimal_cartpole_agent_P2_episode( VS, T, X_0, ts, eps ), aMargin, ts ) > vertical_score_s( optimal_cartpole_agent_P2_episode( vTrn, T, X_0, ts, eps ), aMargin, ts )\n",
    "    println( \"Chose VS as secondary!\" )\n",
    "    vAug, _ = exchange_nonzeros( vAug, VS )\n",
    "    vAug, _ = exchange_nonzeros( vAug, vTrn )\n",
    "else\n",
    "    println( \"Chose vTrn as secondary!\" )\n",
    "    vAug, _ = exchange_nonzeros( vAug, vTrn )\n",
    "    vAug, _ = exchange_nonzeros( vAug, VS )\n",
    "end\n",
    "\n",
    "episodes = 100\n",
    "scoreSum = 0.0\n",
    "trials   = [ 0.00, 0.01, 0.025, 0.05, 0.0625, 0.075, 0.0875, 0.10, 0.125, 0.1375, 0.15, 0.1625, 0.175, 0.1875, 0.20 ]  \n",
    "\n",
    "for trlEps in trials\n",
    "    \n",
    "    scoreSum = 0.0\n",
    "    \n",
    "    for epNo = 1:episodes\n",
    "\n",
    "        res = optimal_cartpole_agent_P2_episode( vAug, T, X_0, ts, trlEps )\n",
    "        s_e = vertical_score_s( res, aMargin, ts )\n",
    "        scoreSum += s_e\n",
    "        if epNo % 10 == 0\n",
    "            println( \"\\tEp \", epNo, \", Score: \", s_e )\n",
    "        end\n",
    "\n",
    "    end\n",
    "    println( \"Average vAug Score (out of \", episodes, \"): \", scoreSum/episodes, \", epsilon = \", trlEps, \"\\n\" )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5df3c49-bb3d-4899-b15f-b6fa334b59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEp 10, Score: 0.0\n",
      "\tEp 20, Score: 0.0\n",
      "\tEp 30, Score: 0.0\n",
      "\tEp 40, Score: 0.0\n",
      "\tEp 50, Score: 0.0\n",
      "\tEp 60, Score: 0.0\n",
      "\tEp 70, Score: 0.0\n",
      "\tEp 80, Score: 0.0\n",
      "\tEp 90, Score: 0.0\n",
      "\tEp 100, Score: 0.0\n",
      "Average Best Score (out of 100): 0.0, epsilon = 0.0\n",
      "\n",
      "\tEp 10, Score: 0.0\n",
      "\tEp 20, Score: 0.23000000000000007\n",
      "\tEp 30, Score: 0.0\n",
      "\tEp 40, Score: 0.0\n",
      "\tEp 50, Score: 0.0\n",
      "\tEp 60, Score: 0.0\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] angular_accel(F::Float64, N_c::Float64, thetaDot::Float64, theta::Float64, xDot::Float64)",
      "   @ Main ~/za_Other/CogArch/CORVID/Contin_Q-Learning/env_cartpole.jl:31",
      " [2] cartpole_dyn(X::Vector{Float64}, A::Float64, ts::Float64)",
      "   @ Main ~/za_Other/CogArch/CORVID/Contin_Q-Learning/env_cartpole.jl:66",
      " [3] learned_action_for_state(X::Vector{Float64}, domain::Matrix{Float64}, res::Vector{Float64}, ts::Float64)",
      "   @ Main ./In[6]:74",
      " [4] optimal_cartpole_agent_P2_episode(Vfunc::Vector{Float64}, T::Int64, X_0::Vector{Float64}, ts::Float64, epsilon::Float64)",
      "   @ Main ./In[15]:21",
      " [5] top-level scope",
      "   @ ./In[19]:11",
      " [6] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "scoreSum = 0.0\n",
    "trials   = [ 0.00, 0.01, 0.025, 0.05, 0.0625, 0.075, 0.0875, 0.10, 0.125, 0.1375, 0.15, 0.1625, 0.175, 0.1875, 0.20 ]  \n",
    "\n",
    "for trlEps in trials\n",
    "    \n",
    "    scoreSum = 0.0\n",
    "    \n",
    "    for epNo = 1:episodes\n",
    "\n",
    "        res = optimal_cartpole_agent_P2_episode( vBst, T, X_0, ts, trlEps )\n",
    "        s_e = vertical_score_s( res, aMargin, ts )\n",
    "        scoreSum += s_e\n",
    "        if epNo % 10 == 0\n",
    "            println( \"\\tEp \", epNo, \", Score: \", s_e )\n",
    "        end\n",
    "\n",
    "    end\n",
    "    println( \"Average Best Score (out of \", episodes, \"): \", scoreSum/episodes, \", epsilon = \", trlEps, \"\\n\" )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2313cd17-3d3d-4644-a837-edda2e464e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "render_cartpole_episode"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create an animated GIF of a cartpole state sequence\n",
    "\"\"\"\n",
    "function render_cartpole_episode( Xs, ts )\n",
    "    \n",
    "    # 0. Init\n",
    "    FPS  = 30\n",
    "    T    = size( Xs, 2 )\n",
    "    skip = Int64( floor((1/FPS)/ts) )\n",
    "    frms = Int64( ceil(T/skip) )\n",
    "\n",
    "    # 1. Background func\n",
    "    function backdrop(scene, framenumber)\n",
    "        background(\"grey50\")\n",
    "    end\n",
    "\n",
    "    # 2. Per frame func\n",
    "    function frame( scene, framenumber )\n",
    "\n",
    "        # Extract relevant state\n",
    "        theta = Xs[3,(framenumber-1)*skip+1]\n",
    "        x     = Xs[6,(framenumber-1)*skip+1]/4\n",
    "        \n",
    "        # Track\n",
    "        sethue(\"blue\")\n",
    "        rule(Point(0, 0), 0.0)\n",
    "\n",
    "        # Cart\n",
    "        sethue(\"black\")\n",
    "        box(Point(x, 0), 70,  40, action = :fill)\n",
    "\n",
    "        # Pole\n",
    "        sethue(\"white\")\n",
    "        Luxor.translate( x, 0 )\n",
    "        Luxor.translate( @polar (-100, theta+pi/2.0) )\n",
    "        Luxor.rotate( theta )\n",
    "        box(Point(0, 0), 15, 200, action=:fill)\n",
    "\n",
    "    end\n",
    "\n",
    "    # 3. Instantiate movie\n",
    "    demo = Movie(800, 800, \"test\")\n",
    "\n",
    "    # 4. Populate movie\n",
    "    Luxor.animate( demo, [\n",
    "        Scene(demo, backdrop, 1:frms),\n",
    "        Scene(demo, frame, 1:frms)\n",
    "    ], creategif=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0334781b-ea6a-45ca-87c6-61c92b38dabd",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[21]:1",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "render_cartpole_episode( bestXs, ts )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ed95c-1a63-4d90-8f9a-cb4be49bc55d",
   "metadata": {},
   "source": [
    "# Spare Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4168e745-554b-46b9-bd06-4e8a262ccc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if false\n",
    "    Xmin, _ = findmin( bestXs, dims=2 )\n",
    "    Xmax, _ = findmax( bestXs, dims=2 )\n",
    "    println( \"Xmin\" )\n",
    "    display( Xmin )\n",
    "    println( \"\\nXmax\" )\n",
    "    display( Xmax )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bea5a2-ca26-4c63-b3c4-66a3e4788e70",
   "metadata": {},
   "source": [
    "## Corpus Learning Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36313a33-41d6-49cb-abb7-ab2c2a6716cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "corpus_iteration"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "One learning step from one predefined Q-state\n",
    "\"\"\"\n",
    "function corpus_iteration( X, A, ts )\n",
    "    global G, V\n",
    "    \n",
    "    # 1. Unpack state\n",
    "    # thetaDotDot = X[1] # Angular acceleration\n",
    "    # thetaDot    = X[2] # Angular velocity\n",
    "    # theta       = X[3] # Angle\n",
    "    # xDotDot     = X[4] # Linear acceleration\n",
    "    # xDot        = X[5] # Linear velocity\n",
    "    # x           = X[6] # Fulcrum position\n",
    "    # N_c         = X[7] # Normal force of the cart on the track\n",
    "    \n",
    "    # 2. Cache last state\n",
    "    # println( typeof( select_X_vector( X ) ) )\n",
    "    # println( typeof( A ) )\n",
    "    qLast = get_Q( select_X_vector( X ), A )\n",
    "    \n",
    "    # 3. Generate the next stae\n",
    "    Xp = cartpole_dyn( X, A, ts )\n",
    "    # println( \"Xp: \", Xp )\n",
    "    \n",
    "    # 4. Collect reward R( s, a, s' )\n",
    "    R_t = cartpole_reward( Xp )\n",
    "    # println( \"R_t: \", R_t )\n",
    "    \n",
    "    # 5. Get the optimal action at the next state\n",
    "    a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "    \n",
    "    # 6. Compute the value at the next state\n",
    "    \n",
    "    V_tp1_opt = query_value_fuzzy( \n",
    "    # V_tp1_opt = query_value_fuzzy_exp( \n",
    "        Q_kdTree, G, V, \n",
    "        get_Q( \n",
    "            select_X_vector( Xp ), \n",
    "            a_tp1_opt \n",
    "        ); \n",
    "        k = vNN \n",
    "    )\n",
    "    if isnan( V_tp1_opt )\n",
    "        V_tp1_opt = 0.0\n",
    "    end\n",
    "    \n",
    "    # 7. Blend the value back into nearest points\n",
    "    \n",
    "    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "    # idxs, wgts = query_contrib_to_neighbors_exp( Q_kdTree, G, V, qLast; k = bNN )\n",
    "    \n",
    "    nNear      = size( idxs, 1 )\n",
    "    for i = 1:nNear\n",
    "        j    = idxs[i]\n",
    "        if !isnan( wgts[i] ) \n",
    "            V[j] =  R_t + gamma * V_tp1_opt # -------------------------------- 0.8 | 6.06\n",
    "            # V[j] = wgts[i] * ( R_t + gamma * V_tp1_opt ) # --------------------\n",
    "            # V[j] = R_t + wgts[i] * ( gamma * V_tp1_opt ) # --------------------\n",
    "            # V[j] = ( 1.0 - wgts[i] )*R_t + wgts[i]*( gamma * V_tp1_opt ) # ---- \n",
    "            # V[j] = wgts[i]*R_t + gamma*V_tp1_opt # ----------------------------\n",
    "            # V[j] = (1.0 - wgts[i])*V[j] + wgts[i]*( R_t + gamma * V_tp1_opt ) # \n",
    "        end\n",
    "        # println( [i j], \" \", V[j] )\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005b0c96-5f21-4fc3-bf65-6045912ed9f5",
   "metadata": {},
   "source": [
    "# Generate Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db8d80a8-86d2-448c-960c-2aee4dc755f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dur_s   = 40\n",
    "ts      = 0.01\n",
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "pers    = [ 0.0625, 0.0702, 0.078, 0.0859, 0.0938, 0.11, 0.125, 0.1875, 0.25, 0.5, 1.0, 2.0, 4.0, 8.0 ]\n",
    "data    = zeros( size( pers, 1 ), size( X_0, 1 ), T )\n",
    "acts    = zeros( size( pers, 1 ), T )\n",
    "X       = X_0\n",
    "i       = 0\n",
    "\n",
    "# 2022-11-30: CORPUS HARMS PERFORMANCE\n",
    "if false\n",
    "    for period in pers\n",
    "        i += 1\n",
    "        data[i,:,1] = X_0\n",
    "        for j = 2:T\n",
    "            t  = (j-1)*ts\n",
    "            A  = -Fmax * cos( (t/period)*2*pi )\n",
    "            Xp = cartpole_dyn( X, A, ts )\n",
    "            data[i,:,j] = Xp \n",
    "            acts[i,j]   = A\n",
    "            X = Xp\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf87970-48e8-4eb4-bf7d-31a9ae6715fc",
   "metadata": {},
   "source": [
    "# Learn Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9038c73-44d8-47f0-952d-304e2e90aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus training SKIPPED!\n"
     ]
    }
   ],
   "source": [
    "# 2022-11-30: CORPUS HARMS PERFORMANCE\n",
    "if false\n",
    "    i = 0\n",
    "    for period in pers\n",
    "        i += 1\n",
    "        for j = 1:T\n",
    "            X = data[i,:,j]\n",
    "            A = acts[i,j]\n",
    "            corpus_iteration( X, A, ts )\n",
    "        end\n",
    "        println( \"Corpus Iteration \", i, \" score: \", vertical_score_s( data[i,:,:], aMargin, ts ) )\n",
    "    end\n",
    "else\n",
    "    println( \"Corpus training SKIPPED!\" )\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
