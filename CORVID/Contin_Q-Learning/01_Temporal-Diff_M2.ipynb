{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cefc7-7c60-4838-9399-26a98ec9736e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43290374-89de-4616-8800-c86799248c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "using DataStructures\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851743ab-a511-40fb-850b-bf90efa9232d",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39765-4abe-409a-bea1-f44fa8ec2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              -10.0 +10.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf920d4-46af-4f22-8933-c3db011ff716",
   "metadata": {},
   "source": [
    "# Q-Learning Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f605b904-b397-4617-9dbe-a27c0b4fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Return the indices and scores of all the peak rewards in the data\n",
    "\"\"\"\n",
    "function find_state_history_R_peaks( X_hist, N_pks )\n",
    "    \n",
    "    epLen   = size( X_hist, 2 )\n",
    "    rising  = false\n",
    "    lastVal = 1e9\n",
    "    lastRis = false\n",
    "    pqPeaks = PriorityQueue();\n",
    "    rtnPeak = []\n",
    "    \n",
    "    for j = 1:epLen\n",
    "        X       = X_hist[:,j]\n",
    "        currVal = cartpole_reward( X )\n",
    "        rising  = (currVal > lastVal)\n",
    "        if (!rising) && lastRis\n",
    "            pqPeaks[j] = -currVal # Store the current index at its current (negative) value\n",
    "        end\n",
    "        lastVal = currVal\n",
    "        lastRis = rising\n",
    "    end\n",
    "    for i = 1:min( N_pks, length( pqPeaks ) )\n",
    "        append!( rtnPeak, dequeue!( pqPeaks ) )\n",
    "    end\n",
    "    \n",
    "    return rtnPeak;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d663e-1ccd-441f-807f-44f84a43e4d0",
   "metadata": {},
   "source": [
    "# Q-Function Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91f06c-df14-4fe7-b81d-12c3184b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721c7-88a9-4b57-bf9f-ad9f9acbf786",
   "metadata": {},
   "source": [
    "# CartPole Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc4097d-9b96-453c-ba4f-4b06fce7fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083b48-38dc-4616-979a-da8874303d32",
   "metadata": {},
   "source": [
    "# Agent Data Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f648d5-8d8e-4da4-bd1e-3f3d9ec7c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres     = Fmax/Fdiv\n",
    "spaceDiv = 4.0 # 1.0 # 2.0 # 5.0 # 7.5  \n",
    "\n",
    "### Construct grid of anchors ###\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ spaceDiv, spaceDiv, spaceDiv, spaceDiv, Fres ] );\n",
    "nPts = size( G )[2]; # ------- Number of anchors\n",
    "mDim = size( G )[1]; # ------- Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db1609-9df1-438b-9675-0286bf01a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states  = zeros( size( X_0, 1 ), T )\n",
    "actions = zeros( T );\n",
    "bestXs  = zeros( size( X_0, 1 ), T )\n",
    "bestAs  = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9f1ef-79bc-41fd-b6e9-ab0554460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d49b4c6-8353-4a01-8a16-9b544e1ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c954412-18b9-45a8-97a6-e61cf19f15d2",
   "metadata": {},
   "source": [
    "# Agent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358ff3d-44a5-491e-9597-0a0a73c6b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Q(TD)-Learning Params #####\n",
    "scale = 7.5; #1.650; # ----------- scale\n",
    "vNN   =  4 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\"\n",
    "\n",
    "alpha    = 0.15\n",
    "gamma    = 0.99 \n",
    "epsMin   = 0.00 # Last iter is policy eval\n",
    "epsMax   = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes =  64 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs   =  64 #128 #64 # 32 #16\n",
    "EXPrand  = 1.00 #0.25 #0.5 # 0.75\n",
    "Alpha    = 0.875\n",
    "aMargin  = (pi/180)*15.0;\n",
    "\n",
    "##### Q-Function Hacks #####\n",
    "beta   = 0.15\n",
    "blSode = true\n",
    "blPoch = false\n",
    "\n",
    "##### Eligibility Params #####\n",
    "N_peaks =  40\n",
    "N_steps = 200\n",
    "lambda  =   0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e910ca2-281c-4d06-98e2-1c96fa7c1916",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d3689b-947a-400b-9031-9f1a13f4df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.2900000000000001, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.2800000000000001, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.4000000000000002, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.3100000000000001, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.10999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.10281250000000006\n",
      "\n",
      "Epoch 2, Best Score: 1.7500000000000013\n",
      "Training Iteration 4 score: 0.15, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.11999999999999998, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.060000000000000005, epsilon: 0.0078125\n",
      "Average Score: 0.07390625000000002\n",
      "\n",
      "Epoch 3, Best Score: 1.7500000000000013\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.11999999999999998, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.2700000000000001, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.11999999999999998, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.16, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.11999999999999998, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.19000000000000003, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.21000000000000005, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.10281250000000006\n",
      "\n",
      "Epoch 4, Best Score: 1.7500000000000013\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.23000000000000007, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.11999999999999998, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.7300000000000004, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.6000000000000003, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.36000000000000015, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.08, epsilon: 0.0078125\n",
      "Average Score: 0.18500000000000003\n",
      "\n",
      "Epoch 5, Best Score: 1.7500000000000013\n",
      "Training Iteration 4 score: 0.19000000000000003, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.5100000000000002, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.34000000000000014, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.47000000000000025, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.7500000000000004, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 1.5600000000000012, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 2.669999999999987, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.07, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.5900000000000003, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.21000000000000005, epsilon: 0.0078125\n",
      "Average Score: 0.32437499999999997\n",
      "\n",
      "Epoch 6, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.45000000000000023, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.22000000000000006, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 1.7300000000000013, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.1285937500000001\n",
      "\n",
      "Epoch 7, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.4200000000000002, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.3100000000000001, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.45000000000000023, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.36000000000000015, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.08, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.13999999999999999, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.12218750000000005\n",
      "\n",
      "Epoch 8, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.16, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.08, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.08, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.08, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.13999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.2700000000000001, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.2900000000000001, epsilon: 0.0078125\n",
      "Average Score: 0.12093750000000007\n",
      "\n",
      "Epoch 9, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.12999999999999998, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.08, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.08, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.07, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.16, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.16, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.21000000000000005, epsilon: 0.0078125\n",
      "Average Score: 0.09562500000000006\n",
      "\n",
      "Epoch 10, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.09999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.34000000000000014, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.17, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.12999999999999998, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.11999999999999998, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.25000000000000006, epsilon: 0.0078125\n",
      "Average Score: 0.13609375\n",
      "\n",
      "Epoch 11, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.26000000000000006, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.08, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.25000000000000006, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.07, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.46000000000000024, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.10999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.07, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.1618750000000001\n",
      "\n",
      "Epoch 12, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.18000000000000002, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.16, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.13999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.19000000000000003, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.08, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.16, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.25000000000000006, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.24000000000000007, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.101875\n",
      "\n",
      "Epoch 13, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.35000000000000014, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.3000000000000001, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.16, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.12999999999999998, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.3300000000000001, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.11218750000000002\n",
      "\n",
      "Epoch 14, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.26000000000000006, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09999999999999999, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.22000000000000006, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.24000000000000007, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.18000000000000002, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.08, epsilon: 0.0078125\n",
      "Average Score: 0.13499999999999998\n",
      "\n",
      "Epoch 15, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.36000000000000015, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.3000000000000001, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.18000000000000002, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 1.380000000000001, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.16, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.08, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.8200000000000005, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.11999999999999998, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.18000000000000002, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.2800000000000001, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 1.0500000000000007, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.3100000000000001, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.8800000000000006, epsilon: 0.0078125\n",
      "Average Score: 0.21765625000000008\n",
      "\n",
      "Epoch 16, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.08, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.13999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09999999999999999, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.2800000000000001, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.8300000000000005, epsilon: 0.0078125\n",
      "Average Score: 0.09109375\n",
      "\n",
      "Epoch 17, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.13999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.20000000000000004, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.18000000000000002, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.10999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.10999999999999999, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.11999999999999998, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.20000000000000004, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09, epsilon: 0.0078125\n",
      "Average Score: 0.09921875000000002\n",
      "\n",
      "Epoch 18, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.16, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.7200000000000004, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.13999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.18000000000000002, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.22000000000000006, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.3000000000000001, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.12999999999999998, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.17, epsilon: 0.0078125\n",
      "Average Score: 0.11093750000000001\n",
      "\n",
      "Epoch 19, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.11999999999999998, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.11999999999999998, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.15, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.10999999999999999, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.15, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.18000000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.11999999999999998, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 1.0500000000000007, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.16, epsilon: 0.0078125\n",
      "Average Score: 0.14937500000000006\n",
      "\n",
      "Epoch 20, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.17, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.23000000000000007, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.37000000000000016, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.16, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.22000000000000006, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.10999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.17, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.18000000000000002, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.3200000000000001, epsilon: 0.0078125\n",
      "Average Score: 0.11875000000000002\n",
      "\n",
      "Epoch 21, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.13999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.26000000000000006, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.08, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.07, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.25000000000000006, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.36000000000000015, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.4000000000000002, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.12765625000000005\n",
      "\n",
      "Epoch 22, Best Score: 2.669999999999987\n",
      "Training Iteration 4 score: 0.4000000000000002, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.12999999999999998, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 2.7899999999999845, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.08, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.09999999999999999, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.34000000000000014, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.2700000000000001, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09999999999999999, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.13999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.3000000000000001, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.21000000000000005, epsilon: 0.0078125\n",
      "Average Score: 0.24390624999999985\n",
      "\n",
      "Epoch 23, Best Score: 2.7899999999999845\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.3900000000000002, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.2800000000000001, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.7200000000000004, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.21000000000000005, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.47000000000000025, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09, epsilon: 0.0078125\n",
      "Average Score: 0.1728125000000001\n",
      "\n",
      "Epoch 24, Best Score: 2.7899999999999845\n",
      "Training Iteration 4 score: 0.18000000000000002, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.19000000000000003, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.2900000000000001, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.4300000000000002, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.10999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.08, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.5900000000000003, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.2800000000000001, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.07, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 13.039999999999766, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.11999999999999998, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.24000000000000007, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.47000000000000025, epsilon: 0.0078125\n",
      "Average Score: 0.4531249999999962\n",
      "\n",
      "Epoch 25, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.18000000000000002, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.13999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.08, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.10999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.7400000000000004, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.2900000000000001, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.20000000000000004, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.11999999999999998, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.08, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.18000000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.2900000000000001, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.48000000000000026, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.08, epsilon: 0.0078125\n",
      "Average Score: 0.20765625000000001\n",
      "\n",
      "Epoch 26, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.8100000000000005, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.25000000000000006, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.6000000000000003, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.21000000000000005, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.08, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.2900000000000001, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.5600000000000003, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.19000000000000003, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.14734375000000008\n",
      "\n",
      "Epoch 27, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.4400000000000002, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.38000000000000017, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.8000000000000005, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.20000000000000004, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.24000000000000007, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.19000000000000003, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.6300000000000003, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.4000000000000002, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.18000000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.5500000000000003, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.8700000000000006, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.35000000000000014, epsilon: 0.0078125\n",
      "Average Score: 0.19187500000000018\n",
      "\n",
      "Epoch 28, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.26000000000000006, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.18000000000000002, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.4200000000000002, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.3000000000000001, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 1.6700000000000013, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.2900000000000001, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.10999999999999999, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.8800000000000006, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.13999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.2799999999999996\n",
      "\n",
      "Epoch 29, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.7200000000000004, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.2700000000000001, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.17, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.18000000000000002, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.17, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.22000000000000006, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.3900000000000002, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.4200000000000002, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 6.259999999999911, epsilon: 0.0078125\n",
      "Average Score: 0.28249999999999864\n",
      "\n",
      "Epoch 30, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.4000000000000002, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.08, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.3300000000000001, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.37000000000000016, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.10999999999999999, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.6100000000000003, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.4100000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.3000000000000001, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 1.6400000000000012, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.10999999999999999, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.41203124999999735\n",
      "\n",
      "Epoch 31, Best Score: 13.039999999999766\n",
      "Training Iteration 4 score: 0.2800000000000001, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 1.350000000000001, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.11999999999999998, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.09, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.7200000000000004, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.19000000000000003, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.3300000000000001, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.45000000000000023, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 21.010000000000485, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.20000000000000004, epsilon: 0.0078125\n",
      "Average Score: 0.5853125000000077\n",
      "\n",
      "Epoch 32, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.17, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.35000000000000014, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.9100000000000006, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.17, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.09, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.3100000000000001, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.6900000000000004, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.21000000000000005, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.12999999999999998, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.16, epsilon: 0.0078125\n",
      "Average Score: 0.19296875000000008\n",
      "\n",
      "Epoch 33, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.09999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.19000000000000003, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.10999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.13999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.19000000000000003, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.1603125000000001\n",
      "\n",
      "Epoch 34, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.13999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.16, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.21000000000000005, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.26000000000000006, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.12999999999999998, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.23000000000000007, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.17, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.10234375000000001\n",
      "\n",
      "Epoch 35, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.21000000000000005, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.16, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.12999999999999998, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.26000000000000006, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.24000000000000007, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.11999999999999998, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.15, epsilon: 0.0078125\n",
      "Average Score: 0.18359374999999972\n",
      "\n",
      "Epoch 36, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.23000000000000007, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.21000000000000005, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.19000000000000003, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.3000000000000001, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.17, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.17, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09999999999999999, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.12999999999999998, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.12140625000000001\n",
      "\n",
      "Epoch 37, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.3000000000000001, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.9300000000000006, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.23000000000000007, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.24000000000000007, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.25000000000000006, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09999999999999999, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.10999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.17, epsilon: 0.0078125\n",
      "Average Score: 0.14781250000000004\n",
      "\n",
      "Epoch 38, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.12999999999999998, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.21000000000000005, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.19000000000000003, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.10999999999999999, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.13999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.07, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.07, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.12999999999999998, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.5800000000000003, epsilon: 0.0078125\n",
      "Average Score: 0.12953125000000001\n",
      "\n",
      "Epoch 39, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 1.1700000000000008, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.21000000000000005, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.07, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.26000000000000006, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.12999999999999998, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.12999999999999998, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.3200000000000001, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.14906250000000007\n",
      "\n",
      "Epoch 40, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.15, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.16, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.10999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.12999999999999998, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.13999999999999999, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09999999999999999, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.18000000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.18000000000000002, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.17296874999999998\n",
      "\n",
      "Epoch 41, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.19000000000000003, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.08, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.07, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09999999999999999, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.25000000000000006, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.07, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.09999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.12999999999999998, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.09, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.4000000000000002, epsilon: 0.0078125\n",
      "Average Score: 0.12640625000000003\n",
      "\n",
      "Epoch 42, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.11999999999999998, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.18000000000000002, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.10999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.17, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.10999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.10578125000000005\n",
      "\n",
      "Epoch 43, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.12999999999999998, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.25000000000000006, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.13999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.22000000000000006, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.16, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.18203125000000003\n",
      "\n",
      "Epoch 44, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.09, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.13999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.08, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.17, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.17, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.16, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.18000000000000002, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.38000000000000017, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.20000000000000004, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.10999999999999999, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.16062500000000005\n",
      "\n",
      "Epoch 45, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.07, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.20000000000000004, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.07, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.13999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.47000000000000025, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.18000000000000002, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.08, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.4400000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.07, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.11999999999999998, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.15031250000000002\n",
      "\n",
      "Epoch 46, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.3100000000000001, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.5000000000000002, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.6000000000000003, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 1.270000000000001, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.04, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.08, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.08, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.07, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.07, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.6400000000000003, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.07, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.08, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.07, epsilon: 0.0078125\n",
      "Average Score: 0.19203125000000004\n",
      "\n",
      "Epoch 47, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.09, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.10999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.17, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.21000000000000005, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.34000000000000014, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.10999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.21000000000000005, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.21000000000000005, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.3900000000000002, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.22000000000000006, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.11937499999999998\n",
      "\n",
      "Epoch 48, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.15, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.16, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.16, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.20000000000000004, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09999999999999999, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.10999999999999999, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.07, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.9900000000000007, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.08, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.1437500000000001\n",
      "\n",
      "Epoch 49, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.11999999999999998, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.17, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.15, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.38000000000000017, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.07, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.10999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.15, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09, epsilon: 0.0078125\n",
      "Average Score: 0.13812500000000005\n",
      "\n",
      "Epoch 50, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.09, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.10999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.08, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.23000000000000007, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.23000000000000007, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.12999999999999998, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.07, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.15, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.24000000000000007, epsilon: 0.0078125\n",
      "Average Score: 0.14078125000000008\n",
      "\n",
      "Epoch 51, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.16, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.11999999999999998, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.45000000000000023, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.16, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.16, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.16, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.17, epsilon: 0.0078125\n",
      "Average Score: 0.13640625\n",
      "\n",
      "Epoch 52, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.3000000000000001, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.16, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.3200000000000001, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.08, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.08, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.18000000000000002, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.16, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.08, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.12999999999999998, epsilon: 0.0078125\n",
      "Average Score: 0.13812500000000005\n",
      "\n",
      "Epoch 53, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.19000000000000003, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.4100000000000002, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.08, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.10999999999999999, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.15, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.13999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.10999999999999999, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.07, epsilon: 0.0078125\n",
      "Average Score: 0.12234375000000004\n",
      "\n",
      "Epoch 54, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.16, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.07, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 1.1700000000000008, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.17, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.12999999999999998, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.2700000000000001, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.4300000000000002, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.11125000000000004\n",
      "\n",
      "Epoch 55, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.17, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.08, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.10999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.16, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.7800000000000005, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.10999999999999999, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.13999999999999999, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.08, epsilon: 0.0078125\n",
      "Average Score: 0.15281250000000007\n",
      "\n",
      "Epoch 56, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.12999999999999998, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.10999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.17, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.18000000000000002, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.08, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.16, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.07, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.10999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.14765625000000004\n",
      "\n",
      "Epoch 57, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.09999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.6500000000000004, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.19000000000000003, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.16, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.08, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.25000000000000006, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.26000000000000006, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.18031250000000004\n",
      "\n",
      "Epoch 58, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 1.7300000000000013, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.2900000000000001, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.17, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.5300000000000002, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.09, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.08, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.19328125000000004\n",
      "\n",
      "Epoch 59, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.16, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.10999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.4200000000000002, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.19000000000000003, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.11999999999999998, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.36000000000000015, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.17, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.07, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.11999999999999998, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.09999999999999999, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.07, epsilon: 0.0078125\n",
      "Average Score: 0.14078125000000005\n",
      "\n",
      "Epoch 60, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.13999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.09999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.11999999999999998, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.18000000000000002, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.09999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.6800000000000004, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.08, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.07, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.07, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.07, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.08, epsilon: 0.0078125\n",
      "Average Score: 0.10218750000000003\n",
      "\n",
      "Epoch 61, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.21000000000000005, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.21000000000000005, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.09999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.12999999999999998, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.10999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.11999999999999998, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.07, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 1.490000000000001, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.0078125\n",
      "Average Score: 0.13921875000000003\n",
      "\n",
      "Epoch 62, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.09999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.09, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.16, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.11999999999999998, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.16, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.18000000000000002, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.07, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.12999999999999998, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.9000000000000006, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09, epsilon: 0.0078125\n",
      "Average Score: 0.10890624999999998\n",
      "\n",
      "Epoch 63, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.09999999999999999, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.10999999999999999, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.10999999999999999, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.10999999999999999, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.15, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.10999999999999999, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.12999999999999998, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.09, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.09, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.11999999999999998, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.17, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.09, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.09999999999999999, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.09999999999999999, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.09999999999999999, epsilon: 0.0078125\n",
      "Average Score: 0.13296875\n",
      "\n",
      "Epoch 64, Best Score: 21.010000000000485\n",
      "Training Iteration 4 score: 0.10999999999999999, epsilon: 0.4765625\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4453125\n",
      "Training Iteration 12 score: 0.060000000000000005, epsilon: 0.4140625\n",
      "Training Iteration 16 score: 0.21000000000000005, epsilon: 0.3828125\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3515625\n",
      "Training Iteration 24 score: 0.19000000000000003, epsilon: 0.3203125\n",
      "Training Iteration 28 score: 0.12999999999999998, epsilon: 0.2890625\n",
      "Training Iteration 32 score: 0.23000000000000007, epsilon: 0.2578125\n",
      "Training Iteration 36 score: 0.15, epsilon: 0.2265625\n",
      "Training Iteration 40 score: 0.07, epsilon: 0.1953125\n",
      "Training Iteration 44 score: 0.5700000000000003, epsilon: 0.1640625\n",
      "Training Iteration 48 score: 0.4200000000000002, epsilon: 0.1328125\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.1015625\n",
      "Training Iteration 56 score: 0.21000000000000005, epsilon: 0.0703125\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.0390625\n",
      "Training Iteration 64 score: 0.6200000000000003, epsilon: 0.0078125\n",
      "Average Score: 0.24437499999999984\n",
      "Saved a trained Q-table with size (76032,), After 23.584738620122273 minutes of training!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip490\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip490)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip491\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip490)\" d=\"\n",
       "M156.598 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.598 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip492\">\n",
       "    <rect x=\"156\" y=\"47\" width=\"2197\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  185.867,1486.45 185.867,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  514.732,1486.45 514.732,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  843.596,1486.45 843.596,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1172.46,1486.45 1172.46,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1501.33,1486.45 1501.33,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1830.19,1486.45 1830.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2159.05,1486.45 2159.05,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  185.867,1486.45 185.867,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  514.732,1486.45 514.732,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  843.596,1486.45 843.596,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1172.46,1486.45 1172.46,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1501.33,1486.45 1501.33,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1830.19,1486.45 1830.19,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2159.05,1486.45 2159.05,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M185.867 1517.37 Q182.256 1517.37 180.427 1520.93 Q178.622 1524.47 178.622 1531.6 Q178.622 1538.71 180.427 1542.27 Q182.256 1545.82 185.867 1545.82 Q189.501 1545.82 191.307 1542.27 Q193.136 1538.71 193.136 1531.6 Q193.136 1524.47 191.307 1520.93 Q189.501 1517.37 185.867 1517.37 M185.867 1513.66 Q191.677 1513.66 194.733 1518.27 Q197.812 1522.85 197.812 1531.6 Q197.812 1540.33 194.733 1544.94 Q191.677 1549.52 185.867 1549.52 Q180.057 1549.52 176.978 1544.94 Q173.923 1540.33 173.923 1531.6 Q173.923 1522.85 176.978 1518.27 Q180.057 1513.66 185.867 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M489.419 1544.91 L497.058 1544.91 L497.058 1518.55 L488.748 1520.21 L488.748 1515.95 L497.012 1514.29 L501.688 1514.29 L501.688 1544.91 L509.327 1544.91 L509.327 1548.85 L489.419 1548.85 L489.419 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M528.771 1517.37 Q525.16 1517.37 523.331 1520.93 Q521.526 1524.47 521.526 1531.6 Q521.526 1538.71 523.331 1542.27 Q525.16 1545.82 528.771 1545.82 Q532.405 1545.82 534.211 1542.27 Q536.04 1538.71 536.04 1531.6 Q536.04 1524.47 534.211 1520.93 Q532.405 1517.37 528.771 1517.37 M528.771 1513.66 Q534.581 1513.66 537.637 1518.27 Q540.715 1522.85 540.715 1531.6 Q540.715 1540.33 537.637 1544.94 Q534.581 1549.52 528.771 1549.52 Q522.961 1549.52 519.882 1544.94 Q516.827 1540.33 516.827 1531.6 Q516.827 1522.85 519.882 1518.27 Q522.961 1513.66 528.771 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M822.37 1544.91 L838.689 1544.91 L838.689 1548.85 L816.745 1548.85 L816.745 1544.91 Q819.407 1542.16 823.99 1537.53 Q828.596 1532.88 829.777 1531.53 Q832.022 1529.01 832.902 1527.27 Q833.805 1525.51 833.805 1523.82 Q833.805 1521.07 831.86 1519.33 Q829.939 1517.6 826.837 1517.6 Q824.638 1517.6 822.184 1518.36 Q819.754 1519.13 816.976 1520.68 L816.976 1515.95 Q819.8 1514.82 822.254 1514.24 Q824.708 1513.66 826.745 1513.66 Q832.115 1513.66 835.309 1516.35 Q838.504 1519.03 838.504 1523.52 Q838.504 1525.65 837.694 1527.57 Q836.907 1529.47 834.8 1532.07 Q834.221 1532.74 831.12 1535.95 Q828.018 1539.15 822.37 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M858.504 1517.37 Q854.893 1517.37 853.064 1520.93 Q851.258 1524.47 851.258 1531.6 Q851.258 1538.71 853.064 1542.27 Q854.893 1545.82 858.504 1545.82 Q862.138 1545.82 863.943 1542.27 Q865.772 1538.71 865.772 1531.6 Q865.772 1524.47 863.943 1520.93 Q862.138 1517.37 858.504 1517.37 M858.504 1513.66 Q864.314 1513.66 867.369 1518.27 Q870.448 1522.85 870.448 1531.6 Q870.448 1540.33 867.369 1544.94 Q864.314 1549.52 858.504 1549.52 Q852.694 1549.52 849.615 1544.94 Q846.559 1540.33 846.559 1531.6 Q846.559 1522.85 849.615 1518.27 Q852.694 1513.66 858.504 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1161.3 1530.21 Q1164.66 1530.93 1166.54 1533.2 Q1168.43 1535.47 1168.43 1538.8 Q1168.43 1543.92 1164.91 1546.72 Q1161.4 1549.52 1154.91 1549.52 Q1152.74 1549.52 1150.42 1549.08 Q1148.13 1548.66 1145.68 1547.81 L1145.68 1543.29 Q1147.62 1544.43 1149.94 1545.01 Q1152.25 1545.58 1154.78 1545.58 Q1159.17 1545.58 1161.47 1543.85 Q1163.78 1542.11 1163.78 1538.8 Q1163.78 1535.75 1161.63 1534.03 Q1159.5 1532.3 1155.68 1532.3 L1151.65 1532.3 L1151.65 1528.45 L1155.86 1528.45 Q1159.31 1528.45 1161.14 1527.09 Q1162.97 1525.7 1162.97 1523.11 Q1162.97 1520.45 1161.07 1519.03 Q1159.2 1517.6 1155.68 1517.6 Q1153.76 1517.6 1151.56 1518.01 Q1149.36 1518.43 1146.72 1519.31 L1146.72 1515.14 Q1149.38 1514.4 1151.7 1514.03 Q1154.04 1513.66 1156.1 1513.66 Q1161.42 1513.66 1164.52 1516.09 Q1167.62 1518.5 1167.62 1522.62 Q1167.62 1525.49 1165.98 1527.48 Q1164.34 1529.45 1161.3 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1187.3 1517.37 Q1183.69 1517.37 1181.86 1520.93 Q1180.05 1524.47 1180.05 1531.6 Q1180.05 1538.71 1181.86 1542.27 Q1183.69 1545.82 1187.3 1545.82 Q1190.93 1545.82 1192.74 1542.27 Q1194.57 1538.71 1194.57 1531.6 Q1194.57 1524.47 1192.74 1520.93 Q1190.93 1517.37 1187.3 1517.37 M1187.3 1513.66 Q1193.11 1513.66 1196.16 1518.27 Q1199.24 1522.85 1199.24 1531.6 Q1199.24 1540.33 1196.16 1544.94 Q1193.11 1549.52 1187.3 1549.52 Q1181.49 1549.52 1178.41 1544.94 Q1175.35 1540.33 1175.35 1531.6 Q1175.35 1522.85 1178.41 1518.27 Q1181.49 1513.66 1187.3 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1489.5 1518.36 L1477.69 1536.81 L1489.5 1536.81 L1489.5 1518.36 M1488.27 1514.29 L1494.15 1514.29 L1494.15 1536.81 L1499.08 1536.81 L1499.08 1540.7 L1494.15 1540.7 L1494.15 1548.85 L1489.5 1548.85 L1489.5 1540.7 L1473.9 1540.7 L1473.9 1536.19 L1488.27 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1516.81 1517.37 Q1513.2 1517.37 1511.37 1520.93 Q1509.57 1524.47 1509.57 1531.6 Q1509.57 1538.71 1511.37 1542.27 Q1513.2 1545.82 1516.81 1545.82 Q1520.45 1545.82 1522.25 1542.27 Q1524.08 1538.71 1524.08 1531.6 Q1524.08 1524.47 1522.25 1520.93 Q1520.45 1517.37 1516.81 1517.37 M1516.81 1513.66 Q1522.62 1513.66 1525.68 1518.27 Q1528.76 1522.85 1528.76 1531.6 Q1528.76 1540.33 1525.68 1544.94 Q1522.62 1549.52 1516.81 1549.52 Q1511 1549.52 1507.92 1544.94 Q1504.87 1540.33 1504.87 1531.6 Q1504.87 1522.85 1507.92 1518.27 Q1511 1513.66 1516.81 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1804.89 1514.29 L1823.25 1514.29 L1823.25 1518.22 L1809.17 1518.22 L1809.17 1526.7 Q1810.19 1526.35 1811.21 1526.19 Q1812.23 1526 1813.25 1526 Q1819.03 1526 1822.41 1529.17 Q1825.79 1532.34 1825.79 1537.76 Q1825.79 1543.34 1822.32 1546.44 Q1818.85 1549.52 1812.53 1549.52 Q1810.35 1549.52 1808.08 1549.15 Q1805.84 1548.78 1803.43 1548.04 L1803.43 1543.34 Q1805.51 1544.47 1807.74 1545.03 Q1809.96 1545.58 1812.44 1545.58 Q1816.44 1545.58 1818.78 1543.48 Q1821.12 1541.37 1821.12 1537.76 Q1821.12 1534.15 1818.78 1532.04 Q1816.44 1529.94 1812.44 1529.94 Q1810.56 1529.94 1808.69 1530.35 Q1806.83 1530.77 1804.89 1531.65 L1804.89 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M1845 1517.37 Q1841.39 1517.37 1839.57 1520.93 Q1837.76 1524.47 1837.76 1531.6 Q1837.76 1538.71 1839.57 1542.27 Q1841.39 1545.82 1845 1545.82 Q1848.64 1545.82 1850.44 1542.27 Q1852.27 1538.71 1852.27 1531.6 Q1852.27 1524.47 1850.44 1520.93 Q1848.64 1517.37 1845 1517.37 M1845 1513.66 Q1850.81 1513.66 1853.87 1518.27 Q1856.95 1522.85 1856.95 1531.6 Q1856.95 1540.33 1853.87 1544.94 Q1850.81 1549.52 1845 1549.52 Q1839.19 1549.52 1836.12 1544.94 Q1833.06 1540.33 1833.06 1531.6 Q1833.06 1522.85 1836.12 1518.27 Q1839.19 1513.66 1845 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2144.46 1529.7 Q2141.31 1529.7 2139.46 1531.86 Q2137.63 1534.01 2137.63 1537.76 Q2137.63 1541.49 2139.46 1543.66 Q2141.31 1545.82 2144.46 1545.82 Q2147.61 1545.82 2149.44 1543.66 Q2151.29 1541.49 2151.29 1537.76 Q2151.29 1534.01 2149.44 1531.86 Q2147.61 1529.7 2144.46 1529.7 M2153.74 1515.05 L2153.74 1519.31 Q2151.98 1518.48 2150.18 1518.04 Q2148.4 1517.6 2146.64 1517.6 Q2142.01 1517.6 2139.55 1520.72 Q2137.12 1523.85 2136.77 1530.17 Q2138.14 1528.15 2140.2 1527.09 Q2142.26 1526 2144.74 1526 Q2149.95 1526 2152.96 1529.17 Q2155.99 1532.32 2155.99 1537.76 Q2155.99 1543.08 2152.84 1546.3 Q2149.69 1549.52 2144.46 1549.52 Q2138.46 1549.52 2135.29 1544.94 Q2132.12 1540.33 2132.12 1531.6 Q2132.12 1523.41 2136.01 1518.55 Q2139.9 1513.66 2146.45 1513.66 Q2148.21 1513.66 2149.99 1514.01 Q2151.8 1514.36 2153.74 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2174.04 1517.37 Q2170.43 1517.37 2168.6 1520.93 Q2166.8 1524.47 2166.8 1531.6 Q2166.8 1538.71 2168.6 1542.27 Q2170.43 1545.82 2174.04 1545.82 Q2177.68 1545.82 2179.48 1542.27 Q2181.31 1538.71 2181.31 1531.6 Q2181.31 1524.47 2179.48 1520.93 Q2177.68 1517.37 2174.04 1517.37 M2174.04 1513.66 Q2179.85 1513.66 2182.91 1518.27 Q2185.99 1522.85 2185.99 1531.6 Q2185.99 1540.33 2182.91 1544.94 Q2179.85 1549.52 2174.04 1549.52 Q2168.23 1549.52 2165.15 1544.94 Q2162.1 1540.33 2162.1 1531.6 Q2162.1 1522.85 2165.15 1518.27 Q2168.23 1513.66 2174.04 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,1376.44 2352.76,1376.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,1110.95 2352.76,1110.95 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,845.456 2352.76,845.456 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,579.965 2352.76,579.965 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,314.474 2352.76,314.474 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip492)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,48.9822 2352.76,48.9822 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1486.45 156.598,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1376.44 175.496,1376.44 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1110.95 175.496,1110.95 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,845.456 175.496,845.456 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,579.965 175.496,579.965 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,314.474 175.496,314.474 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,48.9822 175.496,48.9822 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M64.6495 1362.24 Q61.0384 1362.24 59.2097 1365.8 Q57.4041 1369.34 57.4041 1376.47 Q57.4041 1383.58 59.2097 1387.15 Q61.0384 1390.69 64.6495 1390.69 Q68.2837 1390.69 70.0892 1387.15 Q71.9179 1383.58 71.9179 1376.47 Q71.9179 1369.34 70.0892 1365.8 Q68.2837 1362.24 64.6495 1362.24 M64.6495 1358.53 Q70.4596 1358.53 73.5152 1363.14 Q76.5938 1367.72 76.5938 1376.47 Q76.5938 1385.2 73.5152 1389.81 Q70.4596 1394.39 64.6495 1394.39 Q58.8393 1394.39 55.7606 1389.81 Q52.7051 1385.2 52.7051 1376.47 Q52.7051 1367.72 55.7606 1363.14 Q58.8393 1358.53 64.6495 1358.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M84.8114 1387.84 L89.6956 1387.84 L89.6956 1393.72 L84.8114 1393.72 L84.8114 1387.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M100.691 1389.78 L108.33 1389.78 L108.33 1363.42 L100.02 1365.08 L100.02 1360.83 L108.283 1359.16 L112.959 1359.16 L112.959 1389.78 L120.598 1389.78 L120.598 1393.72 L100.691 1393.72 L100.691 1389.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M65.0198 1096.75 Q61.4087 1096.75 59.58 1100.31 Q57.7745 1103.85 57.7745 1110.98 Q57.7745 1118.09 59.58 1121.65 Q61.4087 1125.2 65.0198 1125.2 Q68.6541 1125.2 70.4596 1121.65 Q72.2883 1118.09 72.2883 1110.98 Q72.2883 1103.85 70.4596 1100.31 Q68.6541 1096.75 65.0198 1096.75 M65.0198 1093.04 Q70.83 1093.04 73.8855 1097.65 Q76.9642 1102.23 76.9642 1110.98 Q76.9642 1119.71 73.8855 1124.32 Q70.83 1128.9 65.0198 1128.9 Q59.2097 1128.9 56.131 1124.32 Q53.0754 1119.71 53.0754 1110.98 Q53.0754 1102.23 56.131 1097.65 Q59.2097 1093.04 65.0198 1093.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M85.1818 1122.35 L90.066 1122.35 L90.066 1128.23 L85.1818 1128.23 L85.1818 1122.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M104.279 1124.29 L120.598 1124.29 L120.598 1128.23 L98.6539 1128.23 L98.6539 1124.29 Q101.316 1121.54 105.899 1116.91 Q110.506 1112.26 111.686 1110.91 Q113.932 1108.39 114.811 1106.65 Q115.714 1104.89 115.714 1103.2 Q115.714 1100.45 113.77 1098.71 Q111.848 1096.98 108.746 1096.98 Q106.547 1096.98 104.094 1097.74 Q101.663 1098.51 98.8854 1100.06 L98.8854 1095.33 Q101.709 1094.2 104.163 1093.62 Q106.617 1093.04 108.654 1093.04 Q114.024 1093.04 117.219 1095.73 Q120.413 1098.41 120.413 1102.9 Q120.413 1105.03 119.603 1106.95 Q118.816 1108.85 116.709 1111.45 Q116.131 1112.12 113.029 1115.33 Q109.927 1118.53 104.279 1124.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M64.0708 831.255 Q60.4597 831.255 58.631 834.82 Q56.8254 838.361 56.8254 845.491 Q56.8254 852.597 58.631 856.162 Q60.4597 859.704 64.0708 859.704 Q67.705 859.704 69.5105 856.162 Q71.3392 852.597 71.3392 845.491 Q71.3392 838.361 69.5105 834.82 Q67.705 831.255 64.0708 831.255 M64.0708 827.551 Q69.8809 827.551 72.9365 832.158 Q76.0151 836.741 76.0151 845.491 Q76.0151 854.218 72.9365 858.824 Q69.8809 863.408 64.0708 863.408 Q58.2606 863.408 55.1819 858.824 Q52.1264 854.218 52.1264 845.491 Q52.1264 836.741 55.1819 832.158 Q58.2606 827.551 64.0708 827.551 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M84.2327 856.857 L89.1169 856.857 L89.1169 862.736 L84.2327 862.736 L84.2327 856.857 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M113.469 844.102 Q116.825 844.82 118.7 847.088 Q120.598 849.357 120.598 852.69 Q120.598 857.806 117.08 860.607 Q113.561 863.408 107.08 863.408 Q104.904 863.408 102.589 862.968 Q100.297 862.551 97.8437 861.695 L97.8437 857.181 Q99.7882 858.315 102.103 858.894 Q104.418 859.472 106.941 859.472 Q111.339 859.472 113.631 857.736 Q115.945 856 115.945 852.69 Q115.945 849.635 113.793 847.922 Q111.663 846.185 107.844 846.185 L103.816 846.185 L103.816 842.343 L108.029 842.343 Q111.478 842.343 113.307 840.977 Q115.135 839.588 115.135 836.996 Q115.135 834.334 113.237 832.922 Q111.362 831.486 107.844 831.486 Q105.922 831.486 103.723 831.903 Q101.524 832.32 98.8854 833.199 L98.8854 829.033 Q101.547 828.292 103.862 827.922 Q106.2 827.551 108.26 827.551 Q113.584 827.551 116.686 829.982 Q119.788 832.389 119.788 836.51 Q119.788 839.38 118.145 841.371 Q116.501 843.338 113.469 844.102 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M62.9365 565.764 Q59.3254 565.764 57.4967 569.328 Q55.6912 572.87 55.6912 580 Q55.6912 587.106 57.4967 590.671 Q59.3254 594.213 62.9365 594.213 Q66.5707 594.213 68.3763 590.671 Q70.205 587.106 70.205 580 Q70.205 572.87 68.3763 569.328 Q66.5707 565.764 62.9365 565.764 M62.9365 562.06 Q68.7467 562.06 71.8022 566.666 Q74.8809 571.25 74.8809 580 Q74.8809 588.726 71.8022 593.333 Q68.7467 597.916 62.9365 597.916 Q57.1264 597.916 54.0477 593.333 Q50.9921 588.726 50.9921 580 Q50.9921 571.25 54.0477 566.666 Q57.1264 562.06 62.9365 562.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M83.0984 591.365 L87.9827 591.365 L87.9827 597.245 L83.0984 597.245 L83.0984 591.365 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M111.015 566.759 L99.2095 585.208 L111.015 585.208 L111.015 566.759 M109.788 562.685 L115.668 562.685 L115.668 585.208 L120.598 585.208 L120.598 589.097 L115.668 589.097 L115.668 597.245 L111.015 597.245 L111.015 589.097 L95.4132 589.097 L95.4132 584.583 L109.788 562.685 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M64.418 300.272 Q60.8069 300.272 58.9782 303.837 Q57.1726 307.379 57.1726 314.508 Q57.1726 321.615 58.9782 325.18 Q60.8069 328.721 64.418 328.721 Q68.0522 328.721 69.8578 325.18 Q71.6865 321.615 71.6865 314.508 Q71.6865 307.379 69.8578 303.837 Q68.0522 300.272 64.418 300.272 M64.418 296.569 Q70.2281 296.569 73.2837 301.175 Q76.3624 305.758 76.3624 314.508 Q76.3624 323.235 73.2837 327.842 Q70.2281 332.425 64.418 332.425 Q58.6078 332.425 55.5291 327.842 Q52.4736 323.235 52.4736 314.508 Q52.4736 305.758 55.5291 301.175 Q58.6078 296.569 64.418 296.569 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M84.5799 325.874 L89.4641 325.874 L89.4641 331.754 L84.5799 331.754 L84.5799 325.874 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M99.6956 297.194 L118.052 297.194 L118.052 301.129 L103.978 301.129 L103.978 309.601 Q104.996 309.254 106.015 309.092 Q107.033 308.906 108.052 308.906 Q113.839 308.906 117.219 312.078 Q120.598 315.249 120.598 320.666 Q120.598 326.244 117.126 329.346 Q113.654 332.425 107.334 332.425 Q105.159 332.425 102.89 332.055 Q100.645 331.684 98.2372 330.943 L98.2372 326.244 Q100.321 327.379 102.543 327.934 Q104.765 328.49 107.242 328.49 Q111.246 328.49 113.584 326.383 Q115.922 324.277 115.922 320.666 Q115.922 317.055 113.584 314.948 Q111.246 312.842 107.242 312.842 Q105.367 312.842 103.492 313.258 Q101.64 313.675 99.6956 314.555 L99.6956 297.194 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M63.2606 34.7809 Q59.6495 34.7809 57.8208 38.3457 Q56.0152 41.8874 56.0152 49.017 Q56.0152 56.1234 57.8208 59.6882 Q59.6495 63.2298 63.2606 63.2298 Q66.8948 63.2298 68.7004 59.6882 Q70.5291 56.1234 70.5291 49.017 Q70.5291 41.8874 68.7004 38.3457 Q66.8948 34.7809 63.2606 34.7809 M63.2606 31.0772 Q69.0707 31.0772 72.1263 35.6837 Q75.205 40.267 75.205 49.017 Q75.205 57.7438 72.1263 62.3502 Q69.0707 66.9335 63.2606 66.9335 Q57.4504 66.9335 54.3717 62.3502 Q51.3162 57.7438 51.3162 49.017 Q51.3162 40.267 54.3717 35.6837 Q57.4504 31.0772 63.2606 31.0772 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M83.4225 60.3826 L88.3067 60.3826 L88.3067 66.2622 L83.4225 66.2622 L83.4225 60.3826 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M109.071 47.1188 Q105.922 47.1188 104.071 49.2716 Q102.242 51.4243 102.242 55.1743 Q102.242 58.9012 104.071 61.0771 Q105.922 63.2298 109.071 63.2298 Q112.219 63.2298 114.047 61.0771 Q115.899 58.9012 115.899 55.1743 Q115.899 51.4243 114.047 49.2716 Q112.219 47.1188 109.071 47.1188 M118.353 32.4661 L118.353 36.7254 Q116.594 35.892 114.788 35.4522 Q113.006 35.0124 111.246 35.0124 Q106.617 35.0124 104.163 38.1374 Q101.733 41.2624 101.385 47.5818 Q102.751 45.5679 104.811 44.5031 Q106.871 43.4151 109.348 43.4151 Q114.557 43.4151 117.566 46.5864 Q120.598 49.7345 120.598 55.1743 Q120.598 60.4984 117.45 63.716 Q114.302 66.9335 109.071 66.9335 Q103.075 66.9335 99.9039 62.3502 Q96.7326 57.7438 96.7326 49.017 Q96.7326 40.8226 100.621 35.9615 Q104.51 31.0772 111.061 31.0772 Q112.82 31.0772 114.603 31.4245 Q116.408 31.7717 118.353 32.4661 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip492)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  218.754,1368.97 251.64,1445.72 284.527,1368.97 317.413,1150.77 350.299,780.743 383.186,1300.53 416.072,1317.53 448.959,1320.85 481.845,1388.05 514.732,1280.61 \n",
       "  547.618,1212.17 580.505,1371.46 613.391,1344.08 646.278,1283.52 679.164,1064.07 712.051,1400.08 744.937,1378.51 777.823,1347.4 810.71,1245.35 843.596,1326.66 \n",
       "  876.483,1303.01 909.369,994.38 942.256,1183.13 975.142,438.923 1008.03,1090.62 1040.92,1250.75 1073.8,1132.52 1106.69,898.555 1139.57,891.917 1172.46,548.023 \n",
       "  1205.35,87.9763 1238.23,1129.62 1271.12,1216.31 1304.01,1370.22 1336.89,1154.5 1369.78,1319.61 1402.67,1249.5 1435.55,1298.04 1468.44,1246.18 1501.33,1182.71 \n",
       "  1534.21,1306.33 1567.1,1361.09 1599.98,1158.65 1632.87,1215.48 1665.76,1242.86 1698.64,1132.1 1731.53,1325 1764.42,1260.29 1797.3,1275.22 1830.19,1268.17 \n",
       "  1863.08,1279.78 1895.96,1275.22 1928.85,1317.12 1961.74,1346.57 1994.62,1236.23 2027.51,1249.92 2060.4,1163.22 2093.28,1128.79 2126.17,1268.17 2159.05,1370.63 \n",
       "  2191.94,1272.32 2224.83,1352.79 2257.71,1288.91 2290.6,993.136 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"\n",
       "M1983.1 198.898 L2279.55 198.898 L2279.55 95.2176 L1983.1 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1983.1,198.898 2279.55,198.898 2279.55,95.2176 1983.1,95.2176 1983.1,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip490)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.5,147.058 2153.92,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip490)\" d=\"M2192.16 166.745 Q2190.35 171.375 2188.64 172.787 Q2186.93 174.199 2184.06 174.199 L2180.65 174.199 L2180.65 170.634 L2183.15 170.634 Q2184.91 170.634 2185.89 169.8 Q2186.86 168.967 2188.04 165.865 L2188.8 163.921 L2178.32 138.412 L2182.83 138.412 L2190.93 158.689 L2199.03 138.412 L2203.55 138.412 L2192.16 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip490)\" d=\"M2210.84 160.402 L2218.48 160.402 L2218.48 134.037 L2210.17 135.703 L2210.17 131.444 L2218.43 129.778 L2223.11 129.778 L2223.11 160.402 L2230.75 160.402 L2230.75 164.338 L2210.84 164.338 L2210.84 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgn       = time()\n",
    "averages  = []\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    if blSode\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore )\n",
    "    end\n",
    "    \n",
    "    if blPoch\n",
    "        println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    end\n",
    "    \n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/episodes\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        X  = X_0\n",
    "        \n",
    "        ##### Double Q-Learning ###########################################\n",
    "\n",
    "        for k = 1:T\n",
    "\n",
    "            # 1. Choose action\n",
    "            if rand() < epsilon\n",
    "                if rand() < EXPrand \n",
    "                    A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                else\n",
    "                    A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                end\n",
    "            else\n",
    "\n",
    "                A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                if A == 1000.0 # Indicates no values in this region\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # 2. Cache last state\n",
    "            qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "            # 3. Generate the next stae\n",
    "            Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "            # 4. Collect reward R( s, a, s' )\n",
    "            R_t = cartpole_reward( Xp )\n",
    "\n",
    "            # 5. Get the optimal action at the next state\n",
    "            a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "\n",
    "            # 6. Compute the value at the next state\n",
    "\n",
    "            V_tp1_opt = query_value_fuzzy( \n",
    "                Q_kdTree, G, V, \n",
    "                get_Q( \n",
    "                    select_X_vector( Xp ), \n",
    "                    a_tp1_opt \n",
    "                ); \n",
    "                k = vNN \n",
    "            )\n",
    "            if isnan( V_tp1_opt )\n",
    "                V_tp1_opt = 0.0\n",
    "            end\n",
    "\n",
    "\n",
    "            # 7. Blend the value back into nearest points\n",
    "\n",
    "            idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "            nNear      = size( idxs, 1 )\n",
    "            for i = 1:nNear\n",
    "                j    = idxs[i]\n",
    "                if !isnan( wgts[i] ) \n",
    "\n",
    "                    VS[j] = R_t + gamma * V_tp1_opt # Q-Learning\n",
    "                    # VS[j] = VS[j] + alpha*( R_t + V_tp1_opt - V[j] ) # Q(TD)-Learning\n",
    "                    \n",
    "                end\n",
    "            end\n",
    "\n",
    "            states[:,k] = Xp\n",
    "            actions[k]  = A\n",
    "\n",
    "            X = Xp\n",
    "        end\n",
    "\n",
    "        s_l    = vertical_score_s( states, aMargin, ts )\n",
    "        s_Totl += s_l\n",
    "    \n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        ##### Eligibility Traces ##########################################\n",
    "        if false\n",
    "        \n",
    "            # 1. Find `N_peaks`\n",
    "            peakDices = find_state_history_R_peaks( states, N_peaks )\n",
    "            # 2. For each peak, iterate back in time through states\n",
    "            for ii = 1:min(N_peaks, length(peakDices))\n",
    "                topDex = peakDices[ ii ]\n",
    "                X      = states[:,topDex]\n",
    "                R_jj    = cartpole_reward( X )\n",
    "                # 3. For each Q-state in the trace\n",
    "                for jj = (topDex-1):-1:max(1,topDex-N_steps)\n",
    "                    X = states[:,jj]\n",
    "                    R_jj *= lambda\n",
    "                    a_jj = actions[jj]\n",
    "                    q_jj = get_Q( select_X_vector( X ), a_jj )\n",
    "                    V_jj = query_value_fuzzy( Q_kdTree, G, V, q_jj; k = vNN )\n",
    "\n",
    "                    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, q_jj; k = bNN )\n",
    "                    nNear      = size( idxs, 1 )\n",
    "\n",
    "                    for kk = 1:nNear\n",
    "                        ll = idxs[kk]\n",
    "                        if !isnan( wgts[kk] ) \n",
    "                            VS[ll] = VS[ll] + alpha*( R_jj + V_jj - V[ll] ) # Q(TD)-Learning\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        # Decay the exploration probability\n",
    "        epsilon -= deltaEp\n",
    "        \n",
    "        \n",
    "        ##### Double Q-Learning ##########################################\n",
    "\n",
    "        # Swap Q-functions for Double Q-Learning\n",
    "        vSwp = copy( VS   )\n",
    "        VS   = copy( V    )\n",
    "        V    = copy( vSwp )\n",
    "        \n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "     \n",
    "    \n",
    "    ##### Q-Function Hacks ################################################\n",
    "    \n",
    "    # Blend Method 1: Best Episode\n",
    "    if blSode\n",
    "        V  = blend_alpha_of_A_into_B( beta, vBst, V  )\n",
    "        VS = blend_alpha_of_A_into_B( beta, vBst, VS )\n",
    "    end\n",
    "    \n",
    "    # if (s_Avg > bestAvg) && true\n",
    "    #     println( \"BLEND\" )\n",
    "    #     bestAvg = s_Avg\n",
    "    #     vBAv    = copy( V ) # Try a blend of both next # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    #     vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V ) # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    # end\n",
    "        \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709555b9-2598-4281-a634-c7b0681277d0",
   "metadata": {},
   "source": [
    "# Method 2 Performance, Average Vertical Duration [s]\n",
    "Each score is the best run out of an entire training period: 64 epochs of 64 episodes each, Q-function swap after every episode \n",
    "\n",
    "### Blend: Best Episode\n",
    "\n",
    "$\\beta = 0.07$:  \n",
    "$\\beta = 0.15$: 0.244\n",
    "\n",
    "| Method      | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 | Mean |\n",
    "| ----------- | ------- | ------- | ------- | ------- | ------- | ---- |\n",
    "| Blend (Epi) |         |         |         |         |         |      |\n",
    "| Blend (Epo) |         |         |         |         |         |      |\n",
    "| TD          |         |         |         |         |         |      |\n",
    "| TD  + ????? |         |         |         |         |         |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c1d8a-58c5-4719-89c8-b69bf6623266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
