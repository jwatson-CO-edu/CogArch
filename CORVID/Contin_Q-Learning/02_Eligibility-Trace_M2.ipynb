{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "118cefc7-7c60-4838-9399-26a98ec9736e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43290374-89de-4616-8800-c86799248c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using NearestNeighbors\n",
    "using StaticArrays\n",
    "using Luxor\n",
    "using DataStructures\n",
    "include(\"utils.jl\"   )\n",
    "include(\"kernels.jl\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851743ab-a511-40fb-850b-bf90efa9232d",
   "metadata": {},
   "source": [
    "# Problem Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39765-4abe-409a-bea1-f44fa8ec2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "_DIM_X    = 4\n",
    "_DIM_A    = 1\n",
    "Fmax      = 10.0 #7.5 #15.0 #25.0 #5.0 #10.0 #20.0\n",
    "Fdiv      = 4.0 #8.0 # 4.0\n",
    "_X_DOMAIN = [ -30.0 +30.0 ; # thetaDotDot\n",
    "              -15.0 +15.0 ; # thetaDot\n",
    "              -20.0 +20.0 ; # theta\n",
    "              -10.0 +10.0 ] # xDot\n",
    "_A_DOMAIN = [ -Fmax +Fmax ]\n",
    "_Q_DOMAIN = [_X_DOMAIN; _A_DOMAIN]\n",
    "_LEAFLEN  = 10;\n",
    "\n",
    "nX = _DIM_X; # ---- State    dims\n",
    "nA = _DIM_A; # ---- Action   dims\n",
    "nQ = nX + nA; # --- Combined dims\n",
    "X  = zeros( nX ); # Current position\n",
    "A  = zeros( nA ); # Current effort\n",
    "Q  = zeros( nQ ); # Current Q state\n",
    "\n",
    "include(\"env_cartpole.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf920d4-46af-4f22-8933-c3db011ff716",
   "metadata": {},
   "source": [
    "# Q-Learning Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f605b904-b397-4617-9dbe-a27c0b4fb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function get_Q( X, A )\n",
    "    res = zeros( nQ );\n",
    "    res[ 1:nX ] = X[:];\n",
    "    if typeof( A ) == Float64\n",
    "        res[ nX+1 ] = A;\n",
    "    else\n",
    "        res[ nX+1:nQ ] = A;\n",
    "    end\n",
    "    return res;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Disassemble <State,Action> into Q-state\n",
    "\"\"\"\n",
    "function XA_from_Q( Q )\n",
    "    return Q[ 1:nX ], Q[ nX+1:nQ ];\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Select the relvant variables from the state vector\n",
    "\"\"\"\n",
    "function select_X_vector( Xbig )\n",
    "    return [ Xbig[1], Xbig[2], Xbig[3], Xbig[5] ]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Normalize `theta` to shortest angle to zero\n",
    "\"\"\"\n",
    "function norm_turn( theta )\n",
    "    thetaN = abs( theta % (2*pi) )\n",
    "    if thetaN > pi\n",
    "        thetaN = (2*pi) - thetaN\n",
    "    end\n",
    "    return thetaN\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Reward high speed at the bottom and low speed at the top\n",
    "\"\"\"\n",
    "function cartpole_reward( X )\n",
    "    \n",
    "    # 0. Set limits\n",
    "    maxThetaDot =  10.0\n",
    "    maxX        =   2.0\n",
    "    # 1. Set weights\n",
    "    thFactor    = 100.0\n",
    "    thDotFactor =   8.0\n",
    "    \n",
    "    # 2. Unpack & Normalize state\n",
    "    thetaDotN   = abs( X[2] ) # ----- Angular velocity\n",
    "    thetaN      = X[3] # Angle\n",
    "    xN          = abs( X[6] ) # ----- Fulcrum position\n",
    "    # 3. Reward high speed at the bottom and low speed at the top\n",
    "    R = thFactor*cos(thetaN) - thDotFactor*cos(thetaN)*(thetaDotN)\n",
    "    \n",
    "    \n",
    "    if xN > maxX\n",
    "        R -= xN\n",
    "    end\n",
    "    return R\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Return the indices and scores of all the peak rewards in the data\n",
    "\"\"\"\n",
    "function find_state_history_R_peaks( X_hist, N_pks )\n",
    "    \n",
    "    epLen   = size( X_hist, 2 )\n",
    "    rising  = false\n",
    "    lastVal = 1e9\n",
    "    lastRis = false\n",
    "    pqPeaks = PriorityQueue();\n",
    "    rtnPeak = []\n",
    "    \n",
    "    for j = 1:epLen\n",
    "        X       = X_hist[:,j]\n",
    "        currVal = cartpole_reward( X )\n",
    "        rising  = (currVal > lastVal)\n",
    "        if (!rising) && lastRis\n",
    "            pqPeaks[j] = -currVal # Store the current index at its current (negative) value\n",
    "        end\n",
    "        lastVal = currVal\n",
    "        lastRis = rising\n",
    "    end\n",
    "    for i = 1:min( N_pks, length( pqPeaks ) )\n",
    "        append!( rtnPeak, dequeue!( pqPeaks ) )\n",
    "    end\n",
    "    \n",
    "    return rtnPeak;\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function optimal_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   = 0.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = cartpole_reward( Xp )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if (Ra != 0.0) && (Ra > bestR)\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Given a state `X`, determine the best action\n",
    "\"\"\"\n",
    "function learned_action_for_state_exp( X, domain, res, ts )\n",
    "    testPts = regular_grid_pts_nD( domain, res )[:]\n",
    "    N       = length( testPts )\n",
    "    bestR   = -1000.0\n",
    "    bestA   =  1000.0\n",
    "    # println( testPts )\n",
    "    for j = 1:N\n",
    "        A  = testPts[j]\n",
    "        Xp = cartpole_dyn( X, A, ts )\n",
    "        Ra = query_value_fuzzy_exp( \n",
    "            Q_kdTree, G, V, \n",
    "            get_Q( \n",
    "                select_X_vector( Xp ), \n",
    "                A \n",
    "            ); \n",
    "            k = vNN \n",
    "        )\n",
    "        if Ra > bestR\n",
    "            bestR = Ra\n",
    "            bestA = A\n",
    "        end\n",
    "    end\n",
    "    # println( bestR )\n",
    "    return bestA\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return number of seconds that penulum was within double-sided `angleMargin` of vertical\n",
    "\"\"\"\n",
    "function vertical_score_s( stateHistory, angleMargin, ts )\n",
    "    angles = stateHistory[3,:]\n",
    "    N      = length( angles )\n",
    "    score  = 0.0\n",
    "    # println( \"vertical_score_s: Analize series of \", N, \" timesteps.\" )\n",
    "    for j = 1:N\n",
    "        if abs( angles[j] ) <= angleMargin\n",
    "            score += ts\n",
    "        end\n",
    "    end\n",
    "    return score\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d663e-1ccd-441f-807f-44f84a43e4d0",
   "metadata": {},
   "source": [
    "# Q-Function Hacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf91f06c-df14-4fe7-b81d-12c3184b807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Blend two vectors by element\n",
    "\"\"\"\n",
    "function blend_alpha_of_A_into_B( alpha, A, B )\n",
    "    return A*alpha + B*(1.0 - alpha)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Exchange nonzero values\n",
    "\"\"\"\n",
    "function exchange_nonzeros( A, B )\n",
    "    rtnA = zeros( size(A, 1) )    \n",
    "    rtnB = zeros( size(B, 1) )\n",
    "    N    = size(A, 1)\n",
    "    for j = 1:N\n",
    "        \n",
    "        # Handle A\n",
    "        if A[j] == 0.0\n",
    "            rtnA[j] = B[j]\n",
    "        else\n",
    "            rtnA[j] = A[j]\n",
    "        end\n",
    "        \n",
    "        # Handle B\n",
    "        if B[j] == 0.0\n",
    "            rtnB[j] = A[j]\n",
    "        else\n",
    "            rtnB[j] = B[j]\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return rtnA, rtnB\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5721c7-88a9-4b57-bf9f-ad9f9acbf786",
   "metadata": {},
   "source": [
    "# CartPole Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc4097d-9b96-453c-ba4f-4b06fce7fb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur_s     = 40\n",
    "ts        = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f083b48-38dc-4616-979a-da8874303d32",
   "metadata": {},
   "source": [
    "# Agent Data Structs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61f648d5-8d8e-4da4-bd1e-3f3d9ec7c2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 76032)\n"
     ]
    }
   ],
   "source": [
    "Fres     = Fmax/Fdiv\n",
    "spaceDiv = 4.0 # 1.0 # 2.0 # 5.0 # 7.5  \n",
    "\n",
    "### Construct grid of anchors ###\n",
    "G    = regular_grid_pts_nD( _Q_DOMAIN, [ spaceDiv, spaceDiv, spaceDiv, spaceDiv, Fres ] );\n",
    "nPts = size( G )[2]; # ------- Number of anchors\n",
    "mDim = size( G )[1]; # ------- Dimensionality of anchors \n",
    "V    = zeros(Float64, nPts); # Values at anchors\n",
    "VS   = zeros(Float64, nPts); # Scratch values\n",
    "vsts = zeros(Int64, nPts); # - Set number of visits to zero\n",
    "println( size( G ) )\n",
    "\n",
    "# Construct spatial trees over anchors (WITHOUT reordering!)\n",
    "Q_kdTree = KDTree( G            ; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "X_kdTree = KDTree( G[1:_DIM_X,:]; leafsize = _LEAFLEN, reorder = false ); # Vals must remain assoc w pnts!\n",
    "Q_blTree = BallTree( G             ); \n",
    "X_blTree = BallTree( G[1:_DIM_X,:] ); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82db1609-9df1-438b-9675-0286bf01a2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "T       = Int64((1/ts)*dur_s)\n",
    "N_0     = N_cart( 0.0, 0.0, pi/2.0 )\n",
    "X_0     = [ 0.0, 0.0, pi, 0.0, 0.0, 10.0 , N_0 ]\n",
    "states  = zeros( size( X_0, 1 ), T )\n",
    "actions = zeros( T );\n",
    "bestXs  = zeros( size( X_0, 1 ), T )\n",
    "bestAs  = zeros( T );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eb9f1ef-79bc-41fd-b6e9-ab0554460bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vSwp = zeros(Float64, nPts); # Swap values\n",
    "vBst = zeros(Float64, nPts); # Best values\n",
    "vBAv = zeros(Float64, nPts); # Values for best average\n",
    "vBlA = zeros(Float64, nPts); # Values for best average\n",
    "vAll = zeros(Float64, nPts); # Absorbs all training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d49b4c6-8353-4a01-8a16-9b544e1ef378",
   "metadata": {},
   "outputs": [],
   "source": [
    "vB25 = zeros(Float64, nPts); # Best 25 : Train 75\n",
    "vB50 = zeros(Float64, nPts); # Best 50 : Train 50\n",
    "vB75 = zeros(Float64, nPts); # Best 75 : Train 25\n",
    "vB90 = zeros(Float64, nPts); # Best 90 : Train 10\n",
    "vB95 = zeros(Float64, nPts); # Best 95 : Train  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c954412-18b9-45a8-97a6-e61cf19f15d2",
   "metadata": {},
   "source": [
    "# Agent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d358ff3d-44a5-491e-9597-0a0a73c6b260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Q(TD)-Learning Params #####\n",
    "scale = 7.5; #1.650; # ----------- scale\n",
    "vNN   =  4 #10 #4 #6 #3 # Value nearest neighbors\n",
    "bNN   =  1; #1 # Blend nearest neighbors\n",
    "\n",
    "@assert Fres < scale \"!! `scale` SET TOO LOW !!\"\n",
    "\n",
    "alpha    = 0.15\n",
    "gamma    = 0.99 \n",
    "epsMin   = 0.05 # 0.05\n",
    "epsMax   = 0.50 #0.50 #0.15 #0.50 # 0.3 # 0.75 # 1.00\n",
    "episodes =  64 # 32 #64 #2048 #1024 #128 #512 #256 #20 # 160 # 40 # 80\n",
    "epochs   =  64 #128 #64 # 32 #16\n",
    "EXPrand  = 1.00 #0.25 #0.5 # 0.75\n",
    "Alpha    = 0.875\n",
    "aMargin  = (pi/180)*15.0;\n",
    "\n",
    "##### Eligibility Params #####\n",
    "N_peaks =  40\n",
    "N_steps = 200\n",
    "lambda  =   0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e910ca2-281c-4d06-98e2-1c96fa7c1916",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d3689b-947a-400b-9031-9f1a13f4df2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Best Score: -100.0, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.26000000000000006, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.4000000000000002, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.13562500000000008\n",
      "\n",
      "Epoch 2, Best Score: 0.8000000000000005, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.6400000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.8000000000000005, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.6700000000000004, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.4400000000000002, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.24000000000000007, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.8500000000000005, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.42124999999999885\n",
      "\n",
      "Epoch 3, Best Score: 4.019999999999959, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 1.0900000000000007, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.6600000000000004, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.45000000000000023, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 9.489999999999842, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.8600000000000005, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 2.53999999999999, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.5800000000000003, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.25000000000000006, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.6246874999999946\n",
      "\n",
      "Epoch 4, Best Score: 9.489999999999842, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.45000000000000023, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.6800000000000004, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 1.5100000000000011, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 1.1200000000000008, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.6900000000000004, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.2900000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 1.1400000000000008, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.6300000000000003, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2789062500000002\n",
      "\n",
      "Epoch 5, Best Score: 9.489999999999842, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.3200000000000001, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.07, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 1.0300000000000007, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.4000000000000002, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.5000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.411093749999995\n",
      "\n",
      "Epoch 6, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.24000000000000007, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 1.9600000000000015, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.5000000000000002, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.18000000000000002, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.3900000000000002, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.49000000000000027, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.2900000000000001, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.6500000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.7500000000000004, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.36000000000000015, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2739062500000002\n",
      "\n",
      "Epoch 7, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.5700000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.25000000000000006, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.3000000000000001, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.24000000000000007, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.26000000000000006, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 1.0400000000000007, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.13999999999999999, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.5700000000000003, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.8200000000000005, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.8600000000000005, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.5100000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22156250000000008\n",
      "\n",
      "Epoch 8, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.6500000000000004, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.08, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.4200000000000002, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.36000000000000015, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.07, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.24000000000000007, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.17, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.22000000000000006, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.08, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.13999999999999999, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 1.1200000000000008, epsilon: 0.05703125000000056\n",
      "Average Score: 0.18281250000000013\n",
      "\n",
      "Epoch 9, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 1.1200000000000008, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.20000000000000004, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.20000000000000004, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.2700000000000001, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.3900000000000002, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.5300000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22921875000000005\n",
      "\n",
      "Epoch 10, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.07, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.11999999999999998, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.21000000000000005, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2800000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.26000000000000006, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.5000000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.16796875000000008\n",
      "\n",
      "Epoch 11, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.19000000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.8300000000000005, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.2800000000000001, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.5800000000000003, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.16, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.17, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.16, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.13999999999999999, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.26000000000000006, epsilon: 0.05703125000000056\n",
      "Average Score: 0.23140625000000015\n",
      "\n",
      "Epoch 12, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 2.0300000000000007, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.21000000000000005, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.35000000000000014, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.5200000000000002, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.23000000000000007, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.2900000000000001, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.24000000000000007, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.23390625000000018\n",
      "\n",
      "Epoch 13, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.21000000000000005, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.12999999999999998, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.4300000000000002, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.36000000000000015, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.2700000000000001, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.13999999999999999, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.7600000000000005, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.3300000000000001, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.2700000000000001, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.3000000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.3100000000000001, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1895312500000001\n",
      "\n",
      "Epoch 14, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.2800000000000001, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.5100000000000002, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.3200000000000001, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.18000000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.7200000000000004, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.2900000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.4000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2073437500000001\n",
      "\n",
      "Epoch 15, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.18000000000000002, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.21000000000000005, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.17, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 1.8100000000000014, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.15, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.3900000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.18000000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.12999999999999998, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.7200000000000004, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.16, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22843750000000015\n",
      "\n",
      "Epoch 16, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.6900000000000004, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.3200000000000001, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.3900000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.24000000000000007, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.08, epsilon: 0.05703125000000056\n",
      "Average Score: 0.11796875000000005\n",
      "\n",
      "Epoch 17, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.2900000000000001, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.16, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.21000000000000005, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 1.7900000000000014, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.5200000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.8400000000000005, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.19000000000000003, epsilon: 0.05703125000000056\n",
      "Average Score: 0.26312500000000016\n",
      "\n",
      "Epoch 18, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 1.2300000000000009, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.45000000000000023, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.21000000000000005, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.23000000000000007, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.45000000000000023, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.17, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.3000000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.16, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.15, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1826562500000001\n",
      "\n",
      "Epoch 19, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.2800000000000001, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.6600000000000004, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.19000000000000003, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.19000000000000003, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.21000000000000005, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.46000000000000024, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.22000000000000006, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.18000000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.2900000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.18531250000000013\n",
      "\n",
      "Epoch 20, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.5500000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 1.2000000000000008, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.16, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.25000000000000006, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.19000000000000003, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.36000000000000015, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.23000000000000007, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.8100000000000005, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.3200000000000001, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.10999999999999999, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.6100000000000003, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1881250000000001\n",
      "\n",
      "Epoch 21, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.12999999999999998, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.15, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.22000000000000006, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.23000000000000007, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.13999999999999999, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.15, epsilon: 0.05703125000000056\n",
      "Average Score: 0.14671875000000006\n",
      "\n",
      "Epoch 22, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.2700000000000001, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.7300000000000004, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.3000000000000001, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.17, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.3200000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.5800000000000003, epsilon: 0.05703125000000056\n",
      "Average Score: 0.19968750000000007\n",
      "\n",
      "Epoch 23, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.5300000000000002, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.49000000000000027, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.11999999999999998, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2900000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.2800000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.7100000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.13999999999999999, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.21875000000000008\n",
      "\n",
      "Epoch 24, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.3900000000000002, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.2700000000000001, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.5900000000000003, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.20000000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.22000000000000006, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.35000000000000014, epsilon: 0.05703125000000056\n",
      "Average Score: 0.19203125000000007\n",
      "\n",
      "Epoch 25, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.34000000000000014, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.3200000000000001, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.49000000000000027, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.4200000000000002, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.36000000000000015, epsilon: 0.05703125000000056\n",
      "Average Score: 0.16984375000000007\n",
      "\n",
      "Epoch 26, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.4400000000000002, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.23000000000000007, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.36000000000000015, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.9000000000000006, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.3000000000000001, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.6700000000000004, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.22000000000000006, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2800000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.19000000000000003, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.12999999999999998, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.13999999999999999, epsilon: 0.05703125000000056\n",
      "Average Score: 0.19453125000000013\n",
      "\n",
      "Epoch 27, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.6100000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.22000000000000006, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.22000000000000006, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.8800000000000006, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.48000000000000026, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.5100000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.15, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.13999999999999999, epsilon: 0.05703125000000056\n",
      "Average Score: 0.18671875000000013\n",
      "\n",
      "Epoch 28, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.5200000000000002, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.10999999999999999, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.47000000000000025, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.15, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 1.2000000000000008, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.12999999999999998, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.20000000000000004, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.7100000000000004, epsilon: 0.05703125000000056\n",
      "Average Score: 0.21015625000000013\n",
      "\n",
      "Epoch 29, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.5300000000000002, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.2800000000000001, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.19000000000000003, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.25000000000000006, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.48000000000000026, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.4100000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.22000000000000006, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.7800000000000005, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22078125000000018\n",
      "\n",
      "Epoch 30, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.21000000000000005, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.26000000000000006, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.09999999999999999, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.3200000000000001, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.17, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.7200000000000004, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.37000000000000016, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.04, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.5000000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 1.0000000000000007, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.19968750000000007\n",
      "\n",
      "Epoch 31, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.7200000000000004, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.19000000000000003, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.38000000000000017, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.6900000000000004, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.49000000000000027, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.20000000000000004, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.17, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.5800000000000003, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.25000000000000006, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.21875000000000006\n",
      "\n",
      "Epoch 32, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.16, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.35000000000000014, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.35000000000000014, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.34000000000000014, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 1.5700000000000012, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.5500000000000003, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.19000000000000003, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.2800000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.5600000000000003, epsilon: 0.05703125000000056\n",
      "Average Score: 0.23671875000000012\n",
      "\n",
      "Epoch 33, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.24000000000000007, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 1.1700000000000008, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.22000000000000006, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.17, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.2900000000000001, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.25000000000000006, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.16453125000000007\n",
      "\n",
      "Epoch 34, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.2800000000000001, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.12999999999999998, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.26000000000000006, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.16, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.7100000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.16625000000000006\n",
      "\n",
      "Epoch 35, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.13999999999999999, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.3100000000000001, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.12999999999999998, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.22000000000000006, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.21000000000000005, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.5600000000000003, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.5600000000000003, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.16, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.26000000000000006, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.16546875000000003\n",
      "\n",
      "Epoch 36, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.47000000000000025, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.5000000000000002, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.38000000000000017, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 1.0700000000000007, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.49000000000000027, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.3900000000000002, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.38000000000000017, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.3900000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.09999999999999999, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 1.290000000000001, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2540625000000002\n",
      "\n",
      "Epoch 37, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 1.370000000000001, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.3900000000000002, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.5200000000000002, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.3300000000000001, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.21000000000000005, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.5100000000000002, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.3000000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.11999999999999998, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.5300000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.5700000000000003, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.19781250000000014\n",
      "\n",
      "Epoch 38, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.11999999999999998, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.21000000000000005, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.16, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.15, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.48000000000000026, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.49000000000000027, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.5300000000000002, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.20000000000000004, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.5200000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.23375000000000012\n",
      "\n",
      "Epoch 39, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.20000000000000004, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.2700000000000001, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.25000000000000006, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.5500000000000003, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.15, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.2900000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.24000000000000007, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1728125000000001\n",
      "\n",
      "Epoch 40, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.13999999999999999, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.22000000000000006, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.19000000000000003, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.4100000000000002, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.25000000000000006, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.20000000000000004, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 1.1100000000000008, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.3100000000000001, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.16750000000000007\n",
      "\n",
      "Epoch 41, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.6500000000000004, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.35000000000000014, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.17, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.19000000000000003, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.13999999999999999, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2900000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.3000000000000001, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.45000000000000023, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.38000000000000017, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.3100000000000001, epsilon: 0.05703125000000056\n",
      "Average Score: 0.21156250000000013\n",
      "\n",
      "Epoch 42, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.4300000000000002, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.5300000000000002, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 1.1900000000000008, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.24000000000000007, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.19000000000000003, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.26000000000000006, epsilon: 0.05703125000000056\n",
      "Average Score: 0.23515625000000007\n",
      "\n",
      "Epoch 43, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.23000000000000007, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.15, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.5600000000000003, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.4100000000000002, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.22000000000000006, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.48000000000000026, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.19000000000000003, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.6400000000000003, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.4200000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.45000000000000023, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.24625000000000014\n",
      "\n",
      "Epoch 44, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.5300000000000002, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.23000000000000007, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.10999999999999999, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.6600000000000004, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.5900000000000003, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2700000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.6300000000000003, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.11999999999999998, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.10999999999999999, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.12999999999999998, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.23140625000000012\n",
      "\n",
      "Epoch 45, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.19000000000000003, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.25000000000000006, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.2800000000000001, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 1.310000000000001, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.6900000000000004, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2800000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.34000000000000014, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.09, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.6000000000000003, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.25656250000000014\n",
      "\n",
      "Epoch 46, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.20000000000000004, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.11999999999999998, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.18000000000000002, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.13999999999999999, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.17, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.18000000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.18000000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.3100000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.10406250000000003\n",
      "\n",
      "Epoch 47, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 1.1500000000000008, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.8800000000000006, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.07, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.13999999999999999, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.9100000000000006, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.16, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.07, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.23000000000000007, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1984375000000001\n",
      "\n",
      "Epoch 48, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.19000000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.4300000000000002, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.8300000000000005, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.3300000000000001, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.21000000000000005, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.4300000000000002, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2700000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.34000000000000014, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.23000000000000007, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.6600000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.18000000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.22000000000000006, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2220312500000001\n",
      "\n",
      "Epoch 49, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.49000000000000027, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.3100000000000001, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.13999999999999999, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.45000000000000023, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.5000000000000002, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.15, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.17, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.17, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.3900000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22875000000000012\n",
      "\n",
      "Epoch 50, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.22000000000000006, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.5800000000000003, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.5100000000000002, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.5800000000000003, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.24000000000000007, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.18718750000000015\n",
      "\n",
      "Epoch 51, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.19000000000000003, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.13999999999999999, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.4100000000000002, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.3100000000000001, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.22000000000000006, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.15, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.21000000000000005, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.35000000000000014, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.7200000000000004, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.19109375000000015\n",
      "\n",
      "Epoch 52, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.34000000000000014, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.5300000000000002, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.5600000000000003, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.47000000000000025, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.5500000000000003, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.6800000000000004, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.46000000000000024, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.08, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.4400000000000002, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.22000000000000006, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2164062500000001\n",
      "\n",
      "Epoch 53, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.7300000000000004, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.18000000000000002, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.2700000000000001, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.17, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.26000000000000006, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.49000000000000027, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.3000000000000001, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.4200000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.8900000000000006, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.3000000000000001, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1937500000000001\n",
      "\n",
      "Epoch 54, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.8600000000000005, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.3000000000000001, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 1.0200000000000007, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.7300000000000004, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 1.2100000000000009, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.6200000000000003, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.20000000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.25515625000000014\n",
      "\n",
      "Epoch 55, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.4400000000000002, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.4000000000000002, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.2800000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.17, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.6600000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.24000000000000007, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.0, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.21421874999999993\n",
      "\n",
      "Epoch 56, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.18000000000000002, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.8100000000000005, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.4000000000000002, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.19000000000000003, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.35000000000000014, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.17, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.3200000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.09, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.18000000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.34000000000000014, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2125000000000001\n",
      "\n",
      "Epoch 57, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.18000000000000002, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.4100000000000002, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.3900000000000002, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.5100000000000002, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.12999999999999998, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.22000000000000006, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.5100000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.24000000000000007, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.17, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 1.1600000000000008, epsilon: 0.05703125000000056\n",
      "Average Score: 0.20375000000000013\n",
      "\n",
      "Epoch 58, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.17, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.5400000000000003, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 1.5500000000000012, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.4000000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.18000000000000002, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 1.0700000000000007, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.4200000000000002, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.25000000000000006, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22093750000000012\n",
      "\n",
      "Epoch 59, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.3900000000000002, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.21000000000000005, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.16, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.4200000000000002, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.5800000000000003, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.25000000000000006, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.5000000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.20000000000000004, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 1.0600000000000007, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2218750000000001\n",
      "\n",
      "Epoch 60, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.23000000000000007, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.0, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.5500000000000003, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.25000000000000006, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.11999999999999998, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.0, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.6000000000000003, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.6000000000000003, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.4400000000000002, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.0, epsilon: 0.05703125000000056\n",
      "Average Score: 0.20312500000000006\n",
      "\n",
      "Epoch 61, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.6600000000000004, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.11999999999999998, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.24000000000000007, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.0, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 1.460000000000001, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.09, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.17, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.3100000000000001, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.9600000000000006, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.4400000000000002, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 1.5300000000000011, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.5400000000000003, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.20000000000000004, epsilon: 0.05703125000000056\n",
      "Average Score: 0.24750000000000014\n",
      "\n",
      "Epoch 62, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.7900000000000005, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.0, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.2700000000000001, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.35000000000000014, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.0, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 1.2200000000000009, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.23000000000000007, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.16, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.11999999999999998, epsilon: 0.05703125000000056\n",
      "Average Score: 0.22640625000000011\n",
      "\n",
      "Epoch 63, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 1.0100000000000007, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.6500000000000004, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.17, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.20000000000000004, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.19000000000000003, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.17, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.0, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.0, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.05, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.35000000000000014, epsilon: 0.05703125000000056\n",
      "Average Score: 0.2351562500000001\n",
      "\n",
      "Epoch 64, Best Score: 13.84999999999975, Best Average: -100.0\n",
      "Training Iteration 4 score: 0.0, epsilon: 0.47890625000000003\n",
      "Training Iteration 8 score: 0.38000000000000017, epsilon: 0.4507812500000001\n",
      "Training Iteration 12 score: 0.25000000000000006, epsilon: 0.4226562500000001\n",
      "Training Iteration 16 score: 0.17, epsilon: 0.39453125000000017\n",
      "Training Iteration 20 score: 0.0, epsilon: 0.3664062500000002\n",
      "Training Iteration 24 score: 0.0, epsilon: 0.33828125000000026\n",
      "Training Iteration 28 score: 0.0, epsilon: 0.3101562500000003\n",
      "Training Iteration 32 score: 0.24000000000000007, epsilon: 0.28203125000000034\n",
      "Training Iteration 36 score: 0.0, epsilon: 0.2539062500000004\n",
      "Training Iteration 40 score: 0.0, epsilon: 0.22578125000000043\n",
      "Training Iteration 44 score: 0.2700000000000001, epsilon: 0.19765625000000048\n",
      "Training Iteration 48 score: 0.2900000000000001, epsilon: 0.16953125000000052\n",
      "Training Iteration 52 score: 0.0, epsilon: 0.14140625000000057\n",
      "Training Iteration 56 score: 0.22000000000000006, epsilon: 0.11328125000000058\n",
      "Training Iteration 60 score: 0.6200000000000003, epsilon: 0.08515625000000057\n",
      "Training Iteration 64 score: 0.18000000000000002, epsilon: 0.05703125000000056\n",
      "Average Score: 0.1835937500000001\n",
      "Saved a trained Q-table with size (76032,), After 30.351690336068472 minutes of training!\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip680\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip680)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip681\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip680)\" d=\"\n",
       "M156.598 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.598 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip682\">\n",
       "    <rect x=\"156\" y=\"47\" width=\"2197\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  185.867,1486.45 185.867,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  514.732,1486.45 514.732,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  843.596,1486.45 843.596,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1172.46,1486.45 1172.46,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1501.33,1486.45 1501.33,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1830.19,1486.45 1830.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2159.05,1486.45 2159.05,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  185.867,1486.45 185.867,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  514.732,1486.45 514.732,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  843.596,1486.45 843.596,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1172.46,1486.45 1172.46,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1501.33,1486.45 1501.33,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1830.19,1486.45 1830.19,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2159.05,1486.45 2159.05,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"M185.867 1517.37 Q182.256 1517.37 180.427 1520.93 Q178.622 1524.47 178.622 1531.6 Q178.622 1538.71 180.427 1542.27 Q182.256 1545.82 185.867 1545.82 Q189.501 1545.82 191.307 1542.27 Q193.136 1538.71 193.136 1531.6 Q193.136 1524.47 191.307 1520.93 Q189.501 1517.37 185.867 1517.37 M185.867 1513.66 Q191.677 1513.66 194.733 1518.27 Q197.812 1522.85 197.812 1531.6 Q197.812 1540.33 194.733 1544.94 Q191.677 1549.52 185.867 1549.52 Q180.057 1549.52 176.978 1544.94 Q173.923 1540.33 173.923 1531.6 Q173.923 1522.85 176.978 1518.27 Q180.057 1513.66 185.867 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M489.419 1544.91 L497.058 1544.91 L497.058 1518.55 L488.748 1520.21 L488.748 1515.95 L497.012 1514.29 L501.688 1514.29 L501.688 1544.91 L509.327 1544.91 L509.327 1548.85 L489.419 1548.85 L489.419 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M528.771 1517.37 Q525.16 1517.37 523.331 1520.93 Q521.526 1524.47 521.526 1531.6 Q521.526 1538.71 523.331 1542.27 Q525.16 1545.82 528.771 1545.82 Q532.405 1545.82 534.211 1542.27 Q536.04 1538.71 536.04 1531.6 Q536.04 1524.47 534.211 1520.93 Q532.405 1517.37 528.771 1517.37 M528.771 1513.66 Q534.581 1513.66 537.637 1518.27 Q540.715 1522.85 540.715 1531.6 Q540.715 1540.33 537.637 1544.94 Q534.581 1549.52 528.771 1549.52 Q522.961 1549.52 519.882 1544.94 Q516.827 1540.33 516.827 1531.6 Q516.827 1522.85 519.882 1518.27 Q522.961 1513.66 528.771 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M822.37 1544.91 L838.689 1544.91 L838.689 1548.85 L816.745 1548.85 L816.745 1544.91 Q819.407 1542.16 823.99 1537.53 Q828.596 1532.88 829.777 1531.53 Q832.022 1529.01 832.902 1527.27 Q833.805 1525.51 833.805 1523.82 Q833.805 1521.07 831.86 1519.33 Q829.939 1517.6 826.837 1517.6 Q824.638 1517.6 822.184 1518.36 Q819.754 1519.13 816.976 1520.68 L816.976 1515.95 Q819.8 1514.82 822.254 1514.24 Q824.708 1513.66 826.745 1513.66 Q832.115 1513.66 835.309 1516.35 Q838.504 1519.03 838.504 1523.52 Q838.504 1525.65 837.694 1527.57 Q836.907 1529.47 834.8 1532.07 Q834.221 1532.74 831.12 1535.95 Q828.018 1539.15 822.37 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M858.504 1517.37 Q854.893 1517.37 853.064 1520.93 Q851.258 1524.47 851.258 1531.6 Q851.258 1538.71 853.064 1542.27 Q854.893 1545.82 858.504 1545.82 Q862.138 1545.82 863.943 1542.27 Q865.772 1538.71 865.772 1531.6 Q865.772 1524.47 863.943 1520.93 Q862.138 1517.37 858.504 1517.37 M858.504 1513.66 Q864.314 1513.66 867.369 1518.27 Q870.448 1522.85 870.448 1531.6 Q870.448 1540.33 867.369 1544.94 Q864.314 1549.52 858.504 1549.52 Q852.694 1549.52 849.615 1544.94 Q846.559 1540.33 846.559 1531.6 Q846.559 1522.85 849.615 1518.27 Q852.694 1513.66 858.504 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M1161.3 1530.21 Q1164.66 1530.93 1166.54 1533.2 Q1168.43 1535.47 1168.43 1538.8 Q1168.43 1543.92 1164.91 1546.72 Q1161.4 1549.52 1154.91 1549.52 Q1152.74 1549.52 1150.42 1549.08 Q1148.13 1548.66 1145.68 1547.81 L1145.68 1543.29 Q1147.62 1544.43 1149.94 1545.01 Q1152.25 1545.58 1154.78 1545.58 Q1159.17 1545.58 1161.47 1543.85 Q1163.78 1542.11 1163.78 1538.8 Q1163.78 1535.75 1161.63 1534.03 Q1159.5 1532.3 1155.68 1532.3 L1151.65 1532.3 L1151.65 1528.45 L1155.86 1528.45 Q1159.31 1528.45 1161.14 1527.09 Q1162.97 1525.7 1162.97 1523.11 Q1162.97 1520.45 1161.07 1519.03 Q1159.2 1517.6 1155.68 1517.6 Q1153.76 1517.6 1151.56 1518.01 Q1149.36 1518.43 1146.72 1519.31 L1146.72 1515.14 Q1149.38 1514.4 1151.7 1514.03 Q1154.04 1513.66 1156.1 1513.66 Q1161.42 1513.66 1164.52 1516.09 Q1167.62 1518.5 1167.62 1522.62 Q1167.62 1525.49 1165.98 1527.48 Q1164.34 1529.45 1161.3 1530.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M1187.3 1517.37 Q1183.69 1517.37 1181.86 1520.93 Q1180.05 1524.47 1180.05 1531.6 Q1180.05 1538.71 1181.86 1542.27 Q1183.69 1545.82 1187.3 1545.82 Q1190.93 1545.82 1192.74 1542.27 Q1194.57 1538.71 1194.57 1531.6 Q1194.57 1524.47 1192.74 1520.93 Q1190.93 1517.37 1187.3 1517.37 M1187.3 1513.66 Q1193.11 1513.66 1196.16 1518.27 Q1199.24 1522.85 1199.24 1531.6 Q1199.24 1540.33 1196.16 1544.94 Q1193.11 1549.52 1187.3 1549.52 Q1181.49 1549.52 1178.41 1544.94 Q1175.35 1540.33 1175.35 1531.6 Q1175.35 1522.85 1178.41 1518.27 Q1181.49 1513.66 1187.3 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M1489.5 1518.36 L1477.69 1536.81 L1489.5 1536.81 L1489.5 1518.36 M1488.27 1514.29 L1494.15 1514.29 L1494.15 1536.81 L1499.08 1536.81 L1499.08 1540.7 L1494.15 1540.7 L1494.15 1548.85 L1489.5 1548.85 L1489.5 1540.7 L1473.9 1540.7 L1473.9 1536.19 L1488.27 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M1516.81 1517.37 Q1513.2 1517.37 1511.37 1520.93 Q1509.57 1524.47 1509.57 1531.6 Q1509.57 1538.71 1511.37 1542.27 Q1513.2 1545.82 1516.81 1545.82 Q1520.45 1545.82 1522.25 1542.27 Q1524.08 1538.71 1524.08 1531.6 Q1524.08 1524.47 1522.25 1520.93 Q1520.45 1517.37 1516.81 1517.37 M1516.81 1513.66 Q1522.62 1513.66 1525.68 1518.27 Q1528.76 1522.85 1528.76 1531.6 Q1528.76 1540.33 1525.68 1544.94 Q1522.62 1549.52 1516.81 1549.52 Q1511 1549.52 1507.92 1544.94 Q1504.87 1540.33 1504.87 1531.6 Q1504.87 1522.85 1507.92 1518.27 Q1511 1513.66 1516.81 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M1804.89 1514.29 L1823.25 1514.29 L1823.25 1518.22 L1809.17 1518.22 L1809.17 1526.7 Q1810.19 1526.35 1811.21 1526.19 Q1812.23 1526 1813.25 1526 Q1819.03 1526 1822.41 1529.17 Q1825.79 1532.34 1825.79 1537.76 Q1825.79 1543.34 1822.32 1546.44 Q1818.85 1549.52 1812.53 1549.52 Q1810.35 1549.52 1808.08 1549.15 Q1805.84 1548.78 1803.43 1548.04 L1803.43 1543.34 Q1805.51 1544.47 1807.74 1545.03 Q1809.96 1545.58 1812.44 1545.58 Q1816.44 1545.58 1818.78 1543.48 Q1821.12 1541.37 1821.12 1537.76 Q1821.12 1534.15 1818.78 1532.04 Q1816.44 1529.94 1812.44 1529.94 Q1810.56 1529.94 1808.69 1530.35 Q1806.83 1530.77 1804.89 1531.65 L1804.89 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M1845 1517.37 Q1841.39 1517.37 1839.57 1520.93 Q1837.76 1524.47 1837.76 1531.6 Q1837.76 1538.71 1839.57 1542.27 Q1841.39 1545.82 1845 1545.82 Q1848.64 1545.82 1850.44 1542.27 Q1852.27 1538.71 1852.27 1531.6 Q1852.27 1524.47 1850.44 1520.93 Q1848.64 1517.37 1845 1517.37 M1845 1513.66 Q1850.81 1513.66 1853.87 1518.27 Q1856.95 1522.85 1856.95 1531.6 Q1856.95 1540.33 1853.87 1544.94 Q1850.81 1549.52 1845 1549.52 Q1839.19 1549.52 1836.12 1544.94 Q1833.06 1540.33 1833.06 1531.6 Q1833.06 1522.85 1836.12 1518.27 Q1839.19 1513.66 1845 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M2144.46 1529.7 Q2141.31 1529.7 2139.46 1531.86 Q2137.63 1534.01 2137.63 1537.76 Q2137.63 1541.49 2139.46 1543.66 Q2141.31 1545.82 2144.46 1545.82 Q2147.61 1545.82 2149.44 1543.66 Q2151.29 1541.49 2151.29 1537.76 Q2151.29 1534.01 2149.44 1531.86 Q2147.61 1529.7 2144.46 1529.7 M2153.74 1515.05 L2153.74 1519.31 Q2151.98 1518.48 2150.18 1518.04 Q2148.4 1517.6 2146.64 1517.6 Q2142.01 1517.6 2139.55 1520.72 Q2137.12 1523.85 2136.77 1530.17 Q2138.14 1528.15 2140.2 1527.09 Q2142.26 1526 2144.74 1526 Q2149.95 1526 2152.96 1529.17 Q2155.99 1532.32 2155.99 1537.76 Q2155.99 1543.08 2152.84 1546.3 Q2149.69 1549.52 2144.46 1549.52 Q2138.46 1549.52 2135.29 1544.94 Q2132.12 1540.33 2132.12 1531.6 Q2132.12 1523.41 2136.01 1518.55 Q2139.9 1513.66 2146.45 1513.66 Q2148.21 1513.66 2149.99 1514.01 Q2151.8 1514.36 2153.74 1515.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M2174.04 1517.37 Q2170.43 1517.37 2168.6 1520.93 Q2166.8 1524.47 2166.8 1531.6 Q2166.8 1538.71 2168.6 1542.27 Q2170.43 1545.82 2174.04 1545.82 Q2177.68 1545.82 2179.48 1542.27 Q2181.31 1538.71 2181.31 1531.6 Q2181.31 1524.47 2179.48 1520.93 Q2177.68 1517.37 2174.04 1517.37 M2174.04 1513.66 Q2179.85 1513.66 2182.91 1518.27 Q2185.99 1522.85 2185.99 1531.6 Q2185.99 1540.33 2182.91 1544.94 Q2179.85 1549.52 2174.04 1549.52 Q2168.23 1549.52 2165.15 1544.94 Q2162.1 1540.33 2162.1 1531.6 Q2162.1 1522.85 2165.15 1518.27 Q2168.23 1513.66 2174.04 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,1456.31 2352.76,1456.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,1195.52 2352.76,1195.52 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,934.73 2352.76,934.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,673.939 2352.76,673.939 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,413.149 2352.76,413.149 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,152.359 2352.76,152.359 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1486.45 156.598,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1456.31 175.496,1456.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1195.52 175.496,1195.52 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,934.73 175.496,934.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,673.939 175.496,673.939 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,413.149 175.496,413.149 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,152.359 175.496,152.359 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"M64.6495 1442.11 Q61.0384 1442.11 59.2097 1445.67 Q57.4041 1449.22 57.4041 1456.35 Q57.4041 1463.45 59.2097 1467.02 Q61.0384 1470.56 64.6495 1470.56 Q68.2837 1470.56 70.0892 1467.02 Q71.9179 1463.45 71.9179 1456.35 Q71.9179 1449.22 70.0892 1445.67 Q68.2837 1442.11 64.6495 1442.11 M64.6495 1438.41 Q70.4596 1438.41 73.5152 1443.01 Q76.5938 1447.6 76.5938 1456.35 Q76.5938 1465.07 73.5152 1469.68 Q70.4596 1474.26 64.6495 1474.26 Q58.8393 1474.26 55.7606 1469.68 Q52.7051 1465.07 52.7051 1456.35 Q52.7051 1447.6 55.7606 1443.01 Q58.8393 1438.41 64.6495 1438.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M84.8114 1467.71 L89.6956 1467.71 L89.6956 1473.59 L84.8114 1473.59 L84.8114 1467.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M100.691 1469.66 L108.33 1469.66 L108.33 1443.29 L100.02 1444.96 L100.02 1440.7 L108.283 1439.03 L112.959 1439.03 L112.959 1469.66 L120.598 1469.66 L120.598 1473.59 L100.691 1473.59 L100.691 1469.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M65.0198 1181.32 Q61.4087 1181.32 59.58 1184.88 Q57.7745 1188.43 57.7745 1195.55 Q57.7745 1202.66 59.58 1206.23 Q61.4087 1209.77 65.0198 1209.77 Q68.6541 1209.77 70.4596 1206.23 Q72.2883 1202.66 72.2883 1195.55 Q72.2883 1188.43 70.4596 1184.88 Q68.6541 1181.32 65.0198 1181.32 M65.0198 1177.62 Q70.83 1177.62 73.8855 1182.22 Q76.9642 1186.8 76.9642 1195.55 Q76.9642 1204.28 73.8855 1208.89 Q70.83 1213.47 65.0198 1213.47 Q59.2097 1213.47 56.131 1208.89 Q53.0754 1204.28 53.0754 1195.55 Q53.0754 1186.8 56.131 1182.22 Q59.2097 1177.62 65.0198 1177.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M85.1818 1206.92 L90.066 1206.92 L90.066 1212.8 L85.1818 1212.8 L85.1818 1206.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M104.279 1208.86 L120.598 1208.86 L120.598 1212.8 L98.6539 1212.8 L98.6539 1208.86 Q101.316 1206.11 105.899 1201.48 Q110.506 1196.83 111.686 1195.49 Q113.932 1192.96 114.811 1191.23 Q115.714 1189.47 115.714 1187.78 Q115.714 1185.02 113.77 1183.29 Q111.848 1181.55 108.746 1181.55 Q106.547 1181.55 104.094 1182.31 Q101.663 1183.08 98.8854 1184.63 L98.8854 1179.91 Q101.709 1178.77 104.163 1178.19 Q106.617 1177.62 108.654 1177.62 Q114.024 1177.62 117.219 1180.3 Q120.413 1182.99 120.413 1187.48 Q120.413 1189.61 119.603 1191.53 Q118.816 1193.43 116.709 1196.02 Q116.131 1196.69 113.029 1199.91 Q109.927 1203.1 104.279 1208.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M64.0708 920.528 Q60.4597 920.528 58.631 924.093 Q56.8254 927.635 56.8254 934.764 Q56.8254 941.871 58.631 945.436 Q60.4597 948.977 64.0708 948.977 Q67.705 948.977 69.5105 945.436 Q71.3392 941.871 71.3392 934.764 Q71.3392 927.635 69.5105 924.093 Q67.705 920.528 64.0708 920.528 M64.0708 916.825 Q69.8809 916.825 72.9365 921.431 Q76.0151 926.014 76.0151 934.764 Q76.0151 943.491 72.9365 948.098 Q69.8809 952.681 64.0708 952.681 Q58.2606 952.681 55.1819 948.098 Q52.1264 943.491 52.1264 934.764 Q52.1264 926.014 55.1819 921.431 Q58.2606 916.825 64.0708 916.825 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M84.2327 946.13 L89.1169 946.13 L89.1169 952.01 L84.2327 952.01 L84.2327 946.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M113.469 933.376 Q116.825 934.093 118.7 936.362 Q120.598 938.63 120.598 941.963 Q120.598 947.079 117.08 949.88 Q113.561 952.681 107.08 952.681 Q104.904 952.681 102.589 952.241 Q100.297 951.825 97.8437 950.968 L97.8437 946.454 Q99.7882 947.588 102.103 948.167 Q104.418 948.746 106.941 948.746 Q111.339 948.746 113.631 947.01 Q115.945 945.274 115.945 941.963 Q115.945 938.908 113.793 937.195 Q111.663 935.459 107.844 935.459 L103.816 935.459 L103.816 931.616 L108.029 931.616 Q111.478 931.616 113.307 930.251 Q115.135 928.862 115.135 926.269 Q115.135 923.607 113.237 922.195 Q111.362 920.76 107.844 920.76 Q105.922 920.76 103.723 921.177 Q101.524 921.593 98.8854 922.473 L98.8854 918.306 Q101.547 917.565 103.862 917.195 Q106.2 916.825 108.26 916.825 Q113.584 916.825 116.686 919.255 Q119.788 921.663 119.788 925.783 Q119.788 928.653 118.145 930.644 Q116.501 932.612 113.469 933.376 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M62.9365 659.738 Q59.3254 659.738 57.4967 663.303 Q55.6912 666.845 55.6912 673.974 Q55.6912 681.081 57.4967 684.645 Q59.3254 688.187 62.9365 688.187 Q66.5707 688.187 68.3763 684.645 Q70.205 681.081 70.205 673.974 Q70.205 666.845 68.3763 663.303 Q66.5707 659.738 62.9365 659.738 M62.9365 656.034 Q68.7467 656.034 71.8022 660.641 Q74.8809 665.224 74.8809 673.974 Q74.8809 682.701 71.8022 687.307 Q68.7467 691.891 62.9365 691.891 Q57.1264 691.891 54.0477 687.307 Q50.9921 682.701 50.9921 673.974 Q50.9921 665.224 54.0477 660.641 Q57.1264 656.034 62.9365 656.034 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M83.0984 685.34 L87.9827 685.34 L87.9827 691.219 L83.0984 691.219 L83.0984 685.34 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M111.015 660.733 L99.2095 679.182 L111.015 679.182 L111.015 660.733 M109.788 656.659 L115.668 656.659 L115.668 679.182 L120.598 679.182 L120.598 683.071 L115.668 683.071 L115.668 691.219 L111.015 691.219 L111.015 683.071 L95.4132 683.071 L95.4132 678.557 L109.788 656.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M64.418 398.948 Q60.8069 398.948 58.9782 402.513 Q57.1726 406.054 57.1726 413.184 Q57.1726 420.29 58.9782 423.855 Q60.8069 427.397 64.418 427.397 Q68.0522 427.397 69.8578 423.855 Q71.6865 420.29 71.6865 413.184 Q71.6865 406.054 69.8578 402.513 Q68.0522 398.948 64.418 398.948 M64.418 395.244 Q70.2281 395.244 73.2837 399.851 Q76.3624 404.434 76.3624 413.184 Q76.3624 421.911 73.2837 426.517 Q70.2281 431.1 64.418 431.1 Q58.6078 431.1 55.5291 426.517 Q52.4736 421.911 52.4736 413.184 Q52.4736 404.434 55.5291 399.851 Q58.6078 395.244 64.418 395.244 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M84.5799 424.55 L89.4641 424.55 L89.4641 430.429 L84.5799 430.429 L84.5799 424.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M99.6956 395.869 L118.052 395.869 L118.052 399.804 L103.978 399.804 L103.978 408.277 Q104.996 407.929 106.015 407.767 Q107.033 407.582 108.052 407.582 Q113.839 407.582 117.219 410.753 Q120.598 413.925 120.598 419.341 Q120.598 424.92 117.126 428.022 Q113.654 431.1 107.334 431.1 Q105.159 431.1 102.89 430.73 Q100.645 430.36 98.2372 429.619 L98.2372 424.92 Q100.321 426.054 102.543 426.61 Q104.765 427.165 107.242 427.165 Q111.246 427.165 113.584 425.059 Q115.922 422.952 115.922 419.341 Q115.922 415.73 113.584 413.624 Q111.246 411.517 107.242 411.517 Q105.367 411.517 103.492 411.934 Q101.64 412.351 99.6956 413.23 L99.6956 395.869 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M63.2606 138.158 Q59.6495 138.158 57.8208 141.722 Q56.0152 145.264 56.0152 152.394 Q56.0152 159.5 57.8208 163.065 Q59.6495 166.606 63.2606 166.606 Q66.8948 166.606 68.7004 163.065 Q70.5291 159.5 70.5291 152.394 Q70.5291 145.264 68.7004 141.722 Q66.8948 138.158 63.2606 138.158 M63.2606 134.454 Q69.0707 134.454 72.1263 139.06 Q75.205 143.644 75.205 152.394 Q75.205 161.12 72.1263 165.727 Q69.0707 170.31 63.2606 170.31 Q57.4504 170.31 54.3717 165.727 Q51.3162 161.12 51.3162 152.394 Q51.3162 143.644 54.3717 139.06 Q57.4504 134.454 63.2606 134.454 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M83.4225 163.759 L88.3067 163.759 L88.3067 169.639 L83.4225 169.639 L83.4225 163.759 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M109.071 150.495 Q105.922 150.495 104.071 152.648 Q102.242 154.801 102.242 158.551 Q102.242 162.278 104.071 164.454 Q105.922 166.606 109.071 166.606 Q112.219 166.606 114.047 164.454 Q115.899 162.278 115.899 158.551 Q115.899 154.801 114.047 152.648 Q112.219 150.495 109.071 150.495 M118.353 135.843 L118.353 140.102 Q116.594 139.269 114.788 138.829 Q113.006 138.389 111.246 138.389 Q106.617 138.389 104.163 141.514 Q101.733 144.639 101.385 150.958 Q102.751 148.945 104.811 147.88 Q106.871 146.792 109.348 146.792 Q114.557 146.792 117.566 149.963 Q120.598 153.111 120.598 158.551 Q120.598 163.875 117.45 167.093 Q114.302 170.31 109.071 170.31 Q103.075 170.31 99.9039 165.727 Q96.7326 161.12 96.7326 152.394 Q96.7326 144.199 100.621 139.338 Q104.51 134.454 111.061 134.454 Q112.82 134.454 114.603 134.801 Q116.408 135.148 118.353 135.843 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip682)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  218.754,1363.4 251.64,618.522 284.527,87.9763 317.413,989.74 350.299,645.008 383.186,1002.78 416.072,1139.29 448.959,1240.34 481.845,1119.32 514.732,1279.05 \n",
       "  547.618,1113.62 580.505,1107.1 613.391,1222.82 646.278,1176.37 679.164,1121.36 712.051,1409.45 744.937,1030.9 777.823,1240.75 810.71,1233.82 843.596,1226.49 \n",
       "  876.483,1334.47 909.369,1196.33 942.256,1146.62 975.142,1216.3 1008.03,1274.16 1040.92,1209.78 1073.8,1230.16 1106.69,1169.03 1139.57,1141.32 1172.46,1196.33 \n",
       "  1205.35,1146.62 1238.23,1099.76 1271.12,1288.02 1304.01,1283.54 1336.89,1285.57 1369.78,1054.53 1402.67,1201.22 1435.55,1107.5 1468.44,1266.42 1501.33,1280.28 \n",
       "  1534.21,1165.37 1567.1,1103.84 1599.98,1074.9 1632.87,1113.62 1665.76,1048.01 1698.64,1445.72 1731.53,1199.59 1764.42,1138.06 1797.3,1120.54 1830.19,1228.93 \n",
       "  1863.08,1218.75 1895.96,1152.73 1928.85,1211.82 1961.74,1051.68 1994.62,1158.44 2027.51,1162.92 2060.4,1185.74 2093.28,1140.92 2126.17,1138.47 2159.05,1187.37 \n",
       "  2191.94,1071.64 2224.83,1126.66 2257.71,1103.84 2290.6,1238.31 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"\n",
       "M1983.1 198.898 L2279.55 198.898 L2279.55 95.2176 L1983.1 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1983.1,198.898 2279.55,198.898 2279.55,95.2176 1983.1,95.2176 1983.1,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.5,147.058 2153.92,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"M2192.16 166.745 Q2190.35 171.375 2188.64 172.787 Q2186.93 174.199 2184.06 174.199 L2180.65 174.199 L2180.65 170.634 L2183.15 170.634 Q2184.91 170.634 2185.89 169.8 Q2186.86 168.967 2188.04 165.865 L2188.8 163.921 L2178.32 138.412 L2182.83 138.412 L2190.93 158.689 L2199.03 138.412 L2203.55 138.412 L2192.16 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M2210.84 160.402 L2218.48 160.402 L2218.48 134.037 L2210.17 135.703 L2210.17 131.444 L2218.43 129.778 L2223.11 129.778 L2223.11 160.402 L2230.75 160.402 L2230.75 164.338 L2210.84 164.338 L2210.84 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgn       = time()\n",
    "averages  = []\n",
    "bestScore = -100.0;\n",
    "bestAvg   = -100.0;\n",
    "\n",
    "for m = 1:epochs\n",
    "    \n",
    "    println( \"\\nEpoch \", m, \", Best Score: \", bestScore, \", Best Average: \", bestAvg )\n",
    "    \n",
    "    epsilon = epsMax \n",
    "    deltaEp = (epsMax - epsMin)/episodes\n",
    "    s_Prev  = 0.0\n",
    "    s_Totl  = 0.0\n",
    "    \n",
    "    for l = 1:episodes\n",
    "        X  = X_0\n",
    "        \n",
    "        ##### Double Q-Learning ###########################################\n",
    "\n",
    "        for k = 1:T\n",
    "\n",
    "            # 1. Choose action\n",
    "            if rand() < epsilon\n",
    "                if rand() < EXPrand \n",
    "                    A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                else\n",
    "                    A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                end\n",
    "            else\n",
    "\n",
    "                A = learned_action_for_state( X, _A_DOMAIN, [ Fmax/Fdiv ], ts )\n",
    "                if A == 1000.0 # Indicates no values in this region\n",
    "                    if rand() < EXPrand \n",
    "                        A = sample_uniform_real( _A_DOMAIN[1] , _A_DOMAIN[2] )\n",
    "                    else\n",
    "                        A = optimal_action_for_state( X, _A_DOMAIN, [ Fres ], ts )\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "\n",
    "            # 2. Cache last state\n",
    "            qLast = get_Q( select_X_vector( X ), A )\n",
    "\n",
    "            # 3. Generate the next stae\n",
    "            Xp = cartpole_dyn( X, A, ts )\n",
    "\n",
    "            # 4. Collect reward R( s, a, s' )\n",
    "            R_t = cartpole_reward( Xp )\n",
    "\n",
    "            # 5. Get the optimal action at the next state\n",
    "            a_tp1_opt = optimal_action_for_state( Xp, _A_DOMAIN, [ Fres ], ts )\n",
    "\n",
    "            # 6. Compute the value at the next state\n",
    "\n",
    "            V_tp1_opt = query_value_fuzzy( \n",
    "                Q_kdTree, G, V, \n",
    "                get_Q( \n",
    "                    select_X_vector( Xp ), \n",
    "                    a_tp1_opt \n",
    "                ); \n",
    "                k = vNN \n",
    "            )\n",
    "            if isnan( V_tp1_opt )\n",
    "                V_tp1_opt = 0.0\n",
    "            end\n",
    "\n",
    "\n",
    "            # 7. Blend the value back into nearest points\n",
    "\n",
    "            idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, qLast; k = bNN )\n",
    "\n",
    "            nNear      = size( idxs, 1 )\n",
    "            for i = 1:nNear\n",
    "                j    = idxs[i]\n",
    "                if !isnan( wgts[i] ) \n",
    "\n",
    "                    # VS[j] = R_t + gamma * V_tp1_opt # Q-Learning\n",
    "                    VS[j] = VS[j] + alpha*( R_t + V_tp1_opt - V[j] ) # Q(TD)-Learning\n",
    "                    \n",
    "                end\n",
    "            end\n",
    "\n",
    "            states[:,k] = Xp\n",
    "            actions[k]  = A\n",
    "\n",
    "            X = Xp\n",
    "        end\n",
    "\n",
    "        s_l    = vertical_score_s( states, aMargin, ts )\n",
    "        s_Totl += s_l\n",
    "    \n",
    "        if s_l > bestScore\n",
    "            bestScore = s_l\n",
    "            bestXs    = copy( states  )\n",
    "            bestAs    = copy( actions )\n",
    "            vBst      = copy( V )\n",
    "        end\n",
    "        \n",
    "        if l%4 == 0\n",
    "            println( \"Training Iteration \", l, \" score: \", s_l, \", epsilon: \", epsilon )\n",
    "        end\n",
    "        \n",
    "        ##### Eligibility Traces ##########################################\n",
    "        if false\n",
    "        \n",
    "            # 1. Find `N_peaks`\n",
    "            peakDices = find_state_history_R_peaks( states, N_peaks )\n",
    "            # 2. For each peak, iterate back in time through states\n",
    "            for ii = 1:min(N_peaks, length(peakDices))\n",
    "                topDex = peakDices[ ii ]\n",
    "                X      = states[:,topDex]\n",
    "                R_jj    = cartpole_reward( X )\n",
    "                # 3. For each Q-state in the trace\n",
    "                for jj = (topDex-1):-1:max(1,topDex-N_steps)\n",
    "                    X = states[:,jj]\n",
    "                    R_jj *= lambda\n",
    "                    a_jj = actions[jj]\n",
    "                    q_jj = get_Q( select_X_vector( X ), a_jj )\n",
    "                    V_jj = query_value_fuzzy( Q_kdTree, G, V, q_jj; k = vNN )\n",
    "\n",
    "                    idxs, wgts = query_contrib_to_neighbors( Q_kdTree, G, V, q_jj; k = bNN )\n",
    "                    nNear      = size( idxs, 1 )\n",
    "\n",
    "                    for kk = 1:nNear\n",
    "                        ll = idxs[kk]\n",
    "                        if !isnan( wgts[kk] ) \n",
    "                            VS[ll] = VS[ll] + alpha*( R_jj + V_jj - V[ll] ) # Q(TD)-Learning\n",
    "                        end\n",
    "                    end\n",
    "                end\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        \n",
    "        # Decay the exploration probability\n",
    "        epsilon -= deltaEp\n",
    "        \n",
    "        \n",
    "        ##### Double Q-Learning ##########################################\n",
    "\n",
    "        # Swap Q-functions for Double Q-Learning\n",
    "        vSwp = copy( VS   )\n",
    "        VS   = copy( V    )\n",
    "        V    = copy( vSwp )\n",
    "        \n",
    "    end\n",
    "    \n",
    "    s_Avg = s_Totl / episodes\n",
    "    println( \"Average Score: \", s_Avg )\n",
    "    \n",
    "    append!( averages, s_Avg )\n",
    "    \n",
    "    ##### Q-Function Hacks ################################################\n",
    "    \n",
    "    # FIXME, START HERE: BLENDING SEEMS FUCKED?\n",
    "    \n",
    "    if (s_Avg > bestAvg) && true\n",
    "        println( \"BLEND\" )\n",
    "        bestAvg = s_Avg\n",
    "        vBAv    = copy( V ) # Try a blend of both next # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "        vBlA    = blend_alpha_of_A_into_B( 0.50, VS, V ) # FIXME: WE NEVER ACTUALLY USE THIS MATRIX!\n",
    "    end\n",
    "        \n",
    "end\n",
    "\n",
    "vTrn = copy( V )\n",
    "println( \"Saved a trained Q-table with size \", size( vTrn ), \", After \", (time()-bgn)/60.0, \" minutes of training!\" )\n",
    "\n",
    "using Plots\n",
    "\n",
    "plot( averages )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c593da-dc6b-4658-a102-7988776030cd",
   "metadata": {},
   "source": [
    "# Method 2 + TD + Trace, Average Vertical Duration [s]\n",
    "Each score is the best run out of an entire training period: 64 epochs of 64 episodes each, Q-function swap after every episode  \n",
    "\n",
    "| Method             | Trial 1 | Trial 2 | Trial 3 | Trial 4 | Trial 5 |\n",
    "| ------------------ | ------- | ------- | ------- | ------- | ------- |\n",
    "| TD + Blend         |         |         |         |         |         |\n",
    "| TD + Trace         | 0.235   |         |         |         |         |\n",
    "| TD + Trace + Blend | 0.186   |         |         |         |         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c1d8a-58c5-4719-89c8-b69bf6623266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
