{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609fdba4-6834-43eb-a744-e883fc5765c5",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea4e769-5560-422b-9bbe-8acf3a32f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9310531-9f36-4009-a47e-696f0526cb21",
   "metadata": {},
   "source": [
    "# Env Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e90cf5-5420-4625-8ecd-bf57f02e5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_OBS_DIM = 4\n",
    "_ACT_DIM = 1\n",
    "_BIGNUM  = 1e5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b185bb3-132d-42ec-ab6a-e159071de80b",
   "metadata": {},
   "source": [
    "# Minimum Viable Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f07e7b1-d3a9-4a16-961a-b44f884a688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K     = 3 # ---------------------- Number of neighbors to query\n",
    "N     = 0 # ---------------------- Number of exemplars\n",
    "F     = np.zeros( (N,_OBS_DIM,) ) # Approximating function\n",
    "V     = np.zeros( (N,) ) # ------- Action values\n",
    "A     = np.zeros( (N,_ACT_DIM,) ) # Actions (Output)\n",
    "KDT   = None # ------------------- Spatial tree\n",
    "eps   = 1.0 # -------------------- Exploration probability\n",
    "decay = 0.99\n",
    "rad   = 0.125 # ------------------ Minimum distance between exemplars (Overwrite radius)\n",
    "vMar  = 0.10 # ------------------- Allowed margin on value estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b83e762-42ad-4edf-9780-ed1372bd3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_from_indices( pnts, ndcs ):\n",
    "    \"\"\" Get the subset of `pnts` designated by `ndcs` \"\"\"\n",
    "    N = len( ndcs )\n",
    "    P = np.zeros( (N,pnts.shape[1],) )\n",
    "    for i, idx in enumerate( ndcs ):\n",
    "        P[i,:] = pnts[idx,:]\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0951ea1-1d07-4ac7-9d7f-df0a75e4acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def add_particle( state, action, value, getTree = False ):\n",
    "    \"\"\" Add a new particle to the value function \"\"\"\n",
    "    global N, F, V, A, KDT\n",
    "    \n",
    "    print( f\"V-Stack: {F.shape} + {state.shape}\" )\n",
    "    \n",
    "    if N < 1:\n",
    "        F = state\n",
    "        V = np.array( [value,] )\n",
    "        A = action\n",
    "    else:\n",
    "        F = np.vstack( (F,state, ) )\n",
    "        V = np.array( V.tolist().append( value ) )\n",
    "        A = np.vstack( (A,action,) )\n",
    "\n",
    "    N += 1\n",
    "    if getTree:\n",
    "        return N, KDTree( F )\n",
    "    else:\n",
    "        return N, None\n",
    "        \n",
    "\n",
    "def recalc_spatial_tree():\n",
    "    \"\"\" Recalculate spatial tree \"\"\"\n",
    "    global N, F, V, A, KDT\n",
    "    if N > 1:\n",
    "        KDT = KDTree( F )\n",
    "\n",
    "    \n",
    "def get_action_and_value_inv_dist( state ):\n",
    "    \"\"\" Estimate the current optimal action and value for the state \"\"\"\n",
    "    global N, F, V, A, KDT\n",
    "    if (N < 1) or (KDT is None):\n",
    "        return None, None\n",
    "    dists, indcs = KDT.query( state, K )\n",
    "    fractV = []\n",
    "    indcsV = []\n",
    "    for i, d in enumerate( dists ):\n",
    "        if d < _BIGNUM:\n",
    "            fractV.append( 1.0/d )\n",
    "            indcsV.append( indcs[i] )\n",
    "    fractV = np.array( fractV )\n",
    "    normD  = np.linalg.norm( fractV )\n",
    "    fractV = fractV / normD\n",
    "    rtnAct = np.zeros( (_ACT_DIM,) )\n",
    "    rtnVal = 0.0\n",
    "    for i, frac in enumerate( fractV ):\n",
    "        idx = indcsV[i]\n",
    "        rtnAct += (F[ idx ] * frac)\n",
    "        rtnVal += (V[ idx ] * frac)\n",
    "    return rtnAct, rtnVal\n",
    "   \n",
    "\n",
    "def eval_particle( state, action, value ):\n",
    "    \"\"\" Decide whetner this point represents a particle worth saving \"\"\"\n",
    "    global N, F, V, A, KDT\n",
    "    # 0. Get our estimate of the value of this state\n",
    "    estAct, estVal = get_action_and_value_inv_dist( state )\n",
    "    \n",
    "    # 1. Find out if there is a particle there\n",
    "    if KDT is not None:\n",
    "        ndcs = KDT.query_ball_point( F, state, rad )\n",
    "    else:\n",
    "        ndcs = []\n",
    "\n",
    "    # 2. If there is a particle already there and the current value is better, then update\n",
    "    if len( ndcs ) and (value > estVal):\n",
    "        index = ndcs[0]\n",
    "        # fNear = points_from_indices( F, ndcs )\n",
    "        vNear = points_from_indices( V, ndcs )\n",
    "        if vNear[0] < value:\n",
    "            A[index,:] = action\n",
    "            V[index]   = value\n",
    "        if len( ndcs ) > 1:\n",
    "            print( \"WARNING: NEARNESS CONSTRAINT VIOLATED\" )\n",
    "\n",
    "    # 3. Elif this is an open space that does NOT estimate the value well\n",
    "    elif (estVal is None) or abs(estVal - value) > abs(value * vMar):\n",
    "        add_particle( state, action, value )\n",
    "        recalc_spatial_tree()\n",
    "    # Else this is an open space that predicts the value well, No update!\n",
    "    # N. Return the current number of particles in the estimator\n",
    "    return N\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3997de2e-e6b7-4358-bde5-1975ca095326",
   "metadata": {},
   "source": [
    "# Simple Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade65299-9291-435b-90db-626462528630",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "epLen    =  500\n",
    "avg_time = 0\n",
    "max_time = -1\n",
    "env      = gym.make( 'CartPole-v1' ).env\n",
    "env      = wrappers.RecordEpisodeStatistics( env, 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafeab77-22bb-4c9a-bfba-7ca223c7a32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Starting at [[-0.00537278  0.00695139 -0.021494   -0.011204  ]]\n",
      "[-0.00523375  0.20237489 -0.02171808 -0.3105903 ] (4,)\n",
      "V-Stack: (0, 4) + (1, 4)\n",
      "[-0.00118626  0.007569   -0.02792988 -0.02483497] (4,)\n",
      "V-Stack: (1, 4) + (1, 4)\n",
      "[-0.00103488  0.20308012 -0.02842658 -0.3261976 ] (4,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m( obs, obs\u001b[38;5;241m.\u001b[39mshape )\n\u001b[1;32m     17\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mreshape( (\u001b[38;5;241m1\u001b[39m,_OBS_DIM,) )\n\u001b[0;32m---> 19\u001b[0m \u001b[43meval_particle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43msLast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated:\n\u001b[1;32m     22\u001b[0m     avg_time \u001b[38;5;241m=\u001b[39m avg_time \u001b[38;5;241m+\u001b[39m t\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36meval_particle\u001b[0;34m(state, action, value)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m N, F, V, A, KDT\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 0. Get our estimate of the value of this state\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m estAct, estVal \u001b[38;5;241m=\u001b[39m \u001b[43mget_action_and_value_inv_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# 1. Find out if there is a particle there\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m KDT \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mget_action_and_value_inv_dist\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     37\u001b[0m indcsV \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m( dists ):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m<\u001b[39m _BIGNUM:\n\u001b[1;32m     40\u001b[0m         fractV\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39md )\n\u001b[1;32m     41\u001b[0m         indcsV\u001b[38;5;241m.\u001b[39mappend( indcs[i] )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "for i_episode in range( EPISODES ):\n",
    "    # instansiating the environment\n",
    "    obs = env.reset()[0].reshape( (1,_OBS_DIM,) )\n",
    "    print( f\"Episode {i_episode+1}: Starting at {obs}\" )\n",
    "    for t in range( epLen ):\n",
    "        # uncomment this is you want to see the rendering \n",
    "        #env.render()\n",
    "        if random() < eps:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action, preVal = get_action_and_value_inv_dist( obs )\n",
    "            \n",
    "        sLast = obs\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = env.step( action )\n",
    "        print( obs, obs.shape )\n",
    "        obs = obs.reshape( (1,_OBS_DIM,) )\n",
    "        \n",
    "        eval_particle( sLast, action, reward )\n",
    "        \n",
    "        if terminated:\n",
    "            avg_time = avg_time + t\n",
    "            if t > max_time:\n",
    "                max_time = t\n",
    "                print( f\"\\tMax. Uptime: {max_time}\" )\n",
    "            #print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "    # resetting the enviroment\n",
    "    env.reset()\n",
    "        \n",
    "\n",
    "# printing the avg time the game lasted\n",
    "avg_time = avg_time/EPISODES\n",
    "print( 'avg time agent survives :', avg_time )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
